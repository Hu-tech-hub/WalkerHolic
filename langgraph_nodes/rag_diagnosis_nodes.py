"""
RAG-based diagnosis nodes for LangGraph-based gait analysis pipeline
Implements 3 nodes: compose_prompt, rag_diagnosis, store_diagnosis
Uses PDF documents for medical knowledge retrieval
"""
import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

# RAG and Vector Database imports
import chromadb
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

import os
from dotenv import load_dotenv

# Conditional imports to handle both module and script execution
try:
    # Try relative imports first (when run as module)
    from .base_node import BaseNode
    from .graph_state import GraphState, StateManager, PipelineStages
except ImportError:
    # Fall back to absolute imports (when run as script)
    import sys
    from pathlib import Path
    
    # Add the parent directory to sys.path so we can import from langgraph_nodes
    current_dir = Path(__file__).parent
    parent_dir = current_dir.parent
    if str(parent_dir) not in sys.path:
        sys.path.insert(0, str(parent_dir))
    
    # Now try absolute imports
    try:
        from langgraph_nodes.base_node import BaseNode
        from langgraph_nodes.graph_state import GraphState, StateManager, PipelineStages
    except ImportError as e:
        print(f"Error: Could not import required modules: {e}")
        print("Make sure you're running this from the correct directory or as a module.")
        sys.exit(1)

# Load environment variables
load_dotenv()

class ComposePromptNode(BaseNode):
    """
    Node 9: Compose diagnostic prompt from gait metrics
    Prepares structured prompt for RAG-based medical diagnosis
    """
    
    def __init__(self):
        super().__init__(PipelineStages.COMPOSE_PROMPT)
    
    def get_system_prompt(self) -> str:
        return """You are a medical data analyst specializing in gait analysis interpretation.
        
        Your task is to compose a comprehensive diagnostic prompt from calculated gait metrics:
        
        Prompt composition requirements:
        - Summarize all 12 gait metrics in clinical context
        - Identify abnormal values based on normative data
        - Highlight asymmetries and stability concerns
        - Structure information for medical diagnosis retrieval
        - Include patient demographics (age estimation from gait patterns)
        
        The composed prompt will be used to search medical literature for:
        - Potential pathological conditions
        - Differential diagnoses
        - Clinical recommendations
        - Rehabilitation strategies
        
        Ensure the prompt is medically accurate and comprehensive.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Compose diagnostic prompt from gait metrics"""
        
        if not self.validate_state_requirements(state, ["gait_metrics", "height_cm"]):
            return StateManager.set_error(state, "Missing required fields: gait_metrics, height_cm", "validation_error")
        
        gait_metrics = state["gait_metrics"]
        height_cm = state["height_cm"]
        date = state.get("date", "unknown")
        session_id = state.get("session_id", "unknown")
        
        try:
            # Create concise, evidence-based diagnostic prompt
            # Focus only on objective metrics, avoid lengthy LLM generation
            
            # Extract ALL 12 gait metrics with normal ranges for comparison
            avg_stride_time = gait_metrics.get('avg_stride_time', 0)
            avg_stride_length = gait_metrics.get('avg_stride_length', 0) 
            avg_walking_speed = gait_metrics.get('avg_walking_speed', 0)
            cadence = gait_metrics.get('cadence', 0)
            stride_time_asymmetry = gait_metrics.get('stride_time_asymmetry', 0)
            stride_length_asymmetry = gait_metrics.get('stride_length_asymmetry', 0)
            stride_time_cv = gait_metrics.get('stride_time_cv', 0)
            walking_speed_cv = gait_metrics.get('walking_speed_cv', 0)
            
            # Additional 4 metrics (previously missing from RAG prompt)
            stride_length_cv = gait_metrics.get('stride_length_cv', 0)
            step_width = gait_metrics.get('step_width', 0)
            gait_regularity_index = gait_metrics.get('gait_regularity_index', 0)
            gait_stability_ratio = gait_metrics.get('gait_stability_ratio', 0)
            
            # New phase ratio metrics
            stance_phase_ratio = gait_metrics.get('stance_phase_ratio', 0.6)
            swing_phase_ratio = gait_metrics.get('swing_phase_ratio', 0.4)
            double_support_ratio = gait_metrics.get('double_support_ratio', 0.2)
            
            # Create comprehensive prompt with ALL 15 metrics
            structured_prompt = f"""ë³´í–‰ ë¶„ì„ ê²°ê³¼

í™˜ì ì •ë³´: ì‹ ì¥ {height_cm}cm, ë‚ ì§œ {date}

ì „ì²´ 15ê°œ ê°ê´€ì  ì§€í‘œ:

ã€ì‹œê°„ì  ì§€í‘œã€‘
â€¢ ë³´í­ ì‹œê°„: {avg_stride_time:.2f}ì´ˆ (ì •ìƒ: 1.0-1.3ì´ˆ)
â€¢ ë³´í–‰ë¥ : {cadence:.0f}ê±¸ìŒ/ë¶„ (ì •ìƒ: 100-120)
â€¢ ë³´í­ ì‹œê°„ ë³€ë™ì„±: {stride_time_cv:.1f}% (ì •ìƒ: <5%)

ã€ê³µê°„ì  ì§€í‘œã€‘
â€¢ ë³´í­ ê¸¸ì´: {avg_stride_length:.2f}m (ì •ìƒ: 1.2-1.6m)
â€¢ ë³´í­ ê¸¸ì´ ë³€ë™ì„±: {stride_length_cv:.1f}% (ì •ìƒ: <5%)
â€¢ ë³´í­ í­: {step_width:.2f}m (ì •ìƒ: 0.1-0.15m)

ã€ì†ë„ ì§€í‘œã€‘
â€¢ ë³´í–‰ ì†ë„: {avg_walking_speed:.2f}m/s (ì •ìƒ: 1.0-1.4m/s)
â€¢ ë³´í–‰ ì†ë„ ë³€ë™ì„±: {walking_speed_cv:.1f}% (ì •ìƒ: <5%)

ã€ë¹„ëŒ€ì¹­ì„± ì§€í‘œã€‘
â€¢ ë³´í­ ì‹œê°„ ë¹„ëŒ€ì¹­ì„±: {stride_time_asymmetry:.1f}% (ì •ìƒ: <5%)
â€¢ ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„±: {stride_length_asymmetry:.1f}% (ì •ìƒ: <5%)

ã€ì•ˆì •ì„± ì§€í‘œã€‘
â€¢ ë³´í–‰ ê·œì¹™ì„± ì§€ìˆ˜: {gait_regularity_index:.3f} (ì •ìƒ: >0.8)
â€¢ ë³´í–‰ ì•ˆì •ì„± ë¹„ìœ¨: {gait_stability_ratio:.3f} (ì •ìƒ: >0.8)

ã€ë³´í–‰ ì£¼ê¸° ì§€í‘œã€‘
â€¢ ì…ê°ê¸° ë¹„ìœ¨: {stance_phase_ratio:.1%} (ì •ìƒ: 60-65%)
â€¢ ìœ ê°ê¸° ë¹„ìœ¨: {swing_phase_ratio:.1%} (ì •ìƒ: 35-40%)
â€¢ ì–‘ë°œì§€ì§€ ë¹„ìœ¨: {double_support_ratio:.1%} (ì •ìƒ: 15-25%)

ì„ìƒ ì§ˆë¬¸: ì´ 15ê°œ ëª¨ë“  ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì„ìƒ í‰ê°€ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? ì •ìƒ ëŒ€ ë³‘ë¦¬í•™ì  íŒ¨í„´ë§Œ ê³ ë ¤í•˜ì„¸ìš”."""
            
            # Update state
            state["prompt_str"] = structured_prompt
            
            self.logger.info(f"Diagnostic prompt composed: {len(structured_prompt)} characters")
            
            return state
            
        except Exception as e:
            error_msg = f"Diagnostic prompt composition failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "prompt_composition_error")

class RagDiagnosisNode(BaseNode):
    """
    Node 10: RAG-based medical diagnosis using PDF knowledge base
    Retrieves relevant medical information and generates diagnosis
    """
    
    def __init__(self):
        super().__init__(PipelineStages.RAG_DIAGNOSIS)
        self.vector_store = None
        self.embeddings = None
        self._initialize_rag_system()
    
    def _initialize_rag_system(self):
        """Initialize the RAG system: vector store, embeddings, and retriever."""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Define ChromaDB path relative to project root
                project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                chroma_db_path = str(project_root / "chroma_db")
                
                # Create directory if it doesn't exist
                Path(chroma_db_path).mkdir(parents=True, exist_ok=True)
                
                self.vector_store = Chroma(
                    embedding_function=self.embeddings,
                    persist_directory=chroma_db_path
                )
                
                # ğŸš€ **ìµœì í™”**: ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                existing_data_count = self._check_existing_embeddings()
                
                if existing_data_count > 0:
                    self.logger.info(f"âœ… ChromaDB ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° ë°œê²¬: {existing_data_count}ê°œ ë¬¸ì„œ")
                    self.logger.info("âš¡ PDF ì¬ë¡œë”© ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ ì„ë² ë”© ì‚¬ìš©")
                else:
                    self.logger.info("ğŸ“š ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš” - PDF ë¡œë”© ì‹œì‘")
                    self._load_medical_pdfs(Path("docs/medical_pdfs"))
                
                self.logger.info("âœ… RAG system initialized successfully.")
                return  # Success, exit retry loop
                
            except Exception as e:
                retry_count += 1
                self.logger.warning(f"RAG initialization attempt {retry_count}/{max_retries} failed: {e}")
                
                if retry_count < max_retries:
                    # Try to clean up ChromaDB directory for retry
                    project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                    chroma_db_path = str(project_root / "chroma_db")
                    try:
                        import shutil
                        if Path(chroma_db_path).exists():
                            shutil.rmtree(chroma_db_path)
                            self.logger.info(f"Cleaned ChromaDB directory for retry {retry_count + 1}")
                    except Exception as cleanup_error:
                        self.logger.warning(f"Failed to cleanup ChromaDB: {cleanup_error}")
                else:
                    self.logger.error(f"Failed to initialize RAG system after {max_retries} attempts: {e}")
                    self.vector_store = None
    
    def _check_existing_embeddings(self) -> int:
        """
        ChromaDBì— ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        
        Returns:
            int: ê¸°ì¡´ ë¬¸ì„œì˜ ê°œìˆ˜ (0ì´ë©´ ìƒˆë¡œ ì„ë² ë”© í•„ìš”)
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ ê°•ì œ ë¦¬ë¡œë”© ì˜µì…˜ ì œê³µ
            force_reload = os.getenv('RAG_FORCE_RELOAD', 'false').lower() == 'true'
            if force_reload:
                self.logger.info("ğŸ”„ RAG_FORCE_RELOAD=true - ê°•ì œë¡œ PDF ì¬ë¡œë”© ìˆ˜í–‰")
                return 0
            
            # ChromaDBì—ì„œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ë¥¼ í™•ì¸
            collection = self.vector_store._collection
            
            # ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            document_count = collection.count()
            
            if document_count > 0:
                self.logger.info(f"ğŸ’¾ ê¸°ì¡´ ChromaDB ë°ì´í„°: {document_count}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                # PDF íŒŒì¼ ë³€ê²½ ê°ì§€ (ì„ íƒì )
                pdf_changed = self._check_pdf_files_changed()
                if pdf_changed:
                    self.logger.info("ğŸ“„ PDF íŒŒì¼ ë³€ê²½ ê°ì§€ - ì¬ì„ë² ë”© ìˆ˜í–‰")
                    return 0
                
                # ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                try:
                    sample_results = collection.peek(limit=3)
                    if sample_results and 'metadatas' in sample_results:
                        for i, metadata in enumerate(sample_results['metadatas'][:3]):
                            source_file = metadata.get('source_file', 'unknown')
                            doc_type = metadata.get('document_type', 'unknown')
                            self.logger.debug(f"   ğŸ“„ ìƒ˜í”Œ {i+1}: {source_file} ({doc_type})")
                except Exception as peek_error:
                    self.logger.debug(f"ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {peek_error}")
                
                return document_count
            else:
                self.logger.info("ğŸ“­ ChromaDBê°€ ë¹„ì–´ìˆìŒ - ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš”")
                return 0
                
        except Exception as e:
            self.logger.warning(f"ê¸°ì¡´ ì„ë² ë”© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìƒˆë¡œ ì„ë² ë”©í•˜ë„ë¡ 0 ë°˜í™˜
            return 0
    
    def _check_pdf_files_changed(self) -> bool:
        """
        PDF íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì„ íƒì  ê¸°ëŠ¥)
        
        Returns:
            bool: PDF íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ True
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ PDF ë³€ê²½ ê°ì§€ í™œì„±í™” ì—¬ë¶€ í™•ì¸
            check_changes = os.getenv('RAG_CHECK_PDF_CHANGES', 'false').lower() == 'true'
            if not check_changes:
                return False
            
            docs_dir = Path("docs/medical_pdfs")
            if not docs_dir.exists():
                return False
            
            # PDF íŒŒì¼ë“¤ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            pdf_files = list(docs_dir.glob("*.pdf"))
            if not pdf_files:
                return False
            
            # ê°€ì¥ ìµœê·¼ PDF ìˆ˜ì • ì‹œê°„ ì°¾ê¸°
            latest_pdf_time = max(f.stat().st_mtime for f in pdf_files)
            
            # ChromaDB ìƒì„± ì‹œê°„ê³¼ ë¹„êµ
            project_root = Path(os.getenv('PROJECT_ROOT', '.'))
            chroma_db_path = project_root / "chroma_db" / "chroma.sqlite3"
            
            if chroma_db_path.exists():
                chroma_db_time = chroma_db_path.stat().st_mtime
                
                if latest_pdf_time > chroma_db_time:
                    self.logger.info(f"ğŸ“„ PDF íŒŒì¼ì´ ChromaDBë³´ë‹¤ ìµœì‹ : PDF={datetime.fromtimestamp(latest_pdf_time)}, DB={datetime.fromtimestamp(chroma_db_time)}")
                    return True
                else:
                    self.logger.debug(f"ğŸ“„ PDF íŒŒì¼ ë³€ê²½ ì—†ìŒ: PDF={datetime.fromtimestamp(latest_pdf_time)}, DB={datetime.fromtimestamp(chroma_db_time)}")
                    return False
            else:
                # ChromaDB íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í•„ìš”
                return True
                
        except Exception as e:
            self.logger.warning(f"PDF íŒŒì¼ ë³€ê²½ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë³€ê²½ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
            return False
    
    def _load_medical_pdfs(self, docs_dir: Path):
        """Load medical PDFs, split them, and add to the vector store."""
        try:
            from langchain_community.document_loaders import PyPDFLoader
            import os
            
            documents = []
            
            if docs_dir.exists():
                pdf_files = [f for f in os.listdir(docs_dir) if f.endswith('.pdf')]
                
                for pdf_file in pdf_files:
                    try:
                        pdf_path = docs_dir / pdf_file
                        loader = PyPDFLoader(str(pdf_path))
                        pdf_docs = loader.load()
                        
                        for doc in pdf_docs:
                            doc.metadata.update({
                                "source_file": pdf_file,
                                "document_type": "medical_literature"
                            })
                        
                        documents.extend(pdf_docs)
                        self.logger.info(f"âœ… Loaded PDF: {pdf_file} ({len(pdf_docs)} pages)")
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to load PDF {pdf_file}: {str(e)}")
                        continue
            
            if not documents:
                self.logger.warning("No PDF files loaded, using sample medical reference")
                # Add a comprehensive sample document if no PDFs are found
                sample_doc = Document(
                    page_content="""
                    ë³´í–‰ ë¶„ì„ ì„ìƒ ì°¸ê³  ìë£Œ - ê·¼ê±° ê¸°ë°˜ ê°€ì´ë“œë¼ì¸
                    
                    ê±´ê°•í•œ ì„±ì¸(20-65ì„¸) ì •ìƒ ë³´í–‰ ì§€í‘œ:
                    - ë³´í­ ì‹œê°„: 1.0-1.3ì´ˆ 
                    - ë³´í–‰ë¥ : 100-120 ê±¸ìŒ/ë¶„
                    - ë³´í­ ê¸¸ì´: 1.2-1.6m (ì‹ ì¥ ì˜ì¡´ì )
                    - ë³´í–‰ ì†ë„: 1.0-1.4 m/s 
                    - ì¢Œìš° ë¹„ëŒ€ì¹­ì„±: <5%
                    - ì‹œê°„ì  ë³€ë™ì„±: <5%
                    - ê³µê°„ì  ë³€ë™ì„±: <5%
                    
                    ë³‘ë¦¬í•™ì  ë³´í–‰ íŒ¨í„´:
                    
                    1. íŒŒí‚¨ìŠ¨ë³‘:
                    - ë³´í­ ê¸¸ì´ ê°ì†Œ (<1.0m)
                    - ë³´í–‰ë¥  ì¦ê°€ (>120 ê±¸ìŒ/ë¶„) 
                    - ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - ë³€ë™ì„± ì¦ê°€ (>10% CV)
                    - íŠ¹ì§•: ì§§ê³  ëŒë¦¬ëŠ” ê±¸ìŒ
                    
                    2. ë‡Œì¡¸ì¤‘ í¸ë§ˆë¹„:
                    - ì‹¬í•œ ë¹„ëŒ€ì¹­ì„± (>15%)
                    - í™˜ì¸¡ ë³´í­ ê¸¸ì´ ê°ì†Œ
                    - ì „ì²´ ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - íŠ¹ì§•: ì¼ì¸¡ì„± ì•½í™” íŒ¨í„´
                    
                    3. ì†Œë‡Œì„± ìš´ë™ì‹¤ì¡°:
                    - ë†’ì€ ë³´í–‰ ë³€ë™ì„± (>15% CV)
                    - ë„“ì€ ë³´í­
                    - ë¶ˆê·œì¹™í•œ ë³´í­ íƒ€ì´ë°
                    - íŠ¹ì§•: í˜‘ì¡°ì„± ë¶€ì¡±
                    """,
                    metadata={"source": "clinical_reference", "type": "sample_data"}
                )
                documents = [sample_doc]
                
            self.logger.info(f"Medical knowledge base loaded: {len(documents)} documents")
            
            # Split documents and add to vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            
            chunks = text_splitter.split_documents(documents)
            self.vector_store.add_documents(chunks)
            
            self.logger.info(f"Added {len(chunks)} chunks to vector store")
                
        except ImportError:
            self.logger.warning("PyPDFLoader not available, using sample medical data")
            sample_doc = Document(
                page_content="ê¸°ë³¸ ë³´í–‰ ë¶„ì„ ì°¸ê³  ìë£Œ: ì •ìƒ ë³´í–‰ ì†ë„ 1.0-1.4 m/s, ë³´í–‰ë¥  100-120 ê±¸ìŒ/ë¶„",
                metadata={"source": "fallback", "type": "basic_reference"}
            )
            # Just return the single doc, assuming no vector store to add to
            return [sample_doc]
    
    def get_system_prompt(self) -> str:
        return """You are a medical AI specialist in gait analysis and movement disorders.
        
        Your task is to provide evidence-based medical diagnosis using retrieved medical knowledge:
        
        Diagnostic process:
        1. Analyze retrieved medical literature
        2. Compare patient metrics with known pathological patterns
        3. Generate differential diagnoses with confidence levels
        4. Provide clinical recommendations
        5. Suggest further evaluations if needed
        
        Output requirements:
        - Primary diagnosis with rationale
        - Differential diagnoses (2-3 alternatives)
        - Confidence level (0-100%)
        - Clinical recommendations
        - Rehabilitation suggestions
        - Red flags or urgent referrals
        
        Base all conclusions on evidence from retrieved medical sources.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Generate RAG-based medical diagnosis"""
        
        if not self.validate_state_requirements(state, ["prompt_str"]):
            return StateManager.set_error(state, "Missing required field: prompt_str", "validation_error")
        
        if self.vector_store is None:
            return StateManager.set_error(state, "RAG system not initialized", "rag_system_error")
        
        prompt_str = state["prompt_str"]
        session_id = state.get("session_id", "unknown")
        gait_metrics = state.get("gait_metrics", {})

        try:
            # Retrieve relevant medical knowledge
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 4})
            relevant_docs = retriever.get_relevant_documents(prompt_str)
            
            # Format retrieved knowledge with source information
            retrieved_knowledge = ""
            source_info = []
            
            for i, doc in enumerate(relevant_docs, 1):
                source_file = doc.metadata.get('source_file', 'unknown_source')
                doc_type = doc.metadata.get('document_type', 'unknown_type')
                page_num = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                # Extract relevant content snippet
                content_snippet = doc.page_content.strip()
                if len(content_snippet) > 300:
                    content_snippet = content_snippet[:300] + "..."
                
                retrieved_knowledge += f"""
=== ì°¸ì¡°ë¬¸í—Œ {i}: {source_file} ===
ë¬¸ì„œìœ í˜•: {doc_type}
í˜ì´ì§€: {page_num}
ê´€ë ¨ë‚´ìš©:
{content_snippet}

"""
                
                source_info.append({
                    "ë²ˆí˜¸": i,
                    "íŒŒì¼ëª…": source_file,
                    "ë¬¸ì„œìœ í˜•": doc_type,
                    "í˜ì´ì§€": page_num,
                    "ë‚´ìš©ê¸¸ì´": len(doc.page_content)
                })
            
            self.logger.info(f"Retrieved {len(relevant_docs)} documents for RAG diagnosis")
            
            # Create comprehensive diagnostic prompt with structured output request
            diagnostic_llm_prompt = f"""
            ë‹¹ì‹ ì€ ì„ìƒ ë³´í–‰ ë¶„ì„ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. ì•„ë˜ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ìë¥¼ ì§„ë‹¨í•˜ê³  êµ¬ì¡°í™”ëœ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”.
            
            === ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ì •ë³´ ===
            {retrieved_knowledge}
            
            === í™˜ì ë³´í–‰ ë¶„ì„ ë°ì´í„° ===
            {prompt_str}
            
            === ì§„ë‹¨ ì§€ì¹¨ ===
            1. **ì˜¤ì§ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œì˜ ê¸°ì¤€ê³¼ ì •ë³´ë§Œ ì‚¬ìš©**í•˜ì—¬ ì§„ë‹¨í•˜ì„¸ìš”
            2. ì§„ë‹¨ ê·¼ê±°ë¥¼ ì œì‹œí•  ë•Œ **êµ¬ì²´ì ì¸ ë¬¸í—Œëª…ê³¼ ë‚´ìš©ì„ ì¸ìš©**í•˜ì„¸ìš”
            3. ê° íŒë‹¨ë§ˆë‹¤ **"ì°¸ì¡°ë¬¸í—Œ Xì— ë”°ë¥´ë©´..."** í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
            4. ê²€ìƒ‰ëœ ì •ë³´ì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ "ì¶”ê°€ ì •ë³´ í•„ìš”"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
            5. ìµœì¢… í‰ê°€ëŠ” ì •í™•í•œ ì ìˆ˜(0-100)ì™€ ìƒíƒœë¥¼ í¬í•¨í•˜ì„¸ìš”
            
            === ì‘ë‹µ í˜•ì‹ (ì •í™•íˆ ì´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ) ===
            CLINICAL_ASSESSMENT: [ì •ìƒ/ì£¼ì˜/ìœ„í—˜ ì¤‘ í•˜ë‚˜]
            SCORE: [0-100 ì‚¬ì´ì˜ ì •ìˆ˜]
            STATUS: [êµ¬ì²´ì ì¸ ìƒíƒœ ì„¤ëª…]
            RISK_LEVEL: [ì •ìƒ ë‹¨ê³„/ì£¼ì˜ ë‹¨ê³„/ìœ„í—˜ ë‹¨ê³„ ì¤‘ í•˜ë‚˜]
            
            ì„ìƒ í‰ê°€: [ê²€ìƒ‰ëœ ë¬¸í—Œ ê¸°ì¤€ìœ¼ë¡œ ìƒì„¸ íŒì •]
            
            ì£¼ìš” ì†Œê²¬: [ê²€ìƒ‰ëœ ë¬¸í—Œì—ì„œ ì°¾ì€ ê´€ë ¨ íŒ¨í„´ê³¼ í™˜ì ë°ì´í„° ë¹„êµ]
            
            ë¬¸í—Œ ê·¼ê±°: 
            - ì°¸ì¡°ë¬¸í—Œ 1 ({source_info[0]["íŒŒì¼ëª…"] if source_info else "ì•Œ ìˆ˜ ì—†ìŒ"}): [êµ¬ì²´ì  ì¸ìš© ë‚´ìš©]
            - ì°¸ì¡°ë¬¸í—Œ 2 ({source_info[1]["íŒŒì¼ëª…"] if len(source_info) > 1 else "ì•Œ ìˆ˜ ì—†ìŒ"}): [êµ¬ì²´ì  ì¸ìš© ë‚´ìš©]
            - ì°¸ì¡°ë¬¸í—Œ 3 ({source_info[2]["íŒŒì¼ëª…"] if len(source_info) > 2 else "ì•Œ ìˆ˜ ì—†ìŒ"}): [êµ¬ì²´ì  ì¸ìš© ë‚´ìš©]
            - ì°¸ì¡°ë¬¸í—Œ 4 ({source_info[3]["íŒŒì¼ëª…"] if len(source_info) > 3 else "ì•Œ ìˆ˜ ì—†ìŒ"}): [êµ¬ì²´ì  ì¸ìš© ë‚´ìš©]
            
            ì‹ ë¢°ë„: [ê²€ìƒ‰ëœ ì •ë³´ì˜ ì¶©ë¶„ì„±ê³¼ ì¼ì¹˜ì„±ì— ë”°ë¥¸ ì‹ ë¢°ë„]
            
            ì§„ë‹¨: [ê²€ìƒ‰ëœ ë¬¸í—Œì— ê¸°ë°˜í•œ ê°€ëŠ¥ì„± ë†’ì€ ì§„ë‹¨ëª…]
            
            ê¶Œì¥ì‚¬í•­: [ê²€ìƒ‰ëœ ë¬¸í—Œì—ì„œ ì œì‹œëœ ì¹˜ë£Œ/ê´€ë¦¬ ë°©ì•ˆ]
            
            ì°¸ê³ ë¬¸í—Œ ëª©ë¡:
            {chr(10).join([f"- {info['íŒŒì¼ëª…']} (í˜ì´ì§€ {info['í˜ì´ì§€']})" for info in source_info])}
            
            **ì¤‘ìš”: ì‘ë‹µ ì‹œì‘ ë¶€ë¶„ì˜ CLINICAL_ASSESSMENT, SCORE, STATUS, RISK_LEVELì„ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , ëª¨ë“  íŒë‹¨ì€ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ì •ë³´ì—ë§Œ ê·¼ê±°í•˜ì„¸ìš”.**
            """
            
            # Get LLM diagnosis
            diagnosis_response = self.invoke_llm(diagnostic_llm_prompt)
            
            # Generate structured JSON diagnosis result with RAG integration
            structured_diagnosis = self._generate_structured_diagnosis(state, gait_metrics, diagnosis_response, source_info)
            
            # Update state with both formats
            state["medical_diagnosis"] = structured_diagnosis  # New JSON format
            state["diagnosis_result"] = structured_diagnosis   # Alternative key for compatibility
            
            # Keep detailed metadata separate
            state["medical_diagnosis_metadata"] = {
                "session_id": session_id,
                "diagnosis_timestamp": datetime.now().isoformat(),
                "raw_diagnosis": diagnosis_response,
                "retrieved_sources": len(relevant_docs),
                "knowledge_base_used": "medical_pdfs",
                "prompt_length": len(prompt_str),
                "response_length": len(diagnosis_response),
                "source_documents": source_info
            }
            
            self.logger.info(f"RAG diagnosis generated: {len(diagnosis_response)} characters from {len(relevant_docs)} sources")
            
            return state
            
        except Exception as e:
            error_msg = f"RAG diagnosis generation failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "rag_diagnosis_error")
    
    def _generate_structured_diagnosis(self, state: GraphState, gait_metrics: dict, raw_diagnosis: str, source_info: list) -> dict:
        """Generate structured JSON diagnosis matching API endpoint format"""
        
        try:
            # Generate indicators from gait metrics
            indicators = self._generate_indicators(gait_metrics)
            
            # Calculate disease probabilities
            diseases = self._calculate_disease_probabilities(gait_metrics)
            
            # Initial assessment calculation
            initial_score, initial_status, initial_risk_level = self._calculate_overall_assessment(gait_metrics, indicators)
            
            # Parse structured RAG assessment from LLM response
            final_score, final_status, final_risk_level = self._parse_structured_rag_assessment(
                raw_diagnosis, initial_score, initial_status, initial_risk_level
            )
            
            # Extract detailed report from raw diagnosis
            detailed_report = self._extract_detailed_report(raw_diagnosis)
            
            # Create structured JSON response with integrated assessment
            structured_result = {
                "success": True,
                "data": {
                    "userId": state.get("user_id", "unknown"),
                    "score": final_score,
                    "status": final_status,
                    "riskLevel": final_risk_level,
                    "analyzedAt": datetime.now().isoformat(),
                    "indicators": indicators,
                    "diseases": diseases,
                    "detailedReport": detailed_report
                }
            }
            
            return structured_result
            
        except Exception as e:
            self.logger.error(f"Failed to generate structured diagnosis: {str(e)}")
            # Return fallback structure
            return {
                "success": False,
                "data": {
                    "userId": "unknown",
                    "score": 50,
                    "status": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                    "riskLevel": "í™•ì¸ í•„ìš”",
                    "analyzedAt": datetime.now().isoformat(),
                    "indicators": [],
                    "diseases": [],
                    "detailedReport": {
                        "title": "ì§„ë‹¨ ì˜¤ë¥˜",
                        "content": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                    }
                }
            }
    
    def _generate_indicators(self, gait_metrics: dict) -> list:
        """Generate indicators array from gait metrics"""
        
        indicators = []
        
        try:
            # 1. Stride Time (ë³´í­ ì‹œê°„)
            stride_time = gait_metrics.get('avg_stride_time', 1.1)
            stride_time_status, stride_time_result = self._assess_stride_time(stride_time)
            indicators.append({
                "id": "stride-time",
                "name": "ë³´í­ ì‹œê°„",
                "value": f"{stride_time:.2f}ì´ˆ",
                "status": stride_time_status,
                "description": "í•œìª½ ë°œì´ ë•…ì— ë‹¿ì€ í›„, ê°™ì€ ë°œì´ ë‹¤ì‹œ ë‹¿ì„ ë•Œê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì…ë‹ˆë‹¤. ê±¸ìŒ í…œí¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.",
                "result": stride_time_result
            })
            
            # 2. Double Support (ì–‘ë°œ ì§€ì§€ ë¹„ìœ¨) - ì‹¤ì œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
            double_support_ratio = gait_metrics.get('double_support_ratio', 0.2) * 100  # Convert ratio to percentage
            ds_status, ds_result = self._assess_double_support(double_support_ratio)
            indicators.append({
                "id": "double-support", 
                "name": "ì–‘ë°œ ì§€ì§€ ë¹„ìœ¨",
                "value": f"{double_support_ratio:.1f}%",
                "status": ds_status,
                "description": "ë‘ ë°œì´ ë™ì‹œì— ë•…ì— ë‹¿ì•„ ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì´ì—ìš”. ë³´í–‰ ê· í˜•ì´ ë¶ˆì•ˆí• ìˆ˜ë¡ ë†’ì•„ì§‘ë‹ˆë‹¤.",
                "result": ds_result
            })
            
            # 3. Stride Difference (ì–‘ë°œ ë³´í­ ì°¨ì´)
            stride_asymmetry = gait_metrics.get('stride_length_asymmetry', 0.0)
            stride_diff_m = self._convert_asymmetry_to_meters(stride_asymmetry, gait_metrics.get('avg_stride_length', 1.2))
            asym_status, asym_result = self._assess_stride_asymmetry(stride_asymmetry)
            indicators.append({
                "id": "stride-difference",
                "name": "ì–‘ë°œ ë³´í­ ì°¨ì´", 
                "value": f"{stride_diff_m:.2f}m",
                "status": asym_status,
                "description": "ì™¼ë°œê³¼ ì˜¤ë¥¸ë°œì˜ ê±¸ìŒ ê¸¸ì´ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¢Œìš° ê· í˜• ìƒíƒœë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì–´ìš”.",
                "result": asym_result
            })
            
            # 4. Walking Speed (í‰ê·  ë³´í–‰ ì†ë„)
            walking_speed = gait_metrics.get('avg_walking_speed', 1.2)
            speed_status, speed_result = self._assess_walking_speed(walking_speed)
            indicators.append({
                "id": "walking-speed",
                "name": "í‰ê·  ë³´í–‰ ì†ë„",
                "value": f"{walking_speed:.1f}m/s", 
                "status": speed_status,
                "description": "ë‹¨ìœ„ ì‹œê°„ ë™ì•ˆ ì´ë™í•œ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì „ì²´ í™œë™ì„±ê³¼ ìš´ë™ ëŠ¥ë ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.",
                "result": speed_result
            })
            
            # 5. Stance Phase Ratio (ì…ê°ê¸° ë¹„ìœ¨)
            stance_phase_ratio = gait_metrics.get('stance_phase_ratio', 0.6)
            stance_status, stance_result = self._assess_stance_phase_ratio(stance_phase_ratio)
            indicators.append({
                "id": "stance-phase",
                "name": "ì…ê°ê¸° ë¹„ìœ¨",
                "value": f"{stance_phase_ratio:.1%}",
                "status": stance_status,
                "description": "ë³´í–‰ ì£¼ê¸° ì¤‘ ë°œì´ ë•…ì— ë‹¿ì•„ ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. ê· í˜•ê³¼ ì•ˆì •ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆì–´ìš”.",
                "result": stance_result
            })
            
        except Exception as e:
            self.logger.error(f"Error generating indicators: {str(e)}")
            
        return indicators
    
    def _calculate_disease_probabilities(self, gait_metrics: dict) -> list:
        """Calculate disease probabilities based on gait metrics"""
        
        diseases = []
        
        try:
            # Parkinson's Disease Risk
            parkinson_prob = self._calculate_parkinson_risk(gait_metrics)
            parkinson_status, parkinson_trend = self._assess_disease_risk(parkinson_prob, "parkinson")
            diseases.append({
                "id": "parkinson",
                "name": "íŒŒí‚¨ìŠ¨ë³‘",
                "probability": round(parkinson_prob, 2),
                "status": parkinson_status,
                "trend": parkinson_trend
            })
            
            # Stroke Risk
            stroke_prob = self._calculate_stroke_risk(gait_metrics)
            stroke_status, stroke_trend = self._assess_disease_risk(stroke_prob, "stroke")
            diseases.append({
                "id": "stroke", 
                "name": "ë‡Œì¡¸ì¤‘",
                "probability": round(stroke_prob, 2),
                "status": stroke_status,
                "trend": stroke_trend
            })
            
        except Exception as e:
            self.logger.error(f"Error calculating disease probabilities: {str(e)}")
            
        return diseases
    
    def _calculate_overall_assessment(self, gait_metrics: dict, indicators: list) -> tuple:
        """Calculate overall score, status, and risk level"""
        
        try:
            # Base score starts at 100
            base_score = 100
            
            # Weight factors for different metrics
            speed_weight = 0.30
            asymmetry_weight = 0.25  
            stability_weight = 0.25
            regularity_weight = 0.20
            
            # Speed score (0-100)
            speed = gait_metrics.get('avg_walking_speed', 1.2)
            speed_score = min(100, max(0, (speed / 1.3) * 100))
            
            # Asymmetry score (inverted - lower asymmetry = higher score)
            asymmetry = gait_metrics.get('stride_length_asymmetry', 0.0)
            asymmetry_score = max(0, 100 - (asymmetry * 10))
            
            # Stability score
            stability = gait_metrics.get('gait_stability_ratio', 0.8)
            stability_score = stability * 100
            
            # Regularity score  
            regularity = gait_metrics.get('gait_regularity_index', 0.8)
            regularity_score = regularity * 100
            
            # Calculate weighted average
            overall_score = int(
                speed_score * speed_weight +
                asymmetry_score * asymmetry_weight +
                stability_score * stability_weight +
                regularity_score * regularity_weight
            )
            
            # Determine status and risk level
            if overall_score >= 80:
                status = "ë³´í–‰ ë§¤ìš° ì•ˆì •ì "
                risk_level = "ì •ìƒ ë‹¨ê³„"
            elif overall_score >= 65:
                status = "ë³´í–‰ ì•ˆì •ì "  
                risk_level = "ì •ìƒ ë‹¨ê³„"
            elif overall_score >= 50:
                status = "ë³´í–‰ ì£¼ì˜ í•„ìš”"
                risk_level = "ì£¼ì˜ ë‹¨ê³„"
            else:
                status = "ë³´í–‰ ë¶ˆì•ˆì •"
                risk_level = "ìœ„í—˜ ë‹¨ê³„"
                
            return overall_score, status, risk_level
            
        except Exception as e:
            self.logger.error(f"Error calculating overall assessment: {str(e)}")
            return 50, "ë¶„ì„ ì˜¤ë¥˜", "í™•ì¸ í•„ìš”"
    
    # Helper methods for indicator assessments
    def _assess_stride_time(self, stride_time: float) -> tuple:
        """Assess stride time and return status and result"""
        if 1.0 <= stride_time <= 1.2:
            return "normal", "ë¶„ì„ ê²°ê³¼ ì •ìƒì…ë‹ˆë‹¤!"
        elif 0.8 <= stride_time < 1.0 or 1.2 < stride_time <= 1.4:
            return "warning", "ë¶„ì„ ê²°ê³¼ ì£¼ì˜ì…ë‹ˆë‹¤!"
        else:
            return "danger", "ë¶„ì„ ê²°ê³¼ ìœ„í—˜ì…ë‹ˆë‹¤!"
    

    
    def _assess_double_support(self, ratio: float) -> tuple:
        """Assess double support ratio"""
        if ratio < 25.0:
            return "normal", "ë¶„ì„ ê²°ê³¼ ì •ìƒì…ë‹ˆë‹¤!"
        elif 25.0 <= ratio <= 30.0:
            return "warning", "ë¶„ì„ ê²°ê³¼ ì£¼ì˜ì…ë‹ˆë‹¤!"
        else:
            return "danger", "ë¶„ì„ ê²°ê³¼ ìœ„í—˜ì…ë‹ˆë‹¤!"
    
    def _convert_asymmetry_to_meters(self, asymmetry_percent: float, avg_stride_length: float) -> float:
        """Convert stride asymmetry percentage to meter difference"""
        return (asymmetry_percent / 100.0) * avg_stride_length
    
    def _assess_stride_asymmetry(self, asymmetry: float) -> tuple:
        """Assess stride length asymmetry"""
        if asymmetry < 3.0:
            return "normal", "ë¶„ì„ ê²°ê³¼ ì •ìƒì…ë‹ˆë‹¤!"
        elif 3.0 <= asymmetry <= 7.0:
            return "warning", "ë¶„ì„ ê²°ê³¼ ì£¼ì˜ì…ë‹ˆë‹¤!"
        else:
            return "danger", "ë¶„ì„ ê²°ê³¼ ìœ„í—˜ì…ë‹ˆë‹¤!"
    
    def _assess_walking_speed(self, speed: float) -> tuple:
        """Assess walking speed"""
        if speed > 1.2:
            return "normal", "ë¶„ì„ ê²°ê³¼ ì •ìƒì…ë‹ˆë‹¤!"
        elif 0.9 <= speed <= 1.2:
            return "warning", "ë¶„ì„ ê²°ê³¼ ì£¼ì˜ì…ë‹ˆë‹¤!"
        else:
            return "danger", "ë¶„ì„ ê²°ê³¼ ìœ„í—˜ì…ë‹ˆë‹¤!"
    
    def _assess_stance_phase_ratio(self, ratio: float) -> tuple:
        """Assess stance phase ratio"""
        if 0.5 <= ratio <= 0.7:
            return "normal", "ë¶„ì„ ê²°ê³¼ ì •ìƒì…ë‹ˆë‹¤!"
        elif 0.3 <= ratio < 0.5 or 0.7 < ratio <= 1.0:
            return "warning", "ë¶„ì„ ê²°ê³¼ ì£¼ì˜ì…ë‹ˆë‹¤!"
        else:
            return "danger", "ë¶„ì„ ê²°ê³¼ ìœ„í—˜ì…ë‹ˆë‹¤!"
    
    # Disease risk calculation methods
    def _calculate_parkinson_risk(self, gait_metrics: dict) -> float:
        """Calculate Parkinson's disease risk score"""
        # Risk factors: low cadence, high stride variability, low regularity
        cadence = gait_metrics.get('cadence', 120.0)
        stride_time_cv = gait_metrics.get('stride_time_cv', 3.0)
        regularity = gait_metrics.get('gait_regularity_index', 0.8)
        
        risk_score = 0.0
        
        # Low cadence increases risk
        if cadence < 100:
            risk_score += 3.0
        elif cadence < 110:
            risk_score += 1.5
        
        # High stride variability increases risk
        if stride_time_cv > 6.0:
            risk_score += 2.5
        elif stride_time_cv > 4.0:
            risk_score += 1.0
        
        # Low regularity increases risk
        if regularity < 0.6:
            risk_score += 2.0
        elif regularity < 0.7:
            risk_score += 1.0
        
        # Normalize to -10 to +10 scale
        return min(10.0, max(-10.0, risk_score - 5.0))
    
    def _calculate_stroke_risk(self, gait_metrics: dict) -> float:
        """Calculate stroke risk score"""
        # Risk factors: high asymmetry, slow speed, instability
        asymmetry = gait_metrics.get('stride_length_asymmetry', 0.0)
        speed = gait_metrics.get('avg_walking_speed', 1.2)
        stability = gait_metrics.get('gait_stability_ratio', 0.8)
        
        risk_score = 0.0
        
        # High asymmetry increases risk
        if asymmetry > 10.0:
            risk_score += 4.0
        elif asymmetry > 5.0:
            risk_score += 2.0
        
        # Slow speed increases risk
        if speed < 0.8:
            risk_score += 3.0
        elif speed < 1.0:
            risk_score += 1.5
        
        # Low stability increases risk
        if stability < 0.6:
            risk_score += 2.5
        elif stability < 0.7:
            risk_score += 1.0
        
        # Normalize to -10 to +10 scale
        return min(10.0, max(-10.0, risk_score - 4.0))
    
    def _assess_disease_risk(self, probability: float, disease_type: str) -> tuple:
        """Assess disease risk and determine status and trend"""
        if probability < -2.0:
            status = "ì •ìƒ ë²”ìœ„"
            trend = "down"
        elif probability < 2.0:
            status = "ê´€ì°° ìœ ì§€"
            trend = "stable"
        elif probability < 5.0:
            status = "ì£¼ì˜ í•„ìš”"
            trend = "up"
        else:
            status = "ìœ„í—˜ ë²”ìœ„"
            trend = "up"
        
        return status, trend
    
    def _parse_structured_rag_assessment(self, rag_response: str, initial_score: int, initial_status: str, initial_risk_level: str) -> tuple:
        """Parse structured assessment from RAG LLM response"""
        
        try:
            # Extract structured fields from LLM response
            lines = rag_response.strip().split('\n')
            
            rag_score = None
            rag_status = None
            rag_risk_level = None
            rag_assessment = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('CLINICAL_ASSESSMENT:'):
                    rag_assessment = line.split(':', 1)[1].strip()
                elif line.startswith('SCORE:'):
                    try:
                        rag_score = int(line.split(':', 1)[1].strip())
                    except (ValueError, IndexError):
                        pass
                elif line.startswith('STATUS:'):
                    rag_status = line.split(':', 1)[1].strip()
                elif line.startswith('RISK_LEVEL:'):
                    rag_risk_level = line.split(':', 1)[1].strip()
            
            # Use RAG assessment if available and valid
            if rag_score is not None and 0 <= rag_score <= 100:
                final_score = rag_score
                self.logger.info(f"Using RAG score: {rag_score} (initial was {initial_score})")
            else:
                final_score = initial_score
                self.logger.warning(f"Invalid RAG score, using initial: {initial_score}")
            
            if rag_status:
                final_status = rag_status
                self.logger.info(f"Using RAG status: {rag_status}")
            else:
                final_status = initial_status
                self.logger.warning(f"No RAG status found, using initial: {initial_status}")
            
            if rag_risk_level and rag_risk_level in ["ì •ìƒ ë‹¨ê³„", "ì£¼ì˜ ë‹¨ê³„", "ìœ„í—˜ ë‹¨ê³„"]:
                final_risk_level = rag_risk_level
                self.logger.info(f"Using RAG risk level: {rag_risk_level}")
            else:
                final_risk_level = initial_risk_level
                self.logger.warning(f"Invalid RAG risk level, using initial: {initial_risk_level}")
            
            # Validate consistency between score and risk level
            if final_score >= 80 and final_risk_level == "ìœ„í—˜ ë‹¨ê³„":
                # Score too high for risk level, adjust
                final_score = min(final_score, 55)
                self.logger.info(f"Adjusted score for consistency: {final_score}")
            elif final_score <= 40 and final_risk_level == "ì •ìƒ ë‹¨ê³„":
                # Score too low for normal level, adjust
                final_risk_level = "ìœ„í—˜ ë‹¨ê³„"
                self.logger.info(f"Adjusted risk level for consistency: {final_risk_level}")
            
            return final_score, final_status, final_risk_level
            
        except Exception as e:
            self.logger.error(f"Error parsing structured RAG assessment: {str(e)}")
            # Return initial assessment on error
            return initial_score, initial_status, initial_risk_level
    
    def _extract_detailed_report(self, raw_diagnosis: str) -> dict:
        """Extract detailed report from raw diagnosis text"""
        try:
            # Try to extract title and content from diagnosis
            lines = raw_diagnosis.strip().split('\n')
            
            # Look for diagnosis or assessment line
            title = "ì˜ë£Œ ì§„ë‹¨ ê²°ê³¼"
            content = raw_diagnosis
            
            for line in lines:
                if "ì§„ë‹¨:" in line:
                    title = line.split(":")[-1].strip()
                    break
                elif "ì„ìƒ í‰ê°€:" in line:
                    title = line.split(":")[-1].strip()
                    break
            
            # Clean up content - allow full content instead of truncating
            # Remove the 500 character limit to show complete diagnosis
            # if len(content) > 500:
            #     content = content[:500] + "..."
            
            return {
                "title": title,
                "content": content
            }
            
        except Exception as e:
            self.logger.error(f"Error extracting detailed report: {str(e)}")
            return {
                "title": "ì§„ë‹¨ ê²°ê³¼",
                "content": "ì§„ë‹¨ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }

class StoreDiagnosisNode(BaseNode):
    """
    Node 11: Store medical diagnosis to Supabase
    Saves RAG-generated diagnosis and recommendations
    """
    
    def __init__(self):
        super().__init__(PipelineStages.STORE_DIAGNOSIS)
    
    def get_system_prompt(self) -> str:
        return """You are a medical records management specialist.
        
        Your task is to store AI-generated medical diagnoses in the database:
        
        Storage requirements:
        - Store complete diagnosis with metadata
        - Link to corresponding gait metrics record
        - Include confidence levels and sources
        - Maintain audit trail for medical decisions
        
        Database validation:
        - Verify diagnosis completeness
        - Check foreign key relationships
        - Validate JSON structure
        
        Provide confirmation of successful storage with record linkage.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Store medical diagnosis to Supabase database"""
        
        required = ["medical_diagnosis", "medical_diagnosis_metadata", "user_id", "session_id"]
        if not self.validate_state_requirements(state, required):
            return StateManager.set_error(state, f"Missing required fields: {required}", "validation_error")
        
        diagnosis_result = state["medical_diagnosis"]
        diagnosis_metadata = state["medical_diagnosis_metadata"]
        user_id = state["user_id"]
        session_id = state.get("session_id")

        try:
            from supabase import create_client
            import json
            
            # Use Service Role key to bypass RLS policies
            supabase_url = os.getenv('SUPABASE_URL')
            supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
            
            if not supabase_url or not supabase_key:
                raise ValueError("Supabase credentials not found in environment variables")
                
            supabase = create_client(supabase_url, supabase_key)
            
            # Handle both old format (string) and new format (structured JSON)
            if isinstance(diagnosis_result, dict) and diagnosis_result.get("success") is not None:
                # New structured JSON format
                diagnosis_content = diagnosis_result
                confidence_score = self._extract_confidence_score(diagnosis_result)
            else:
                # Legacy text format - convert to structured format for compatibility
                diagnosis_content = {
                    "success": True,
                    "data": {
                        "userId": user_id,
                        "score": 50,  # Default score for legacy data
                        "status": "Legacy ì§„ë‹¨",
                        "riskLevel": "í™•ì¸ í•„ìš”",
                        "analyzedAt": diagnosis_metadata.get('diagnosis_timestamp', datetime.now().isoformat()),
                        "indicators": [],
                        "diseases": [],
                        "detailedReport": {
                            "title": "Legacy ì§„ë‹¨ ê²°ê³¼",
                            "content": str(diagnosis_result)[:500]
                        }
                    },
                    # Add legacy metadata for compatibility
                    "legacy_metadata": {
                        'diagnosis_text': str(diagnosis_result),
                        'diagnosis_timestamp': diagnosis_metadata.get('diagnosis_timestamp'),
                        'knowledge_base_used': diagnosis_metadata.get('knowledge_base_used'),
                        'prompt_length': diagnosis_metadata.get('prompt_length'),
                        'response_length': diagnosis_metadata.get('response_length'),
                        'diagnosis_method': 'RAG_PDF_BASED',
                        'ai_model_used': os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
                        'confidence_level': 'AI_GENERATED'
                    }
                }
                confidence_score = None

            storage_data = {
                'session_id': session_id,
                'user_id': user_id,
                'diagnosis_json': diagnosis_content,  # Store as JSONB directly
                'retrieved_papers': diagnosis_metadata.get('retrieved_sources', 0),
                'ai_model_used': os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
                'confidence_score': confidence_score,  # Numerical confidence score
                'knowledge_base_version': 'medical_pdfs_v1',
                'processing_time_seconds': None  # Could be extracted from metadata if available
            }

            # Store to Supabase in 'gait_diagnosis' table
            result = supabase.table('gait_diagnosis').insert(storage_data).execute()
            
            if result.data:
                stored_record = result.data[0]
                record_id = stored_record.get('id')
                state["diagnosis_record_id"] = record_id
                state["diagnosis_stored"] = True
                self.logger.info(f"Medical diagnosis stored successfully: Record ID {record_id}")
                return state
            else:
                error_info = getattr(result, 'error', 'Unknown error')
                return StateManager.set_error(state, f"Failed to store medical diagnosis: {error_info}", "storage_error")
            
        except Exception as e:
            error_msg = f"Medical diagnosis storage failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "storage_execution_error")
    
    def _extract_confidence_score(self, diagnosis_result: dict) -> float:
        """Extract numerical confidence score from structured diagnosis"""
        try:
            if diagnosis_result.get("success") and "data" in diagnosis_result:
                data = diagnosis_result["data"]
                
                # Use the overall score as confidence (convert 0-100 to 0-1)
                score = data.get("score", 50)
                confidence = score / 100.0
                
                # Adjust based on risk level
                risk_level = data.get("riskLevel", "í™•ì¸ í•„ìš”")
                if risk_level == "ì •ìƒ ë‹¨ê³„":
                    confidence = min(1.0, confidence + 0.1)
                elif risk_level == "ìœ„í—˜ ë‹¨ê³„":
                    confidence = max(0.0, confidence - 0.2)
                
                return round(min(1.0, max(0.0, confidence)), 3)
            else:
                return 0.5  # Default confidence for failed analysis
                
        except Exception as e:
            self.logger.error(f"Error extracting confidence score: {str(e)}")
            return 0.5

class FormatResponseNode(BaseNode):
    """
    Node 12: Format final JSON response for API
    Aggregates all results into a structured output
    """
    
    def __init__(self):
        super().__init__(PipelineStages.FORMAT_RESPONSE)
    
    def get_system_prompt(self) -> str:
        return """You are an API response formatting specialist.
        Your task is to create a clean, structured, and developer-friendly JSON 
        response from the completed gait analysis pipeline state.
        
        The final JSON should include:
        - session_id for tracking
        - patient_info (date, height)
        - A summary of the gait analysis with key findings.
        - The full list of calculated gait_metrics.
        - The complete medical_diagnosis text.
        - Metadata about the diagnosis process (sources, model, etc.).
        - Clear recommendations for the user or clinician.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Formats the final JSON response."""
        
        if not self.validate_state_requirements(state, ["session_id", "date", "height_cm", "gait_metrics", "medical_diagnosis"]):
            return StateManager.set_error(state, "Missing required fields for final response", "validation_error")
            
        start_time = state.get("start_time", datetime.now().isoformat())
        end_time = datetime.now()
        
        # Calculate processing time if start_time is a valid ISO format string
        try:
            processing_time = (end_time - datetime.fromisoformat(start_time)).total_seconds()
        except (TypeError, ValueError):
            processing_time = 0

        # Extract key findings for the summary
        gait_metrics = state.get("gait_metrics", {})
        key_findings = []
        if gait_metrics.get("avg_walking_speed", 1.2) < 1.0:
            key_findings.append("ë³´í–‰ ì†ë„ ê°ì†Œ")
        if gait_metrics.get("stride_length_asymmetry", 0) > 5.0:
            key_findings.append("ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„± ì¦ê°€")
        if gait_metrics.get("stride_time_cv", 0) > 5.0:
            key_findings.append("ë³´í–‰ ì•ˆì •ì„± ì €í•˜ (ì‹œê°„ì  ë³€ë™ì„± ì¦ê°€)")
            
        if not key_findings:
            key_findings.append("ì „ë°˜ì ìœ¼ë¡œ ì •ìƒ ë²”ìœ„ì˜ ë³´í–‰ íŒ¨í„´")

        final_response = {
            "session_id": state.get("session_id"),
            "patient_info": {
                "analysis_date": state.get("date"),
                "height_cm": state.get("height_cm")
            },
            "gait_analysis": {
                "summary": {
                    "primary_assessment": "ì •ìƒ ë³´í–‰" if not key_findings or "ì •ìƒ" in key_findings[0] else "ë¹„ì •ìƒ ë³´í–‰ íŒ¨í„´ ê°ì§€",
                    "key_findings": key_findings,
                },
                "metrics": gait_metrics,
            },
            "medical_diagnosis": {
                "primary_diagnosis": state.get("medical_diagnosis"),
                "diagnosis_metadata": state.get("medical_diagnosis_metadata")
            },
            "recommendations": {
                "immediate_actions": ["ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”."],
                "follow_up": ["6ê°œì›” í›„ ì •ê¸°ì ì¸ ì¬í‰ê°€ ê¶Œì¥"]
            },
            "pipeline_metadata": {
                "processing_time_seconds": round(processing_time, 2),
                "metrics_record_id": state.get("metrics_record_id"),
                "diagnosis_record_id": state.get("diagnosis_record_id"),
            }
        }
        
        state['response'] = final_response
        state['processing_time'] = processing_time
        
        return state 
