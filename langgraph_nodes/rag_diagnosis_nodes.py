"""
RAG-based diagnosis nodes for LangGraph-based gait analysis pipeline
Implements 3 nodes: compose_prompt, rag_diagnosis, store_diagnosis
Uses PDF documents for medical knowledge retrieval
"""
import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

# RAG and Vector Database imports
import chromadb
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

import os
from dotenv import load_dotenv

from .base_node import BaseNode
from .graph_state import GraphState, StateManager, PipelineStages

# Load environment variables
load_dotenv()

class ComposePromptNode(BaseNode):
    """
    Node 9: Compose diagnostic prompt from gait metrics
    RAG-based 2-stage query system for evidence-based diagnosis
    """
    
    def __init__(self):
        super().__init__(PipelineStages.COMPOSE_PROMPT)
    
    def get_system_prompt(self) -> str:
        return """You are a medical data analyst specializing in gait analysis interpretation.
        
        Your task is to compose two structured RAG queries for evidence-based diagnosis:
        
        1. Normal Range Extraction Query: Extract patient-specific normal ranges from medical literature
        2. Comprehensive Diagnosis Query: Perform evidence-based pattern analysis and diagnosis
        
        Ensure all queries are medically accurate and reference-based.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Compose 2-stage RAG queries from gait metrics and patient info"""
        
        required_fields = ["gait_metrics", "height_cm", "user_id"]
        if not self.validate_state_requirements(state, required_fields):
            return StateManager.set_error(state, f"Missing required fields: {required_fields}", "validation_error")
        
        gait_metrics = state["gait_metrics"]
        height_cm = state["height_cm"]
        user_id = state["user_id"]
        gender = state.get("gender", "unknown")
        date = state.get("date", "unknown")
        session_id = state.get("session_id", "unknown")
        
        try:
            # Extract all 15 gait metrics
            metrics_data = {
                'avg_stride_time': gait_metrics.get('avg_stride_time', 0),
                'avg_stride_length': gait_metrics.get('avg_stride_length', 0),
                'avg_walking_speed': gait_metrics.get('avg_walking_speed', 0),
                'cadence': gait_metrics.get('cadence', 0),
                'stride_time_asymmetry': gait_metrics.get('stride_time_asymmetry', 0),
                'stride_length_asymmetry': gait_metrics.get('stride_length_asymmetry', 0),
                'stride_time_cv': gait_metrics.get('stride_time_cv', 0),
                'stride_length_cv': gait_metrics.get('stride_length_cv', 0),
                'walking_speed_cv': gait_metrics.get('walking_speed_cv', 0),
                'step_width': gait_metrics.get('step_width', 0),
                'gait_regularity_index': gait_metrics.get('gait_regularity_index', 0),
                'gait_stability_ratio': gait_metrics.get('gait_stability_ratio', 0),
                'stance_phase_ratio': gait_metrics.get('stance_phase_ratio', 0.6),
                'swing_phase_ratio': gait_metrics.get('swing_phase_ratio', 0.4),
                'double_support_ratio': gait_metrics.get('double_support_ratio', 0.2)
            }
            
            # Patient information (60ì„¸ ê³ ì •, APIì—ì„œ ì„±ë³„/í‚¤ ë°›ìŒ)
            patient_info = {
                'age': 60,
                'gender': gender,
                'height_cm': height_cm,
                'user_id': user_id
            }
            
            # Stage 1: Normal Range Extraction Query
            stage1_query = self._create_normal_ranges_query(patient_info, metrics_data)
            
            # Stage 2: Comprehensive Diagnosis Query (will be created after Stage 1 results)
            stage2_template = self._create_diagnosis_query_template(patient_info, metrics_data)
            
            # Update state with both queries
            state["rag_query_stage1"] = stage1_query
            state["rag_query_stage2_template"] = stage2_template
            state["patient_info"] = patient_info
            state["metrics_data"] = metrics_data
            
            self.logger.info(f"RAG 2-stage queries composed for patient: {user_id}")
            self.logger.info(f"Stage 1 query length: {len(stage1_query)} characters")
            
            return state
            
        except Exception as e:
            error_msg = f"RAG query composition failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "rag_query_composition_error")
    
    def _create_normal_ranges_query(self, patient_info: dict, metrics_data: dict) -> str:
        """Create Stage 1 RAG query for normal range extraction"""
        
        return f"""ì˜ë£Œë¬¸í—Œ ê¸°ë°˜ ì •ìƒë²”ìœ„ ì¶”ì¶œ ìš”ì²­

ã€í™˜ì ì •ë³´ã€‘
- ì—°ë ¹: {patient_info['age']}ì„¸
- ì„±ë³„: {patient_info['gender']}
- ì‹ ì¥: {patient_info['height_cm']}cm

ã€15ê°œ ë³´í–‰ ì§€í‘œ í˜„ì¬ ì¸¡ì •ê°’ã€‘
1. ë³´í­ ì‹œê°„: {metrics_data['avg_stride_time']:.2f}ì´ˆ
2. ë³´í–‰ë¥ : {metrics_data['cadence']:.0f}ê±¸ìŒ/ë¶„
3. ë³´í­ ê¸¸ì´: {metrics_data['avg_stride_length']:.2f}m
4. ë³´í–‰ ì†ë„: {metrics_data['avg_walking_speed']:.2f}m/s
5. ë³´í­ í­: {metrics_data['step_width']:.2f}m
6. ë³´í­ ì‹œê°„ ë³€ë™ì„±: {metrics_data['stride_time_cv']:.1f}%
7. ë³´í­ ê¸¸ì´ ë³€ë™ì„±: {metrics_data['stride_length_cv']:.1f}%
8. ë³´í–‰ ì†ë„ ë³€ë™ì„±: {metrics_data['walking_speed_cv']:.1f}%
9. ë³´í­ ì‹œê°„ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_time_asymmetry']:.1f}%
10. ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_length_asymmetry']:.1f}%
11. ë³´í–‰ ê·œì¹™ì„± ì§€ìˆ˜: {metrics_data['gait_regularity_index']:.3f}
12. ë³´í–‰ ì•ˆì •ì„± ë¹„ìœ¨: {metrics_data['gait_stability_ratio']:.3f}
13. ì…ê°ê¸° ë¹„ìœ¨: {metrics_data['stance_phase_ratio']:.1%}
14. ìœ ê°ê¸° ë¹„ìœ¨: {metrics_data['swing_phase_ratio']:.1%}
15. ì–‘ë°œì§€ì§€ ë¹„ìœ¨: {metrics_data['double_support_ratio']:.1%}

ã€ìš”ì²­ ì‘ë‹µ í˜•ì‹ã€‘
NORMAL_RANGES:
stride_time: [ìµœì†Œ]-[ìµœëŒ€]ì´ˆ (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
cadence: [ìµœì†Œ]-[ìµœëŒ€]ê±¸ìŒ/ë¶„ (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stride_length: [ìµœì†Œ]-[ìµœëŒ€]m (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
walking_speed: [ìµœì†Œ]-[ìµœëŒ€]m/s (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
step_width: [ìµœì†Œ]-[ìµœëŒ€]m (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stride_time_cv: <[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stride_length_cv: <[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
walking_speed_cv: <[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stride_time_asymmetry: <[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stride_length_asymmetry: <[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
gait_regularity_index: >[ìµœì†Œ] (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
gait_stability_ratio: >[ìµœì†Œ] (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
stance_phase_ratio: [ìµœì†Œ]-[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
swing_phase_ratio: [ìµœì†Œ]-[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])
double_support_ratio: [ìµœì†Œ]-[ìµœëŒ€]% (ì¶œì²˜: [ë…¼ë¬¸ëª…, ì—°ë„])

{patient_info['age']}ì„¸ {patient_info['gender']} í™˜ìì˜ ì˜ë£Œë¬¸í—Œ ê¸°ë°˜ ì •ìƒë²”ìœ„ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

    def _create_diagnosis_query_template(self, patient_info: dict, metrics_data: dict) -> str:
        """Create Stage 2 RAG query template (will be filled with Stage 1 results)"""
        
        return f"""RAG ê¸°ë°˜ ì¢…í•© ë³´í–‰ ì§„ë‹¨ ìš”ì²­

ã€í™˜ì ì •ë³´ã€‘
- ì—°ë ¹: {patient_info['age']}ì„¸
- ì„±ë³„: {patient_info['gender']}
- ì‹ ì¥: {patient_info['height_cm']}cm

ã€ì •ìƒë²”ìœ„ ê¸°ì¤€ (1ë‹¨ê³„ RAG ê²°ê³¼)ã€‘
{{NORMAL_RANGES_RESULTS}}

ã€í˜„ì¬ ì¸¡ì •ê°’ã€‘
- ë³´í­ ì‹œê°„: {metrics_data['avg_stride_time']:.2f}ì´ˆ
- ë³´í–‰ë¥ : {metrics_data['cadence']:.0f}ê±¸ìŒ/ë¶„
- ë³´í­ ê¸¸ì´: {metrics_data['avg_stride_length']:.2f}m
- ë³´í–‰ ì†ë„: {metrics_data['avg_walking_speed']:.2f}m/s
- ë³´í­ í­: {metrics_data['step_width']:.2f}m
- ë³´í­ ì‹œê°„ ë³€ë™ì„±: {metrics_data['stride_time_cv']:.1f}%
- ë³´í­ ê¸¸ì´ ë³€ë™ì„±: {metrics_data['stride_length_cv']:.1f}%
- ë³´í–‰ ì†ë„ ë³€ë™ì„±: {metrics_data['walking_speed_cv']:.1f}%
- ë³´í­ ì‹œê°„ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_time_asymmetry']:.1f}%
- ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_length_asymmetry']:.1f}%
- ë³´í–‰ ê·œì¹™ì„± ì§€ìˆ˜: {metrics_data['gait_regularity_index']:.3f}
- ë³´í–‰ ì•ˆì •ì„± ë¹„ìœ¨: {metrics_data['gait_stability_ratio']:.3f}
- ì…ê°ê¸° ë¹„ìœ¨: {metrics_data['stance_phase_ratio']:.1%}
- ìœ ê°ê¸° ë¹„ìœ¨: {metrics_data['swing_phase_ratio']:.1%}
- ì–‘ë°œì§€ì§€ ë¹„ìœ¨: {metrics_data['double_support_ratio']:.1%}

ã€ìš”ì²­ ì‘ë‹µ í˜•ì‹ã€‘
ABNORMAL_FINDINGS:
- [ì§€í‘œëª…]: [í˜„ì¬ê°’] (ì •ìƒ: [ì •ìƒë²”ìœ„]) â†’ [ì˜í•™ì  ì˜ë¯¸] (ì¶œì²˜: [ë…¼ë¬¸ëª…])

PATTERN_ANALYSIS:
- ì‹œê°„ì  íŒ¨í„´: [ë¶„ì„ ë‚´ìš©]
- ê³µê°„ì  íŒ¨í„´: [ë¶„ì„ ë‚´ìš©]
- ì•ˆì •ì„± íŒ¨í„´: [ë¶„ì„ ë‚´ìš©]
- ë¹„ëŒ€ì¹­ì„± íŒ¨í„´: [ë¶„ì„ ë‚´ìš©]

DISEASE_PATTERNS:
- íŒŒí‚¨ìŠ¨ë³‘ íŒ¨í„´ ì¼ì¹˜ë„: [0-100]% (ê·¼ê±°: [ì˜ë£Œë¬¸í—Œ])
- ë‡Œì¡¸ì¤‘ íŒ¨í„´ ì¼ì¹˜ë„: [0-100]% (ê·¼ê±°: [ì˜ë£Œë¬¸í—Œ])
- ê¸°íƒ€ ì§ˆí™˜ íŒ¨í„´: [ë¶„ì„]

FINAL_DIAGNOSIS:
- ì¢…í•© ì ìˆ˜: [0-100ì ]
- ìœ„í—˜ ìˆ˜ì¤€: [ì •ìƒ/ì£¼ì˜/ìœ„í—˜]
- ì£¼ìš” ì†Œê²¬: [í•µì‹¬ ë°œê²¬ì‚¬í•­]
- ê¶Œì¥ì‚¬í•­: [ì˜ë£Œì§„ ìƒë‹´/ì¶”ê°€ê²€ì‚¬/ìš´ë™ì¹˜ë£Œ ë“±]
- ì‹ ë¢°ë„: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ] (ê·¼ê±° ì¶©ë¶„ì„±)

ì˜ë£Œë¬¸í—Œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì§„ë‹¨ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

class RagDiagnosisNode(BaseNode):
    """
    Node 10: RAG-based medical diagnosis using PDF knowledge base
    Retrieves relevant medical information and generates diagnosis
    """
    
    def __init__(self):
        super().__init__(PipelineStages.RAG_DIAGNOSIS)
        self.vector_store = None
        self.embeddings = None
        self._initialize_rag_system()
    
    def _initialize_rag_system(self):
        """Initialize the RAG system: vector store, embeddings, and retriever."""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Define ChromaDB path relative to project root
                project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                chroma_db_path = str(project_root / "chroma_db")
                
                # Create directory if it doesn't exist
                Path(chroma_db_path).mkdir(parents=True, exist_ok=True)
                
                self.vector_store = Chroma(
                    embedding_function=self.embeddings,
                    persist_directory=chroma_db_path
                )
                
                # ğŸš€ **ìµœì í™”**: ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                existing_data_count = self._check_existing_embeddings()
                
                if existing_data_count > 0:
                    self.logger.info(f"âœ… ChromaDB ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° ë°œê²¬: {existing_data_count}ê°œ ë¬¸ì„œ")
                    self.logger.info("âš¡ PDF ì¬ë¡œë”© ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ ì„ë² ë”© ì‚¬ìš©")
                else:
                    self.logger.info("ğŸ“š ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš” - PDF ë¡œë”© ì‹œì‘")
                    self._load_medical_pdfs(Path("docs/medical_pdfs"))
                
                self.logger.info("âœ… RAG system initialized successfully.")
                return  # Success, exit retry loop
                
            except Exception as e:
                retry_count += 1
                self.logger.warning(f"RAG initialization attempt {retry_count}/{max_retries} failed: {e}")
                
                if retry_count < max_retries:
                    # Try to clean up ChromaDB directory for retry
                    project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                    chroma_db_path = str(project_root / "chroma_db")
                    try:
                        import shutil
                        if Path(chroma_db_path).exists():
                            shutil.rmtree(chroma_db_path)
                            self.logger.info(f"Cleaned ChromaDB directory for retry {retry_count + 1}")
                    except Exception as cleanup_error:
                        self.logger.warning(f"Failed to cleanup ChromaDB: {cleanup_error}")
                else:
                    self.logger.error(f"Failed to initialize RAG system after {max_retries} attempts: {e}")
                    self.vector_store = None
    
    def _check_existing_embeddings(self) -> int:
        """
        ChromaDBì— ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        
        Returns:
            int: ê¸°ì¡´ ë¬¸ì„œì˜ ê°œìˆ˜ (0ì´ë©´ ìƒˆë¡œ ì„ë² ë”© í•„ìš”)
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ ê°•ì œ ë¦¬ë¡œë”© ì˜µì…˜ ì œê³µ
            force_reload = os.getenv('RAG_FORCE_RELOAD', 'false').lower() == 'true'
            if force_reload:
                self.logger.info("ğŸ”„ RAG_FORCE_RELOAD=true - ê°•ì œë¡œ PDF ì¬ë¡œë”© ìˆ˜í–‰")
                return 0
            
            # ChromaDBì—ì„œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ë¥¼ í™•ì¸
            collection = self.vector_store._collection
            
            # ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            document_count = collection.count()
            
            if document_count > 0:
                self.logger.info(f"ğŸ’¾ ê¸°ì¡´ ChromaDB ë°ì´í„°: {document_count}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                # PDF íŒŒì¼ ë³€ê²½ ê°ì§€ (ì„ íƒì )
                pdf_changed = self._check_pdf_files_changed()
                if pdf_changed:
                    self.logger.info("ğŸ“„ PDF íŒŒì¼ ë³€ê²½ ê°ì§€ - ì¬ì„ë² ë”© ìˆ˜í–‰")
                    return 0
                
                # ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                try:
                    sample_results = collection.peek(limit=3)
                    if sample_results and 'metadatas' in sample_results:
                        for i, metadata in enumerate(sample_results['metadatas'][:3]):
                            source_file = metadata.get('source_file', 'unknown')
                            doc_type = metadata.get('document_type', 'unknown')
                            self.logger.debug(f"   ğŸ“„ ìƒ˜í”Œ {i+1}: {source_file} ({doc_type})")
                except Exception as peek_error:
                    self.logger.debug(f"ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {peek_error}")
                
                return document_count
            else:
                self.logger.info("ğŸ“­ ChromaDBê°€ ë¹„ì–´ìˆìŒ - ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš”")
                return 0
                
        except Exception as e:
            self.logger.warning(f"ê¸°ì¡´ ì„ë² ë”© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìƒˆë¡œ ì„ë² ë”©í•˜ë„ë¡ 0 ë°˜í™˜
            return 0
    
    def _check_pdf_files_changed(self) -> bool:
        """
        PDF íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì„ íƒì  ê¸°ëŠ¥)
        
        Returns:
            bool: PDF íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ True
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ PDF ë³€ê²½ ê°ì§€ í™œì„±í™” ì—¬ë¶€ í™•ì¸
            check_changes = os.getenv('RAG_CHECK_PDF_CHANGES', 'false').lower() == 'true'
            if not check_changes:
                return False
            
            docs_dir = Path("docs/medical_pdfs")
            if not docs_dir.exists():
                return False
            
            # PDF íŒŒì¼ë“¤ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            pdf_files = list(docs_dir.glob("*.pdf"))
            if not pdf_files:
                return False
            
            # ê°€ì¥ ìµœê·¼ PDF ìˆ˜ì • ì‹œê°„ ì°¾ê¸°
            latest_pdf_time = max(f.stat().st_mtime for f in pdf_files)
            
            # ChromaDB ìƒì„± ì‹œê°„ê³¼ ë¹„êµ
            project_root = Path(os.getenv('PROJECT_ROOT', '.'))
            chroma_db_path = project_root / "chroma_db" / "chroma.sqlite3"
            
            if chroma_db_path.exists():
                chroma_db_time = chroma_db_path.stat().st_mtime
                
                if latest_pdf_time > chroma_db_time:
                    self.logger.info(f"ğŸ“„ PDF íŒŒì¼ì´ ChromaDBë³´ë‹¤ ìµœì‹ : PDF={datetime.fromtimestamp(latest_pdf_time)}, DB={datetime.fromtimestamp(chroma_db_time)}")
                    return True
                else:
                    self.logger.debug(f"ğŸ“„ PDF íŒŒì¼ ë³€ê²½ ì—†ìŒ: PDF={datetime.fromtimestamp(latest_pdf_time)}, DB={datetime.fromtimestamp(chroma_db_time)}")
                    return False
            else:
                # ChromaDB íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í•„ìš”
                return True
                
        except Exception as e:
            self.logger.warning(f"PDF íŒŒì¼ ë³€ê²½ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë³€ê²½ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
            return False
    
    def _load_medical_pdfs(self, docs_dir: Path):
        """Load medical PDFs, split them, and add to the vector store."""
        try:
            from langchain_community.document_loaders import PyPDFLoader
            import os
            
            documents = []
            
            if docs_dir.exists():
                pdf_files = [f for f in os.listdir(docs_dir) if f.endswith('.pdf')]
                
                for pdf_file in pdf_files:
                    try:
                        pdf_path = docs_dir / pdf_file
                        loader = PyPDFLoader(str(pdf_path))
                        pdf_docs = loader.load()
                        
                        for doc in pdf_docs:
                            doc.metadata.update({
                                "source_file": pdf_file,
                                "document_type": "medical_literature"
                            })
                        
                        documents.extend(pdf_docs)
                        self.logger.info(f"âœ… Loaded PDF: {pdf_file} ({len(pdf_docs)} pages)")
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to load PDF {pdf_file}: {str(e)}")
                        continue
            
            if not documents:
                self.logger.warning("No PDF files loaded, using sample medical reference")
                # Add a comprehensive sample document if no PDFs are found
                sample_doc = Document(
                    page_content="""
                    ë³´í–‰ ë¶„ì„ ì„ìƒ ì°¸ê³  ìë£Œ - ê·¼ê±° ê¸°ë°˜ ê°€ì´ë“œë¼ì¸
                    
                    ê±´ê°•í•œ ì„±ì¸(20-65ì„¸) ì •ìƒ ë³´í–‰ ì§€í‘œ:
                    - ë³´í­ ì‹œê°„: 1.0-1.3ì´ˆ 
                    - ë³´í–‰ë¥ : 100-120 ê±¸ìŒ/ë¶„
                    - ë³´í­ ê¸¸ì´: 1.2-1.6m (ì‹ ì¥ ì˜ì¡´ì )
                    - ë³´í–‰ ì†ë„: 1.0-1.4 m/s 
                    - ì¢Œìš° ë¹„ëŒ€ì¹­ì„±: <5%
                    - ì‹œê°„ì  ë³€ë™ì„±: <5%
                    - ê³µê°„ì  ë³€ë™ì„±: <5%
                    
                    ë³‘ë¦¬í•™ì  ë³´í–‰ íŒ¨í„´:
                    
                    1. íŒŒí‚¨ìŠ¨ë³‘:
                    - ë³´í­ ê¸¸ì´ ê°ì†Œ (<1.0m)
                    - ë³´í–‰ë¥  ì¦ê°€ (>120 ê±¸ìŒ/ë¶„) 
                    - ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - ë³€ë™ì„± ì¦ê°€ (>10% CV)
                    - íŠ¹ì§•: ì§§ê³  ëŒë¦¬ëŠ” ê±¸ìŒ
                    
                    2. ë‡Œì¡¸ì¤‘ í¸ë§ˆë¹„:
                    - ì‹¬í•œ ë¹„ëŒ€ì¹­ì„± (>15%)
                    - í™˜ì¸¡ ë³´í­ ê¸¸ì´ ê°ì†Œ
                    - ì „ì²´ ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - íŠ¹ì§•: ì¼ì¸¡ì„± ì•½í™” íŒ¨í„´
                    
                    3. ì†Œë‡Œì„± ìš´ë™ì‹¤ì¡°:
                    - ë†’ì€ ë³´í–‰ ë³€ë™ì„± (>15% CV)
                    - ë„“ì€ ë³´í­
                    - ë¶ˆê·œì¹™í•œ ë³´í­ íƒ€ì´ë°
                    - íŠ¹ì§•: í˜‘ì¡°ì„± ë¶€ì¡±
                    """,
                    metadata={"source": "clinical_reference", "type": "sample_data"}
                )
                documents = [sample_doc]
                
            self.logger.info(f"Medical knowledge base loaded: {len(documents)} documents")
            
            # Split documents and add to vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            
            chunks = text_splitter.split_documents(documents)
            self.vector_store.add_documents(chunks)
            
            self.logger.info(f"Added {len(chunks)} chunks to vector store")
                
        except ImportError:
            self.logger.warning("PyPDFLoader not available, using sample medical data")
            sample_doc = Document(
                page_content="ê¸°ë³¸ ë³´í–‰ ë¶„ì„ ì°¸ê³  ìë£Œ: ì •ìƒ ë³´í–‰ ì†ë„ 1.0-1.4 m/s, ë³´í–‰ë¥  100-120 ê±¸ìŒ/ë¶„",
                metadata={"source": "fallback", "type": "basic_reference"}
            )
            # Just return the single doc, assuming no vector store to add to
            return [sample_doc]
    
    def get_system_prompt(self) -> str:
        return """You are a medical AI specialist in gait analysis and movement disorders.
        
        Your task is to provide evidence-based medical diagnosis using retrieved medical knowledge:
        
        Diagnostic process:
        1. Analyze retrieved medical literature
        2. Compare patient metrics with known pathological patterns
        3. Generate differential diagnoses with confidence levels
        4. Provide clinical recommendations
        5. Suggest further evaluations if needed
        
        Output requirements:
        - Primary diagnosis with rationale
        - Differential diagnoses (2-3 alternatives)
        - Confidence level (0-100%)
        - Clinical recommendations
        - Rehabilitation suggestions
        - Red flags or urgent referrals
        
        Base all conclusions on evidence from retrieved medical sources.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Execute 2-stage RAG-based medical diagnosis"""
        
        required_fields = ["rag_query_stage1", "rag_query_stage2_template", "patient_info", "metrics_data"]
        if not self.validate_state_requirements(state, required_fields):
            return StateManager.set_error(state, f"Missing required fields: {required_fields}", "validation_error")
        
        if self.vector_store is None:
            return StateManager.set_error(state, "RAG system not initialized", "rag_system_error")
        
        stage1_query = state["rag_query_stage1"]
        stage2_template = state["rag_query_stage2_template"]
        patient_info = state["patient_info"]
        metrics_data = state["metrics_data"]
        session_id = state.get("session_id", "unknown")

        try:
            # STAGE 1: Normal Range Extraction
            self.logger.info("ğŸ” Stage 1: Normal Range Extraction")
            
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 4})
            stage1_docs = retriever.get_relevant_documents(stage1_query)
            
            # Format Stage 1 retrieved knowledge
            stage1_knowledge = self._format_retrieved_knowledge(stage1_docs, "Stage1")
            source_info_stage1 = self._extract_source_info(stage1_docs)
            
            # Create Stage 1 LLM prompt
            stage1_llm_prompt = f"""ë‹¹ì‹ ì€ ì˜ë£Œë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œì—ì„œ í™˜ì ë§ì¶¤í˜• ì •ìƒë²”ìœ„ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

=== ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ===
{stage1_knowledge}

=== ì¶”ì¶œ ìš”ì²­ ===
{stage1_query}

=== ì‘ë‹µ ì§€ì¹¨ ===
1. **ë°˜ë“œì‹œ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œì˜ ë°ì´í„°ë§Œ ì‚¬ìš©**í•˜ì„¸ìš”
2. ê° ì •ìƒë²”ìœ„ë§ˆë‹¤ **êµ¬ì²´ì ì¸ ì¶œì²˜ (ë…¼ë¬¸ëª…, ì—°ë„)** ëª…ì‹œ
3. í™˜ì íŠ¹ì„± (60ì„¸, {patient_info['gender']}, {patient_info['height_cm']}cm)ì„ ê³ ë ¤í•œ ë²”ìœ„ ì œì‹œ
4. ì •í™•í•œ ìˆ˜ì¹˜ì™€ ë‹¨ìœ„ ì‚¬ìš©
5. ê·¼ê±°ê°€ ì—†ëŠ” ì§€í‘œëŠ” "ë¬¸í—Œ ê·¼ê±° ë¶€ì¡±"ìœ¼ë¡œ í‘œì‹œ

**ì •í™•íˆ ìš”ì²­ëœ NORMAL_RANGES í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.**"""

            # Get Stage 1 results
            stage1_response = self.invoke_llm(stage1_llm_prompt)
            self.logger.info(f"âœ… Stage 1 ì™„ë£Œ: {len(stage1_response)} characters")
            
            # STAGE 2: Comprehensive Diagnosis
            self.logger.info("ğŸ¥ Stage 2: Comprehensive Diagnosis")
            
            # Fill Stage 2 template with Stage 1 results
            stage2_query = stage2_template.replace("{NORMAL_RANGES_RESULTS}", stage1_response)
            
            # Retrieve documents for Stage 2
            stage2_docs = retriever.get_relevant_documents(stage2_query)
            stage2_knowledge = self._format_retrieved_knowledge(stage2_docs, "Stage2")
            source_info_stage2 = self._extract_source_info(stage2_docs)
            
            # Create Stage 2 LLM prompt
            stage2_llm_prompt = f"""ë‹¹ì‹ ì€ ì„ìƒ ë³´í–‰ ë¶„ì„ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. 1ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•œ ì •ìƒë²”ìœ„ì™€ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì§„ë‹¨ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

=== 1ë‹¨ê³„ ì¶”ì¶œ ì •ìƒë²”ìœ„ ===
{stage1_response}

=== ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ===
{stage2_knowledge}

=== ì§„ë‹¨ ìš”ì²­ ===
{stage2_query}

=== ì§„ë‹¨ ì§€ì¹¨ ===
1. **1ë‹¨ê³„ ì •ìƒë²”ìœ„ì™€ ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œë§Œ ì‚¬ìš©**í•˜ì—¬ ì§„ë‹¨
2. ëª¨ë“  íŒë‹¨ì— **êµ¬ì²´ì ì¸ ì˜ë£Œë¬¸í—Œ ì¶œì²˜** ëª…ì‹œ
3. í™˜ì ì¸¡ì •ê°’ì„ ì •ìƒë²”ìœ„ì™€ ì •í™•íˆ ë¹„êµ
4. ì˜í•™ì  íŒ¨í„´ ë¶„ì„ì€ ë¬¸í—Œ ê·¼ê±° ê¸°ë°˜ìœ¼ë¡œë§Œ ìˆ˜í–‰
5. ì‹ ë¢°ë„ëŠ” ë¬¸í—Œ ì¶©ë¶„ì„±ê³¼ ì¼ì¹˜ì„±ìœ¼ë¡œ í‰ê°€

**ì •í™•íˆ ìš”ì²­ëœ ì‘ë‹µ í˜•ì‹ (ABNORMAL_FINDINGS, PATTERN_ANALYSIS, DISEASE_PATTERNS, FINAL_DIAGNOSIS)ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.**"""

            # Get Stage 2 results
            stage2_response = self.invoke_llm(stage2_llm_prompt)
            self.logger.info(f"âœ… Stage 2 ì™„ë£Œ: {len(stage2_response)} characters")
            
            # Parse RAG responses and generate API-compatible structure
            structured_diagnosis = self._generate_rag_based_diagnosis(
                state, stage1_response, stage2_response, 
                source_info_stage1 + source_info_stage2
            )
            
            # Update state with results
            state["medical_diagnosis"] = structured_diagnosis
            state["diagnosis_result"] = structured_diagnosis
            state["rag_stage1_response"] = stage1_response
            state["rag_stage2_response"] = stage2_response
            
            # Metadata
            state["medical_diagnosis_metadata"] = {
                "session_id": session_id,
                "diagnosis_timestamp": datetime.now().isoformat(),
                "rag_stage1_sources": len(stage1_docs),
                "rag_stage2_sources": len(stage2_docs),
                "total_sources": len(stage1_docs) + len(stage2_docs),
                "knowledge_base_used": "medical_pdfs",
                "stage1_response_length": len(stage1_response),
                "stage2_response_length": len(stage2_response),
                "source_documents": source_info_stage1 + source_info_stage2
            }
            
            self.logger.info(f"ğŸ¯ RAG 2-stage diagnosis completed for patient: {patient_info['user_id']}")
            
            return state
            
        except Exception as e:
            error_msg = f"RAG 2-stage diagnosis failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "rag_diagnosis_error")
    
    def _format_retrieved_knowledge(self, docs: list, stage_name: str) -> str:
        """Format retrieved documents for LLM prompt"""
        
        knowledge = ""
        for i, doc in enumerate(docs, 1):
            source_file = doc.metadata.get('source_file', 'unknown_source')
            doc_type = doc.metadata.get('document_type', 'unknown_type')
            page_num = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
            
            content_snippet = doc.page_content.strip()
            if len(content_snippet) > 500:
                content_snippet = content_snippet[:500] + "..."
            
            knowledge += f"""
=== ì°¸ì¡°ë¬¸í—Œ {i} ({stage_name}): {source_file} ===
ë¬¸ì„œìœ í˜•: {doc_type}
í˜ì´ì§€: {page_num}
ê´€ë ¨ë‚´ìš©:
{content_snippet}

"""
        return knowledge
    
    def _extract_source_info(self, docs: list) -> list:
        """Extract source information from documents"""
        
        source_info = []
        for i, doc in enumerate(docs, 1):
            source_info.append({
                "ë²ˆí˜¸": i,
                "íŒŒì¼ëª…": doc.metadata.get('source_file', 'unknown_source'),
                "ë¬¸ì„œìœ í˜•": doc.metadata.get('document_type', 'unknown_type'),
                "í˜ì´ì§€": doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                "ë‚´ìš©ê¸¸ì´": len(doc.page_content)
            })
        return source_info
    
    def _generate_rag_based_diagnosis(self, state: GraphState, stage1_response: str, stage2_response: str, source_info: list) -> dict:
        """Generate structured JSON diagnosis from RAG responses"""
        
        try:
            patient_info = state["patient_info"]
            metrics_data = state["metrics_data"]
            
            # Parse Stage 1: Normal Ranges
            normal_ranges = self._parse_normal_ranges(stage1_response)
            
            # Parse Stage 2: Comprehensive Diagnosis
            abnormal_findings = self._parse_section(stage2_response, "ABNORMAL_FINDINGS")
            pattern_analysis = self._parse_section(stage2_response, "PATTERN_ANALYSIS")
            disease_patterns = self._parse_section(stage2_response, "DISEASE_PATTERNS")
            final_diagnosis = self._parse_section(stage2_response, "FINAL_DIAGNOSIS")
            
            # Extract structured data from parsed sections
            indicators = self._create_rag_indicators(abnormal_findings, normal_ranges, metrics_data)
            diseases = self._create_rag_diseases(disease_patterns)
            score = self._extract_rag_score(final_diagnosis)
            status = self._extract_rag_status(final_diagnosis)
            risk_level = self._extract_rag_risk_level(final_diagnosis)
            
            # Create detailed report
            detailed_report = {
                "title": "RAG ê¸°ë°˜ ë³´í–‰ ë¶„ì„ ê²°ê³¼",
                "content": self._format_rag_detailed_content(final_diagnosis, pattern_analysis),
                "normalRanges": normal_ranges,
                "abnormalFindings": abnormal_findings,
                "patternAnalysis": pattern_analysis,
                "diseasePatterns": disease_patterns,
                "confidence": self._extract_rag_confidence(final_diagnosis),
                "sourceDocuments": source_info
            }
            
            # Create API-compatible structured response
            structured_result = {
                "success": True,
                "data": {
                    "userId": patient_info["user_id"],
                    "score": score,
                    "status": status,
                    "riskLevel": risk_level,
                    "analyzedAt": datetime.now().isoformat(),
                    "indicators": indicators,
                    "diseases": diseases,
                    "detailedReport": detailed_report
                }
            }
            
            self.logger.info(f"âœ… RAG-based diagnosis generated: score={score}, status={status}")
            return structured_result
            
        except Exception as e:
            self.logger.error(f"Failed to generate RAG-based diagnosis: {str(e)}")
            # Return fallback structure
            return {
                "success": False,
                "data": {
                    "userId": state.get("patient_info", {}).get("user_id", "unknown"),
                    "score": 75,
                    "status": "RAG ë¶„ì„ ì™„ë£Œ",
                    "riskLevel": "í™•ì¸ í•„ìš”",
                    "analyzedAt": datetime.now().isoformat(),
                    "indicators": [],
                    "diseases": [],
                    "detailedReport": {
                        "title": "RAG ì§„ë‹¨ ì˜¤ë¥˜",
                        "content": "RAG ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.",
                        "error": str(e)
                    }
                }
            }
    
    def _parse_normal_ranges(self, stage1_response: str) -> dict:
        """Parse normal ranges from Stage 1 RAG response"""
        
        normal_ranges = {}
        try:
            lines = stage1_response.strip().split('\n')
            for line in lines:
                if ':' in line and any(keyword in line.lower() for keyword in ['stride_time', 'cadence', 'walking_speed', 'step_width']):
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        normal_ranges[key] = value
        except Exception as e:
            self.logger.warning(f"Failed to parse normal ranges: {e}")
        
        return normal_ranges
    
    def _parse_section(self, stage2_response: str, section_name: str) -> str:
        """Parse specific section from Stage 2 RAG response"""
        
        try:
            lines = stage2_response.strip().split('\n')
            section_content = ""
            in_section = False
            
            for line in lines:
                if line.strip().startswith(f"{section_name}:"):
                    in_section = True
                    continue
                elif line.strip().startswith(("ABNORMAL_FINDINGS:", "PATTERN_ANALYSIS:", "DISEASE_PATTERNS:", "FINAL_DIAGNOSIS:")):
                    if in_section:
                        break
                    in_section = False
                elif in_section:
                    section_content += line + "\n"
            
            return section_content.strip()
        except Exception as e:
            self.logger.warning(f"Failed to parse section {section_name}: {e}")
            return ""
    
    def _create_rag_indicators(self, abnormal_findings: str, normal_ranges: dict, metrics_data: dict) -> list:
        """Create indicators array from RAG analysis"""
        
        indicators = []
        try:
            # Parse abnormal findings to create indicators
            findings_lines = abnormal_findings.split('\n')
            
            for line in findings_lines:
                if '-' in line and ':' in line:
                    # Extract indicator info from RAG findings
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        indicator_name = parts[0].strip().replace('-', '').strip()
                        analysis = parts[1].strip()
                        
                        # Map to metrics data
                        indicator_id = self._map_indicator_name_to_id(indicator_name)
                        if indicator_id:
                            value = self._get_metric_value(indicator_id, metrics_data)
                            status = self._determine_rag_status(analysis)
                            
            indicators.append({
                                "id": indicator_id,
                                "name": indicator_name,
                                "value": value,
                                "status": status,
                                "description": f"RAG ë¶„ì„: {analysis[:100]}...",
                                "result": f"RAG ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ {status}ì…ë‹ˆë‹¤!"
                            })
            
            # Ensure minimum indicators if parsing fails
            if len(indicators) < 3:
                indicators.extend(self._create_fallback_indicators(metrics_data))
            
        except Exception as e:
            self.logger.warning(f"Failed to create RAG indicators: {e}")
            indicators = self._create_fallback_indicators(metrics_data)
            
        return indicators[:5]  # Limit to 5 indicators for API compatibility
    
    def _create_rag_diseases(self, disease_patterns: str) -> list:
        """Create diseases array from RAG disease pattern analysis"""
        
        diseases = []
        try:
            lines = disease_patterns.split('\n')
            
            for line in lines:
                if 'íŒ¨í„´ ì¼ì¹˜ë„' in line and '%' in line:
                    # Extract disease info
                    if 'íŒŒí‚¨ìŠ¨ë³‘' in line:
                        probability = self._extract_percentage(line)
                        status = "ì •ìƒ ë²”ìœ„" if probability < 30 else "ì£¼ì˜ í•„ìš”" if probability < 60 else "ìœ„í—˜ ë²”ìœ„"
            diseases.append({
                "id": "parkinson",
                "name": "íŒŒí‚¨ìŠ¨ë³‘",
                            "probability": probability,
                            "status": status,
                            "trend": "stable"
                        })
                    elif 'ë‡Œì¡¸ì¤‘' in line:
                        probability = self._extract_percentage(line)
                        status = "ì •ìƒ ë²”ìœ„" if probability < 25 else "ì£¼ì˜ í•„ìš”" if probability < 55 else "ìœ„í—˜ ë²”ìœ„"
            diseases.append({
                "id": "stroke", 
                "name": "ë‡Œì¡¸ì¤‘",
                            "probability": probability,
                            "status": status,
                            "trend": "stable"
                        })
            
            # Ensure minimum diseases if parsing fails
            if len(diseases) == 0:
                diseases = [
                    {"id": "parkinson", "name": "íŒŒí‚¨ìŠ¨ë³‘", "probability": 25, "status": "ì •ìƒ ë²”ìœ„", "trend": "stable"},
                    {"id": "stroke", "name": "ë‡Œì¡¸ì¤‘", "probability": 20, "status": "ì •ìƒ ë²”ìœ„", "trend": "stable"}
                ]
            
        except Exception as e:
            self.logger.warning(f"Failed to create RAG diseases: {e}")
            diseases = [
                {"id": "parkinson", "name": "íŒŒí‚¨ìŠ¨ë³‘", "probability": 25, "status": "ì •ìƒ ë²”ìœ„", "trend": "stable"},
                {"id": "stroke", "name": "ë‡Œì¡¸ì¤‘", "probability": 20, "status": "ì •ìƒ ë²”ìœ„", "trend": "stable"}
            ]
            
        return diseases
    
    def _extract_rag_score(self, final_diagnosis: str) -> int:
        """Extract score from final diagnosis"""
        
        try:
            lines = final_diagnosis.split('\n')
            for line in lines:
                if 'ì¢…í•© ì ìˆ˜' in line or 'ì ìˆ˜' in line:
                    # Extract number
                    import re
                    numbers = re.findall(r'\d+', line)
                    if numbers:
                        score = int(numbers[0])
                        return max(0, min(100, score))  # Ensure 0-100 range
        except Exception as e:
            self.logger.warning(f"Failed to extract RAG score: {e}")
        
        return 75  # Default score
    
    def _extract_rag_status(self, final_diagnosis: str) -> str:
        """Extract status from final diagnosis"""
        
        try:
            if 'ì •ìƒ' in final_diagnosis:
                return "ë³´í–‰ ì•ˆì •ì "
            elif 'ì£¼ì˜' in final_diagnosis:
                return "ë³´í–‰ ì£¼ì˜ í•„ìš”"
            elif 'ìœ„í—˜' in final_diagnosis:
                return "ë³´í–‰ ë¶ˆì•ˆì •"
        except Exception as e:
            self.logger.warning(f"Failed to extract RAG status: {e}")
        
        return "RAG ë¶„ì„ ì™„ë£Œ"
    
    def _extract_rag_risk_level(self, final_diagnosis: str) -> str:
        """Extract risk level from final diagnosis"""
        
        try:
            if 'ì •ìƒ ë‹¨ê³„' in final_diagnosis:
                return "ì •ìƒ ë‹¨ê³„"
            elif 'ì£¼ì˜ ë‹¨ê³„' in final_diagnosis:
                return "ì£¼ì˜ ë‹¨ê³„"
            elif 'ìœ„í—˜ ë‹¨ê³„' in final_diagnosis:
                return "ìœ„í—˜ ë‹¨ê³„"
        except Exception as e:
            self.logger.warning(f"Failed to extract RAG risk level: {e}")
        
        return "í™•ì¸ í•„ìš”"
    
    def _extract_rag_confidence(self, final_diagnosis: str) -> str:
        """Extract confidence from final diagnosis"""
        
        try:
            if 'ì‹ ë¢°ë„' in final_diagnosis:
                if 'ë†’ìŒ' in final_diagnosis:
                    return "ë†’ìŒ"
                elif 'ë³´í†µ' in final_diagnosis:
                    return "ë³´í†µ"
                elif 'ë‚®ìŒ' in final_diagnosis:
                    return "ë‚®ìŒ"
        except Exception as e:
            self.logger.warning(f"Failed to extract RAG confidence: {e}")
        
        return "ë³´í†µ"
    
    def _format_rag_detailed_content(self, final_diagnosis: str, pattern_analysis: str) -> str:
        """Format detailed content from RAG responses"""
        
        content = f"""RAG ê¸°ë°˜ ì¢…í•© ë³´í–‰ ë¶„ì„ ê²°ê³¼

ã€ìµœì¢… ì§„ë‹¨ã€‘
{final_diagnosis}

ã€íŒ¨í„´ ë¶„ì„ã€‘
{pattern_analysis}

ì´ ë¶„ì„ì€ ì˜ë£Œë¬¸í—Œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œì„ í†µí•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."""
        
        return content
    
    def _map_indicator_name_to_id(self, name: str) -> str:
        """Map Korean indicator name to ID"""
        
        mapping = {
            "ë³´í­ ì‹œê°„": "stride-time",
            "ì–‘ë°œ ì§€ì§€": "double-support", 
            "ë³´í­ ì°¨ì´": "stride-difference",
            "ë³´í–‰ ì†ë„": "walking-speed",
            "ì…ê°ê¸°": "stance-phase"
        }
        
        for key, value in mapping.items():
            if key in name:
                return value
        
        return None
    
    def _get_metric_value(self, indicator_id: str, metrics_data: dict) -> str:
        """Get formatted metric value"""
        
        try:
            if indicator_id == "stride-time":
                return f"{metrics_data.get('avg_stride_time', 1.0):.2f}ì´ˆ"
            elif indicator_id == "double-support":
                return f"{metrics_data.get('double_support_ratio', 0.2) * 100:.1f}%"
            elif indicator_id == "stride-difference":
                return f"{metrics_data.get('stride_length_asymmetry', 0.0):.1f}%"
            elif indicator_id == "walking-speed":
                return f"{metrics_data.get('avg_walking_speed', 1.2):.1f}m/s"
            elif indicator_id == "stance-phase":
                return f"{metrics_data.get('stance_phase_ratio', 0.6):.1%}"
        except Exception:
            pass
        
        return "N/A"
    
    def _determine_rag_status(self, analysis: str) -> str:
        """Determine status from RAG analysis text"""
        
        analysis_lower = analysis.lower()
        if 'ì •ìƒ' in analysis_lower:
            return "normal"
        elif 'ì£¼ì˜' in analysis_lower or 'ìœ„í—˜' in analysis_lower:
            return "warning"
            else:
            return "normal"
    
    def _extract_percentage(self, text: str) -> int:
        """Extract percentage from text"""
        
        import re
        percentages = re.findall(r'(\d+)%', text)
        if percentages:
            return int(percentages[0])
        return 25  # Default
    
    def _create_fallback_indicators(self, metrics_data: dict) -> list:
        """Create fallback indicators when RAG parsing fails"""
        
        return [
            {
                "id": "stride-time",
                "name": "ë³´í­ ì‹œê°„",
                "value": f"{metrics_data.get('avg_stride_time', 1.0):.2f}ì´ˆ",
                "status": "normal",
                "description": "RAG ë¶„ì„ ê¸°ë°˜ ë³´í­ ì‹œê°„ í‰ê°€",
                "result": "RAG ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ"
            },
            {
                "id": "walking-speed", 
                "name": "ë³´í–‰ ì†ë„",
                "value": f"{metrics_data.get('avg_walking_speed', 1.2):.1f}m/s",
                "status": "normal",
                "description": "RAG ë¶„ì„ ê¸°ë°˜ ë³´í–‰ ì†ë„ í‰ê°€",
                "result": "RAG ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ"
            }
        ]

    # ğŸ—‘ï¸ ì„ì˜ ê¸°ì¤€ ë©”ì„œë“œë“¤ ì œê±°ë¨ - RAG ê¸°ë°˜ìœ¼ë¡œ ì™„ì „ ëŒ€ì²´
    # ì œê±°ëœ ë©”ì„œë“œë“¤:
    # - _generate_indicators: RAG ê¸°ë°˜ _create_rag_indicatorsë¡œ ëŒ€ì²´
    # - _calculate_disease_probabilities: RAG ê¸°ë°˜ _create_rag_diseasesë¡œ ëŒ€ì²´  
    # - _calculate_overall_assessment: RAG ê¸°ë°˜ ì ìˆ˜ ì¶”ì¶œë¡œ ëŒ€ì²´
    # - _assess_stride_time, _assess_double_support, _assess_stride_asymmetry: RAG ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
    # - _assess_walking_speed, _assess_stance_phase_ratio: RAG ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
    # - _calculate_parkinson_risk, _calculate_stroke_risk: RAG íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
    # - _assess_disease_risk: RAG ì§ˆë³‘ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
    
    # ğŸ’¡ ìƒˆë¡œìš´ RAG ê¸°ë°˜ ì‹œìŠ¤í…œì´ ëª¨ë“  ì„ì˜ ê¸°ì¤€ì„ ì˜ë£Œë¬¸í—Œ ê·¼ê±°ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤!

class StoreDiagnosisNode(BaseNode):
    """
    Node 11: Store medical diagnosis to Supabase
    Saves RAG-generated diagnosis and recommendations
    """
    
    def __init__(self):
        super().__init__(PipelineStages.STORE_DIAGNOSIS)
    
    def get_system_prompt(self) -> str:
        return """You are a medical records management specialist.
        
        Your task is to store AI-generated medical diagnoses in the database:
        
        Storage requirements:
        - Store complete diagnosis with metadata
        - Link to corresponding gait metrics record
        - Include confidence levels and sources
        - Maintain audit trail for medical decisions
        
        Database validation:
        - Verify diagnosis completeness
        - Check foreign key relationships
        - Validate JSON structure
        
        Provide confirmation of successful storage with record linkage.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Store medical diagnosis to Supabase database"""
        
        required = ["medical_diagnosis", "medical_diagnosis_metadata", "user_id", "session_id"]
        if not self.validate_state_requirements(state, required):
            return StateManager.set_error(state, f"Missing required fields: {required}", "validation_error")
        
        diagnosis_result = state["medical_diagnosis"]
        diagnosis_metadata = state["medical_diagnosis_metadata"]
        user_id = state["user_id"]
        session_id = state.get("session_id")

        try:
            from supabase import create_client
            import json
            
            # Use Service Role key to bypass RLS policies
            supabase_url = os.getenv('SUPABASE_URL')
            supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
            
            if not supabase_url or not supabase_key:
                raise ValueError("Supabase credentials not found in environment variables")
                
            supabase = create_client(supabase_url, supabase_key)
            
            # Handle both old format (string) and new format (structured JSON)
            if isinstance(diagnosis_result, dict) and diagnosis_result.get("success") is not None:
                # New structured JSON format
                diagnosis_content = diagnosis_result
                confidence_score = self._extract_confidence_score(diagnosis_result)
            else:
                # Legacy text format - convert to structured format for compatibility
                diagnosis_content = {
                    "success": True,
                    "data": {
                        "userId": user_id,
                        "score": 50,  # Default score for legacy data
                        "status": "Legacy ì§„ë‹¨",
                        "riskLevel": "í™•ì¸ í•„ìš”",
                        "analyzedAt": diagnosis_metadata.get('diagnosis_timestamp', datetime.now().isoformat()),
                        "indicators": [],
                        "diseases": [],
                        "detailedReport": {
                            "title": "Legacy ì§„ë‹¨ ê²°ê³¼",
                            "content": str(diagnosis_result)[:500]
                        }
                    },
                    # Add legacy metadata for compatibility
                    "legacy_metadata": {
                        'diagnosis_text': str(diagnosis_result),
                        'diagnosis_timestamp': diagnosis_metadata.get('diagnosis_timestamp'),
                        'knowledge_base_used': diagnosis_metadata.get('knowledge_base_used'),
                        'prompt_length': diagnosis_metadata.get('prompt_length'),
                        'response_length': diagnosis_metadata.get('response_length'),
                        'diagnosis_method': 'RAG_PDF_BASED',
                        'ai_model_used': os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
                        'confidence_level': 'AI_GENERATED'
                    }
                }
                confidence_score = None

            storage_data = {
                'session_id': session_id,
                'user_id': user_id,
                'diagnosis_json': diagnosis_content,  # Store as JSONB directly
                'retrieved_papers': diagnosis_metadata.get('retrieved_sources', 0),
                'ai_model_used': os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
                'confidence_score': confidence_score,  # Numerical confidence score
                'knowledge_base_version': 'medical_pdfs_v1',
                'processing_time_seconds': None  # Could be extracted from metadata if available
            }

            # Store to Supabase in 'gait_diagnosis' table
            result = supabase.table('gait_diagnosis').insert(storage_data).execute()
            
            if result.data:
                stored_record = result.data[0]
                record_id = stored_record.get('id')
                state["diagnosis_record_id"] = record_id
                state["diagnosis_stored"] = True
                self.logger.info(f"Medical diagnosis stored successfully: Record ID {record_id}")
                return state
            else:
                error_info = getattr(result, 'error', 'Unknown error')
                return StateManager.set_error(state, f"Failed to store medical diagnosis: {error_info}", "storage_error")
            
        except Exception as e:
            error_msg = f"Medical diagnosis storage failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "storage_execution_error")
    
    def _extract_confidence_score(self, diagnosis_result: dict) -> float:
        """Extract numerical confidence score from structured diagnosis"""
        try:
            if diagnosis_result.get("success") and "data" in diagnosis_result:
                data = diagnosis_result["data"]
                
                # Use the overall score as confidence (convert 0-100 to 0-1)
                score = data.get("score", 50)
                confidence = score / 100.0
                
                # Adjust based on risk level
                risk_level = data.get("riskLevel", "í™•ì¸ í•„ìš”")
                if risk_level == "ì •ìƒ ë‹¨ê³„":
                    confidence = min(1.0, confidence + 0.1)
                elif risk_level == "ìœ„í—˜ ë‹¨ê³„":
                    confidence = max(0.0, confidence - 0.2)
                
                return round(min(1.0, max(0.0, confidence)), 3)
            else:
                return 0.5  # Default confidence for failed analysis
                
        except Exception as e:
            self.logger.error(f"Error extracting confidence score: {str(e)}")
            return 0.5

class FormatResponseNode(BaseNode):
    """
    Node 12: Format final JSON response for API
    Aggregates all results into a structured output
    """
    
    def __init__(self):
        super().__init__(PipelineStages.FORMAT_RESPONSE)
    
    def get_system_prompt(self) -> str:
        return """You are an API response formatting specialist.
        Your task is to create a clean, structured, and developer-friendly JSON 
        response from the completed gait analysis pipeline state.
        
        The final JSON should include:
        - session_id for tracking
        - patient_info (date, height)
        - A summary of the gait analysis with key findings.
        - The full list of calculated gait_metrics.
        - The complete medical_diagnosis text.
        - Metadata about the diagnosis process (sources, model, etc.).
        - Clear recommendations for the user or clinician.
        """
    
    def execute(self, state: GraphState) -> GraphState:
        """Formats the final JSON response."""
        
        if not self.validate_state_requirements(state, ["session_id", "date", "height_cm", "gait_metrics", "medical_diagnosis"]):
            return StateManager.set_error(state, "Missing required fields for final response", "validation_error")
            
        start_time = state.get("start_time", datetime.now().isoformat())
        end_time = datetime.now()
        
        # Calculate processing time if start_time is a valid ISO format string
        try:
            processing_time = (end_time - datetime.fromisoformat(start_time)).total_seconds()
        except (TypeError, ValueError):
            processing_time = 0

        # Extract key findings for the summary
        gait_metrics = state.get("gait_metrics", {})
        key_findings = []
        if gait_metrics.get("avg_walking_speed", 1.2) < 1.0:
            key_findings.append("ë³´í–‰ ì†ë„ ê°ì†Œ")
        if gait_metrics.get("stride_length_asymmetry", 0) > 5.0:
            key_findings.append("ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„± ì¦ê°€")
        if gait_metrics.get("stride_time_cv", 0) > 5.0:
            key_findings.append("ë³´í–‰ ì•ˆì •ì„± ì €í•˜ (ì‹œê°„ì  ë³€ë™ì„± ì¦ê°€)")
            
        if not key_findings:
            key_findings.append("ì „ë°˜ì ìœ¼ë¡œ ì •ìƒ ë²”ìœ„ì˜ ë³´í–‰ íŒ¨í„´")

        final_response = {
            "session_id": state.get("session_id"),
            "patient_info": {
                "analysis_date": state.get("date"),
                "height_cm": state.get("height_cm")
            },
            "gait_analysis": {
                "summary": {
                    "primary_assessment": "ì •ìƒ ë³´í–‰" if not key_findings or "ì •ìƒ" in key_findings[0] else "ë¹„ì •ìƒ ë³´í–‰ íŒ¨í„´ ê°ì§€",
                    "key_findings": key_findings,
                },
                "metrics": gait_metrics,
            },
            "medical_diagnosis": {
                "primary_diagnosis": state.get("medical_diagnosis"),
                "diagnosis_metadata": state.get("medical_diagnosis_metadata")
            },
            "recommendations": {
                "immediate_actions": ["ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”."],
                "follow_up": ["6ê°œì›” í›„ ì •ê¸°ì ì¸ ì¬í‰ê°€ ê¶Œì¥"]
            },
            "pipeline_metadata": {
                "processing_time_seconds": round(processing_time, 2),
                "metrics_record_id": state.get("metrics_record_id"),
                "diagnosis_record_id": state.get("diagnosis_record_id"),
            }
        }
        
        state['response'] = final_response
        state['processing_time'] = processing_time
        
        return state 
