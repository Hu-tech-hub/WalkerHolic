"""
RAG-based diagnosis nodes for LangGraph-based gait analysis pipeline
Implements 3 nodes: compose_prompt, rag_diagnosis, store_diagnosis
Uses PDF documents for medical knowledge retrieval
"""
import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

# RAG and Vector Database imports
import chromadb
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

import os
from dotenv import load_dotenv

from .base_node import BaseNode
from .graph_state import GraphState, StateManager, PipelineStages

# Load environment variables
load_dotenv()

class ComposePromptNode(BaseNode):
    """
    Node 9: Compose diagnostic prompt from gait metrics
    Simple 2-stage query system: Stage 1 (Individual Indicators) + Stage 2 (Overall Assessment)
    """
    
    def __init__(self):
        super().__init__(PipelineStages.COMPOSE_PROMPT)
    
    def execute(self, state: GraphState) -> GraphState:
        """Compose 2-stage RAG queries from gait metrics and patient info"""
        
        required_fields = ["gait_metrics", "height_cm", "user_id"]
        if not self.validate_state_requirements(state, required_fields):
            return StateManager.set_error(state, f"Missing required fields: {required_fields}", "validation_error")
        
        gait_metrics = state["gait_metrics"]
        height_cm = state["height_cm"]
        user_id = state["user_id"]
        gender = state.get("gender", "unknown")
        
        try:
            # Extract all 15 gait metrics
            metrics_data = {
                'avg_stride_time': gait_metrics.get('avg_stride_time', 0),
                'avg_stride_length': gait_metrics.get('avg_stride_length', 0),
                'avg_walking_speed': gait_metrics.get('avg_walking_speed', 0),
                'cadence': gait_metrics.get('cadence', 0),
                'stride_time_asymmetry': gait_metrics.get('stride_time_asymmetry', 0),
                'stride_length_asymmetry': gait_metrics.get('stride_length_asymmetry', 0),
                'stride_time_cv': gait_metrics.get('stride_time_cv', 0),
                'stride_length_cv': gait_metrics.get('stride_length_cv', 0),
                'walking_speed_cv': gait_metrics.get('walking_speed_cv', 0),
                'step_width': gait_metrics.get('step_width', 0),
                'gait_regularity_index': gait_metrics.get('gait_regularity_index', 0),
                'gait_stability_ratio': gait_metrics.get('gait_stability_ratio', 0),
                'stance_phase_ratio': gait_metrics.get('stance_phase_ratio', 0.6),
                'swing_phase_ratio': gait_metrics.get('swing_phase_ratio', 0.4),
                'double_support_ratio': gait_metrics.get('double_support_ratio', 0.2)
            }
            
            # Patient information
            patient_info = {
                'age': 60,  # Fixed age
                'gender': gender,
                'height_cm': height_cm,
                'user_id': user_id
            }
            
            # Stage 1: Individual Indicator Analysis Query
            stage1_query = self._create_stage1_query(patient_info, metrics_data)
            
            # Stage 2: Overall Assessment Query Template
            stage2_template = self._create_stage2_template(patient_info, metrics_data)
            
            # Update state
            state["rag_query_stage1"] = stage1_query
            state["rag_query_stage2_template"] = stage2_template
            state["patient_info"] = patient_info
            state["metrics_data"] = metrics_data
            
            self.logger.info(f"RAG 2-stage queries composed for patient: {user_id}")
            
            return state
            
        except Exception as e:
            error_msg = f"RAG query composition failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "rag_query_composition_error")
    
    def _create_stage1_query(self, patient_info: dict, metrics_data: dict) -> str:
        """Create Stage 1 query for individual indicator analysis"""
        
        return f"""60ì„¸ {patient_info['gender']} í™˜ìì˜ ë³´í–‰ ì§€í‘œë³„ ì •ìƒë²”ìœ„ ë° ê°œë³„ ì§„ë‹¨ ìš”ì²­

ã€í™˜ì ì •ë³´ã€‘ì—°ë ¹: 60ì„¸, ì„±ë³„: {patient_info['gender']}, ì‹ ì¥: {patient_info['height_cm']}cm

ã€ì¸¡ì •ëœ ë³´í–‰ ì§€í‘œ 15ê°œã€‘
1. ë³´í­ ì‹œê°„: {metrics_data['avg_stride_time']:.2f}ì´ˆ
2. ë³´í–‰ë¥ : {metrics_data['cadence']:.0f}ê±¸ìŒ/ë¶„
3. ë³´í­ ê¸¸ì´: {metrics_data['avg_stride_length']:.2f}m
4. ë³´í–‰ ì†ë„: {metrics_data['avg_walking_speed']:.2f}m/s
5. ë³´í­ í­: {metrics_data['step_width']:.2f}m
6. ë³´í­ ì‹œê°„ ë³€ë™ì„±: {metrics_data['stride_time_cv']:.1f}%
7. ë³´í­ ê¸¸ì´ ë³€ë™ì„±: {metrics_data['stride_length_cv']:.1f}%
8. ë³´í–‰ ì†ë„ ë³€ë™ì„±: {metrics_data['walking_speed_cv']:.1f}%
9. ë³´í­ ì‹œê°„ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_time_asymmetry']:.1f}%
10. ë³´í­ ê¸¸ì´ ë¹„ëŒ€ì¹­ì„±: {metrics_data['stride_length_asymmetry']:.1f}%
11. ë³´í–‰ ê·œì¹™ì„± ì§€ìˆ˜: {metrics_data['gait_regularity_index']:.3f}
12. ë³´í–‰ ì•ˆì •ì„± ë¹„ìœ¨: {metrics_data['gait_stability_ratio']:.3f}
13. ì…ê°ê¸° ë¹„ìœ¨: {metrics_data['stance_phase_ratio']:.1%}
14. ìœ ê°ê¸° ë¹„ìœ¨: {metrics_data['swing_phase_ratio']:.1%}
15. ì–‘ë°œì§€ì§€ ë¹„ìœ¨: {metrics_data['double_support_ratio']:.1%}

ê° ì§€í‘œë³„ë¡œ ì •ìƒë²”ìœ„ë¥¼ ì°¾ê³  í˜„ì¬ ì¸¡ì •ê°’ê³¼ ë¹„êµí•˜ì—¬ ê°œë³„ ì§„ë‹¨ì„ í•´ì£¼ì„¸ìš”."""

    def _create_stage2_template(self, patient_info: dict, metrics_data: dict) -> str:
        """Create Stage 2 template for overall assessment"""
        
        return f"""Stage 1 ì§€í‘œë³„ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í‰ê°€ ë° ë§ì¶¤ ê¶Œì¥ì‚¬í•­ ì‘ì„±

ã€í™˜ì ì •ë³´ã€‘60ì„¸ {patient_info['gender']}, ì‹ ì¥ {patient_info['height_cm']}cm

ã€Stage 1 ê°œë³„ ì§€í‘œ ì§„ë‹¨ ê²°ê³¼ã€‘
{{STAGE1_RESULTS}}

ã€ìš”ì²­ì‚¬í•­ã€‘
1. ì§ˆë³‘ ìœ„í—˜ë„ í‰ê°€ (íŒŒí‚¨ìŠ¨ë³‘, ë‡Œì¡¸ì¤‘ ë“±)
2. ì˜ì‚¬ê°€ ë…¸ì¸ì—ê²Œ ì„¤ëª…í•˜ë“¯ ì¹œê·¼í•˜ê³  ì‰¬ìš´ í†¤ìœ¼ë¡œ ì¢…í•© ì†Œê²¬ ì‘ì„±
3. ì§‘ì—ì„œ ì‰½ê²Œ í•  ìˆ˜ ìˆëŠ” ë§ì¶¤ ìš´ë™ ê¶Œì¥ì‚¬í•­ ì œì‹œ

ì˜ë£Œë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì§„ë‹¨ì„ í•´ì£¼ì„¸ìš”."""

class RagDiagnosisNode(BaseNode):
    """
    Node 10: RAG-based medical diagnosis using PDF knowledge base
    Retrieves relevant medical information and generates diagnosis
    """
    
    def __init__(self):
        super().__init__(PipelineStages.RAG_DIAGNOSIS)
        self.vector_store = None
        self.embeddings = None
        self._initialize_rag_system()
    
    def _initialize_rag_system(self):
        """Initialize the RAG system: vector store, embeddings, and retriever."""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Define ChromaDB path relative to project root
                project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                chroma_db_path = str(project_root / "chroma_db")
                
                # Create directory if it doesn't exist
                Path(chroma_db_path).mkdir(parents=True, exist_ok=True)
                
                self.vector_store = Chroma(
                    embedding_function=self.embeddings,
                    persist_directory=chroma_db_path
                )
                
                # ğŸš€ **ìµœì í™”**: ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                existing_data_count = self._check_existing_embeddings()
                
                if existing_data_count > 0:
                    self.logger.info(f"âœ… ChromaDB ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° ë°œê²¬: {existing_data_count}ê°œ ë¬¸ì„œ")
                    self.logger.info("âš¡ PDF ì¬ë¡œë”© ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ ì„ë² ë”© ì‚¬ìš©")
                else:
                    self.logger.info("ğŸ“š ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš” - PDF ë¡œë”© ì‹œì‘")
                    self._load_medical_pdfs(Path("docs/medical_pdfs"))
                
                self.logger.info("âœ… RAG system initialized successfully.")
                return  # Success, exit retry loop
                
            except Exception as e:
                retry_count += 1
                self.logger.warning(f"RAG initialization attempt {retry_count}/{max_retries} failed: {e}")
                
                if retry_count < max_retries:
                    # Try to clean up ChromaDB directory for retry
                    project_root = Path(os.getenv('PROJECT_ROOT', '.'))
                    chroma_db_path = str(project_root / "chroma_db")
                    try:
                        import shutil
                        if Path(chroma_db_path).exists():
                            shutil.rmtree(chroma_db_path)
                            self.logger.info(f"Cleaned ChromaDB directory for retry {retry_count + 1}")
                    except Exception as cleanup_error:
                        self.logger.warning(f"Failed to cleanup ChromaDB: {cleanup_error}")
                else:
                    self.logger.error(f"Failed to initialize RAG system after {max_retries} attempts: {e}")
                    self.vector_store = None
    
    def _check_existing_embeddings(self) -> int:
        """
        ChromaDBì— ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        
        Returns:
            int: ê¸°ì¡´ ë¬¸ì„œì˜ ê°œìˆ˜ (0ì´ë©´ ìƒˆë¡œ ì„ë² ë”© í•„ìš”)
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ ê°•ì œ ë¦¬ë¡œë”© ì˜µì…˜ ì œê³µ
            force_reload = os.getenv('RAG_FORCE_RELOAD', 'false').lower() == 'true'
            if force_reload:
                self.logger.info("ğŸ”„ RAG_FORCE_RELOAD=true - ê°•ì œë¡œ PDF ì¬ë¡œë”© ìˆ˜í–‰")
                return 0
            
            # ChromaDBì—ì„œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ë¥¼ í™•ì¸
            collection = self.vector_store._collection
            
            # ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            document_count = collection.count()
            
            if document_count > 0:
                self.logger.info(f"ğŸ’¾ ê¸°ì¡´ ChromaDB ë°ì´í„°: {document_count}ê°œ ë¬¸ì„œ ë°œê²¬")
                return document_count
            else:
                self.logger.info("ğŸ“­ ChromaDBê°€ ë¹„ì–´ìˆìŒ - ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš”")
                return 0
                
        except Exception as e:
            self.logger.warning(f"ê¸°ì¡´ ì„ë² ë”© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìƒˆë¡œ ì„ë² ë”©í•˜ë„ë¡ 0 ë°˜í™˜
            return 0
    
    def _load_medical_pdfs(self, docs_dir: Path):
        """Load medical PDFs, split them, and add to the vector store."""
        try:
            from langchain_community.document_loaders import PyPDFLoader
            import os
            
            documents = []
            
            if docs_dir.exists():
                pdf_files = [f for f in os.listdir(docs_dir) if f.endswith('.pdf')]
                
                for pdf_file in pdf_files:
                    try:
                        pdf_path = docs_dir / pdf_file
                        loader = PyPDFLoader(str(pdf_path))
                        pdf_docs = loader.load()
                        
                        for doc in pdf_docs:
                            doc.metadata.update({
                                "source_file": pdf_file,
                                "document_type": "medical_literature"
                            })
                        
                        documents.extend(pdf_docs)
                        self.logger.info(f"âœ… Loaded PDF: {pdf_file} ({len(pdf_docs)} pages)")
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to load PDF {pdf_file}: {str(e)}")
                        continue
            
            if not documents:
                self.logger.warning("No PDF files loaded, using sample medical reference")
                # Add a comprehensive sample document if no PDFs are found
                sample_doc = Document(
                    page_content="""
                    ë³´í–‰ ë¶„ì„ ì„ìƒ ì°¸ê³  ìë£Œ - ê·¼ê±° ê¸°ë°˜ ê°€ì´ë“œë¼ì¸
                    
                    ê±´ê°•í•œ ì„±ì¸(20-65ì„¸) ì •ìƒ ë³´í–‰ ì§€í‘œ:
                    - ë³´í­ ì‹œê°„: 1.0-1.3ì´ˆ 
                    - ë³´í–‰ë¥ : 100-120 ê±¸ìŒ/ë¶„
                    - ë³´í­ ê¸¸ì´: 1.2-1.6m (ì‹ ì¥ ì˜ì¡´ì )
                    - ë³´í–‰ ì†ë„: 1.0-1.4 m/s 
                    - ì¢Œìš° ë¹„ëŒ€ì¹­ì„±: <5%
                    - ì‹œê°„ì  ë³€ë™ì„±: <5%
                    - ê³µê°„ì  ë³€ë™ì„±: <5%
                    
                    ë³‘ë¦¬í•™ì  ë³´í–‰ íŒ¨í„´:
                    
                    1. íŒŒí‚¨ìŠ¨ë³‘:
                    - ë³´í­ ê¸¸ì´ ê°ì†Œ (<1.0m)
                    - ë³´í–‰ë¥  ì¦ê°€ (>120 ê±¸ìŒ/ë¶„) 
                    - ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - ë³€ë™ì„± ì¦ê°€ (>10% CV)
                    - íŠ¹ì§•: ì§§ê³  ëŒë¦¬ëŠ” ê±¸ìŒ
                    
                    2. ë‡Œì¡¸ì¤‘ í¸ë§ˆë¹„:
                    - ì‹¬í•œ ë¹„ëŒ€ì¹­ì„± (>15%)
                    - í™˜ì¸¡ ë³´í­ ê¸¸ì´ ê°ì†Œ
                    - ì „ì²´ ë³´í–‰ ì†ë„ ê°ì†Œ (<0.8 m/s)
                    - íŠ¹ì§•: ì¼ì¸¡ì„± ì•½í™” íŒ¨í„´
                    
                    3. ì†Œë‡Œì„± ìš´ë™ì‹¤ì¡°:
                    - ë†’ì€ ë³´í–‰ ë³€ë™ì„± (>15% CV)
                    - ë„“ì€ ë³´í­
                    - ë¶ˆê·œì¹™í•œ ë³´í­ íƒ€ì´ë°
                    - íŠ¹ì§•: í˜‘ì¡°ì„± ë¶€ì¡±
                    """,
                    metadata={"source": "clinical_reference", "type": "sample_data"}
                )
                documents = [sample_doc]
                
            self.logger.info(f"Medical knowledge base loaded: {len(documents)} documents")
            
            # Split documents and add to vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            
            chunks = text_splitter.split_documents(documents)
            self.vector_store.add_documents(chunks)
            
            self.logger.info(f"Added {len(chunks)} chunks to vector store")
                
        except ImportError:
            self.logger.warning("PyPDFLoader not available, using sample medical data")
            sample_doc = Document(
                page_content="ê¸°ë³¸ ë³´í–‰ ë¶„ì„ ì°¸ê³  ìë£Œ: ì •ìƒ ë³´í–‰ ì†ë„ 1.0-1.4 m/s, ë³´í–‰ë¥  100-120 ê±¸ìŒ/ë¶„",
                metadata={"source": "fallback", "type": "basic_reference"}
            )
            # Just return the single doc, assuming no vector store to add to
            return [sample_doc]
    
    def execute(self, state: GraphState) -> GraphState:
        """Execute 2-stage RAG-based medical diagnosis"""
        
        required_fields = ["rag_query_stage1", "rag_query_stage2_template", "patient_info", "metrics_data"]
        if not self.validate_state_requirements(state, required_fields):
            return StateManager.set_error(state, f"Missing required fields: {required_fields}", "validation_error")
        
        if self.vector_store is None:
            return StateManager.set_error(state, "RAG system not initialized", "rag_system_error")
        
        stage1_query = state["rag_query_stage1"]
        stage2_template = state["rag_query_stage2_template"]
        patient_info = state["patient_info"]
        metrics_data = state["metrics_data"]
        session_id = state.get("session_id", "unknown")

        try:
            # STAGE 1: Individual Indicator Analysis
            self.logger.info("ğŸ” Stage 1: Individual Indicator Analysis")
            
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 4})
            stage1_docs = retriever.get_relevant_documents(stage1_query)
            
            # Format Stage 1 retrieved knowledge
            stage1_knowledge = self._format_retrieved_knowledge(stage1_docs, "Stage1")
            source_info_stage1 = self._extract_source_info(stage1_docs)
            
            # Create Stage 1 LLM prompt for individual indicator analysis
            stage1_llm_prompt = f"""ë‹¹ì‹ ì€ ë³´í–‰ ë¶„ì„ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. ì•„ë˜ ì˜ë£Œ ë¬¸í—Œì„ ë°”íƒ•ìœ¼ë¡œ ê° ë³´í–‰ ì§€í‘œë³„ ê°œë³„ ì§„ë‹¨ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

=== ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ===
{stage1_knowledge}

=== í™˜ì ë³´í–‰ ì§€í‘œ ë¶„ì„ ìš”ì²­ ===
{stage1_query}

=== ì‘ë‹µ í˜•ì‹ ===
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ëª¨ë“  í•„ë“œê°€ ì •í™•íˆ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

{{
  "indicators": [
    {{
      "id": "stride-time",
      "name": "ë³´í­ ì‹œê°„",
      "value": "{metrics_data['avg_stride_time']:.2f}ì´ˆ",
      "status": "normal",
      "description": "í•œìª½ ë°œì´ ë•…ì— ë‹¿ì€ í›„, ê°™ì€ ë°œì´ ë‹¤ì‹œ ë‹¿ì„ ë•Œê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì…ë‹ˆë‹¤. ê±¸ìŒ í…œí¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.",
      "result": "ì •ìƒë²”ìœ„(1.0-1.3ì´ˆ) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê±¸ìŒ í…œí¬ê°€ ì•ˆì •ì ì…ë‹ˆë‹¤."
    }},
    {{
      "id": "cadence",
      "name": "ë³´í–‰ë¥ ",
      "value": "{metrics_data['cadence']:.0f}ê±¸ìŒ/ë¶„",
      "status": "normal",
      "description": "1ë¶„ ë™ì•ˆ ê±·ëŠ” ê±¸ìŒ ìˆ˜ì…ë‹ˆë‹¤. ë³´í–‰ ë¦¬ë“¬ì„ ë‚˜íƒ€ë‚´ìš”.",
      "result": "ì •ìƒë²”ìœ„(100-120ê±¸ìŒ/ë¶„) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ë³´í–‰ ë¦¬ë“¬ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }},
    {{
      "id": "stride-length",
      "name": "ë³´í­ ê¸¸ì´",
      "value": "{metrics_data['avg_stride_length']:.2f}m",
      "status": "normal",
      "description": "í•œ ê±¸ìŒì˜ ê¸¸ì´ì…ë‹ˆë‹¤. ê·¼ë ¥ê³¼ ê´€ì ˆ ê°€ë™ì„±ì„ ë°˜ì˜í•´ìš”.",
      "result": "ì •ìƒë²”ìœ„(1.2-1.6m) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê·¼ë ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤."
    }},
    {{
      "id": "walking-speed",
      "name": "í‰ê·  ë³´í–‰ ì†ë„",
      "value": "{metrics_data['avg_walking_speed']:.2f}m/s",
      "status": "normal",
      "description": "ë‹¨ìœ„ ì‹œê°„ ë™ì•ˆ ì´ë™í•œ ê±°ë¦¬ì…ë‹ˆë‹¤. ì „ì²´ í™œë™ì„±ê³¼ ìš´ë™ ëŠ¥ë ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.",
      "result": "ì •ìƒë²”ìœ„(1.0-1.4m/s) ë‚´ì— ìˆìŠµë‹ˆë‹¤. í™œë™ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }},
    {{
      "id": "step-width",
      "name": "ë³´í­ í­",
      "value": "{metrics_data['step_width']:.2f}m",
      "status": "normal",
      "description": "ì¢Œìš° ë°œ ì‚¬ì´ì˜ ê°„ê²©ì…ë‹ˆë‹¤. ê· í˜• ëŠ¥ë ¥ì„ ë‚˜íƒ€ë‚´ìš”.",
      "result": "ì •ìƒë²”ìœ„(0.08-0.15m) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê· í˜•ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }},
    {{
      "id": "stride-time-cv",
      "name": "ë³´í­ ì‹œê°„ ë³€ë™ì„±",
      "value": "{metrics_data['stride_time_cv']:.1f}%",
      "status": "normal",
      "description": "ê±¸ìŒ ì‹œê°„ì˜ ì¼ì •í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì—ìš”.",
      "result": "ì •ìƒë²”ìœ„(<5%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê±¸ìŒì´ ì¼ì •í•©ë‹ˆë‹¤."
    }},
    {{
      "id": "stride-length-cv",
      "name": "ë³´í­ ê¸¸ì´ ë³€ë™ì„±",
      "value": "{metrics_data['stride_length_cv']:.1f}%",
      "status": "normal",
      "description": "ê±¸ìŒ ê¸¸ì´ì˜ ì¼ì •í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì—ìš”.",
      "result": "ì •ìƒë²”ìœ„(<5%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê±¸ìŒ ê¸¸ì´ê°€ ì¼ì •í•©ë‹ˆë‹¤."
    }},
    {{
      "id": "walking-speed-cv",
      "name": "ë³´í–‰ ì†ë„ ë³€ë™ì„±",
      "value": "{metrics_data['walking_speed_cv']:.1f}%",
      "status": "normal",
      "description": "ë³´í–‰ ì†ë„ì˜ ì¼ì •í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì—ìš”.",
      "result": "ì •ìƒë²”ìœ„(<5%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ì†ë„ê°€ ì¼ì •í•©ë‹ˆë‹¤."
    }},
    {{
      "id": "stride-difference",
      "name": "ì–‘ë°œ ë³´í­ ì°¨ì´",
      "value": "{metrics_data['stride_length_asymmetry']:.1f}%",
      "status": "normal",
      "description": "ì™¼ë°œê³¼ ì˜¤ë¥¸ë°œì˜ ê±¸ìŒ ê¸¸ì´ ì°¨ì´ì…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ê· í˜•ì´ ì¢‹ì•„ìš”.",
      "result": "ì •ìƒë²”ìœ„(<5%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ì¢Œìš° ê· í˜•ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }},
    {{
      "id": "stride-time-asymmetry",
      "name": "ë³´í­ ì‹œê°„ ë¹„ëŒ€ì¹­ì„±",
      "value": "{metrics_data['stride_time_asymmetry']:.1f}%",
      "status": "normal",
      "description": "ì¢Œìš° ë°œ ê±¸ìŒ ì‹œê°„ì˜ ì°¨ì´ì…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ê· í˜•ì´ ì¢‹ì•„ìš”.",
      "result": "ì •ìƒë²”ìœ„(<5%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ì‹œê°„ ê· í˜•ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }},
    {{
      "id": "gait-regularity",
      "name": "ë³´í–‰ ê·œì¹™ì„± ì§€ìˆ˜",
      "value": "{metrics_data['gait_regularity_index']:.3f}",
      "status": "normal",
      "description": "ê±¸ìŒì˜ ê·œì¹™ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì¼ì •í•œ ê±¸ìŒì´ì—ìš”.",
      "result": "ì •ìƒë²”ìœ„(>0.8) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê±¸ìŒì´ ê·œì¹™ì ì…ë‹ˆë‹¤."
    }},
    {{
      "id": "gait-stability",
      "name": "ë³´í–‰ ì•ˆì •ì„± ë¹„ìœ¨",
      "value": "{metrics_data['gait_stability_ratio']:.3f}",
      "status": "normal",
      "description": "ê±¸ìŒì˜ ì•ˆì •ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì—ìš”.",
      "result": "ì •ìƒë²”ìœ„(>0.7) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê±¸ìŒì´ ì•ˆì •ì ì…ë‹ˆë‹¤."
    }},
    {{
      "id": "stance-phase",
      "name": "ì…ê°ê¸° ë¹„ìœ¨",
      "value": "{metrics_data['stance_phase_ratio']:.1%}",
      "status": "normal",
      "description": "ë°œì´ ë•…ì— ë‹¿ì•„ ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.",
      "result": "ì •ìƒë²”ìœ„(60-65%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ì§€ì§€ ì‹œê°„ì´ ì ì ˆí•©ë‹ˆë‹¤."
    }},
    {{
      "id": "swing-phase",
      "name": "ìœ ê°ê¸° ë¹„ìœ¨",
      "value": "{metrics_data['swing_phase_ratio']:.1%}",
      "status": "normal",
      "description": "ë°œì´ ê³µì¤‘ì— ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.",
      "result": "ì •ìƒë²”ìœ„(35-40%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ìŠ¤ìœ™ ì‹œê°„ì´ ì ì ˆí•©ë‹ˆë‹¤."
    }},
    {{
      "id": "double-support",
      "name": "ì–‘ë°œ ì§€ì§€ ë¹„ìœ¨",
      "value": "{metrics_data['double_support_ratio']:.1%}",
      "status": "normal",
      "description": "ë‘ ë°œì´ ë™ì‹œì— ë•…ì— ë‹¿ì•„ ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì´ì—ìš”. ê· í˜•ê³¼ ê´€ë ¨ìˆì–´ìš”.",
      "result": "ì •ìƒë²”ìœ„(15-25%) ë‚´ì— ìˆìŠµë‹ˆë‹¤. ê· í˜•ì´ ì¢‹ìŠµë‹ˆë‹¤."
    }}
  ]
}}

**ì¤‘ìš” ì§€ì¹¨**:
1. ì˜ë£Œë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì§€í‘œì˜ ì •ìƒë²”ìœ„ë¥¼ ì°¾ì•„ í˜„ì¬ê°’ê³¼ ë¹„êµ
2. statusëŠ” ë°˜ë“œì‹œ "normal", "warning", "danger" ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©
3. valueëŠ” ì¸¡ì •ê°’ê³¼ ë‹¨ìœ„ë¥¼ ì •í™•íˆ í¬í•¨
4. resultëŠ” êµ¬ì²´ì ì¸ ì •ìƒë²”ìœ„ì™€ í•´ì„ì„ í¬í•¨
5. ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

            # Get Stage 1 results
            stage1_response = self.invoke_llm(stage1_llm_prompt)
            self.logger.info(f"âœ… Stage 1 ì™„ë£Œ: {len(stage1_response)} characters")
            
            # Parse Stage 1 JSON response with robust error handling
            try:
                # Remove any markdown formatting and extract JSON
                stage1_response_clean = stage1_response.strip()
                if stage1_response_clean.startswith('```json'):
                    stage1_response_clean = stage1_response_clean[7:]
                if stage1_response_clean.endswith('```'):
                    stage1_response_clean = stage1_response_clean[:-3]
                stage1_response_clean = stage1_response_clean.strip()
                
                stage1_result = json.loads(stage1_response_clean)
                indicators = stage1_result.get('indicators', [])
                
                # Validate indicators structure
                if not indicators or not isinstance(indicators, list):
                    raise ValueError("Invalid indicators structure from Stage 1")
                
                # Ensure all indicators have required fields
                for indicator in indicators:
                    if not all(key in indicator for key in ['id', 'name', 'value', 'status', 'description', 'result']):
                        raise ValueError(f"Missing required fields in indicator: {indicator}")
                    # Validate status values
                    if indicator['status'] not in ['normal', 'warning', 'danger']:
                        indicator['status'] = 'normal'  # Default to normal for invalid status
                
                print(f"âœ… Stage 1 ì„±ê³µ: {len(indicators)}ê°œ ì§€í‘œ ë¶„ì„ ì™„ë£Œ")
                
            except (json.JSONDecodeError, ValueError, KeyError) as e:
                print(f"âŒ Stage 1 JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                # Fallback indicators matching GaitAnalysisPage.jsx structure
                indicators = [
                    {
                        "id": "stride-time",
                        "name": "ë³´í­ ì‹œê°„",
                        "value": f"{metrics_data['avg_stride_time']:.2f}ì´ˆ",
                        "status": "normal",
                        "description": "í•œìª½ ë°œì´ ë•…ì— ë‹¿ì€ í›„, ê°™ì€ ë°œì´ ë‹¤ì‹œ ë‹¿ì„ ë•Œê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì…ë‹ˆë‹¤.",
                        "result": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    },
                    {
                        "id": "cadence",
                        "name": "ë³´í–‰ë¥ ",
                        "value": f"{metrics_data['cadence']:.0f}ê±¸ìŒ/ë¶„",
                        "status": "normal",
                        "description": "1ë¶„ ë™ì•ˆ ê±·ëŠ” ê±¸ìŒ ìˆ˜ì…ë‹ˆë‹¤.",
                        "result": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    },
                    {
                        "id": "stride-length",
                        "name": "ë³´í­ ê¸¸ì´",
                        "value": f"{metrics_data['avg_stride_length']:.2f}m",
                        "status": "normal",
                        "description": "í•œ ê±¸ìŒì˜ ê¸¸ì´ì…ë‹ˆë‹¤.",
                        "result": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    },
                    {
                        "id": "walking-speed",
                        "name": "í‰ê·  ë³´í–‰ ì†ë„",
                        "value": f"{metrics_data['avg_walking_speed']:.2f}m/s",
                        "status": "normal",
                        "description": "ë‹¨ìœ„ ì‹œê°„ ë™ì•ˆ ì´ë™í•œ ê±°ë¦¬ì…ë‹ˆë‹¤.",
                        "result": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    },
                    {
                        "id": "double-support",
                        "name": "ì–‘ë°œ ì§€ì§€ ë¹„ìœ¨",
                        "value": f"{metrics_data['double_support_ratio']:.1%}",
                        "status": "normal",
                        "description": "ë‘ ë°œì´ ë™ì‹œì— ë•…ì— ë‹¿ì•„ ìˆëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì´ì—ìš”.",
                        "result": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                    }
                ]
            
            # Store Stage 1 results for Stage 2
            state["stage1_indicators"] = indicators
            state["stage1_response"] = stage1_response
            state["stage1_source_info"] = source_info_stage1
            
            self.logger.info(f"ğŸ¯ Stage 1 completed - {len(indicators)} indicators analyzed")
            
            # STAGE 2: Overall Assessment & Disease Risk Evaluation
            self.logger.info("ğŸ¥ Stage 2: Overall Assessment & Disease Risk Evaluation")
            
            # Create Stage 2 query with Stage 1 results
            stage2_query = stage2_template.replace("{STAGE1_RESULTS}", json.dumps(indicators, ensure_ascii=False, indent=2))
            
            # Retrieve documents for Stage 2 (disease patterns, recommendations)
            stage2_docs = retriever.get_relevant_documents(stage2_query)
            stage2_knowledge = self._format_retrieved_knowledge(stage2_docs, "Stage2")
            source_info_stage2 = self._extract_source_info(stage2_docs)
            
            # Create Stage 2 LLM prompt for disease risk and recommendations
            stage2_llm_prompt = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•œ ë³´í–‰ ë¶„ì„ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. Stage 1 ì§€í‘œë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë³‘ ìœ„í—˜ë„ë¥¼ í‰ê°€í•˜ê³ , ë…¸ì¸ í™˜ìì—ê²Œ ì¹œê·¼í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

=== Stage 1 ì§€í‘œë³„ ë¶„ì„ ê²°ê³¼ ===
{json.dumps(indicators, ensure_ascii=False, indent=2)}

=== ê²€ìƒ‰ëœ ì˜ë£Œ ë¬¸í—Œ ===
{stage2_knowledge}

=== í™˜ì ì •ë³´ ===
- ì—°ë ¹: 60ì„¸ {patient_info['gender']}
- ì‹ ì¥: {patient_info['height_cm']}cm

=== ì‘ë‹µ í˜•ì‹ ===
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ëª¨ë“  í•„ë“œê°€ ì •í™•íˆ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

{{
  "score": 85,
  "status": "ë³´í–‰ì´ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤",
  "riskLevel": "ì •ìƒ ë‹¨ê³„",
  "diseases": [
    {{
      "id": "parkinson",
      "name": "íŒŒí‚¨ìŠ¨ë³‘",
      "probability": 0.15,
      "status": "ì •ìƒ ë²”ìœ„",
      "trend": "stable"
    }},
    {{
      "id": "stroke",
      "name": "ë‡Œì¡¸ì¤‘",
      "probability": 0.10,
      "status": "ì •ìƒ ë²”ìœ„",
      "trend": "stable"
    }},
    {{
      "id": "fall_risk",
      "name": "ë‚™ìƒ ìœ„í—˜",
      "probability": 0.20,
      "status": "ì •ìƒ ë²”ìœ„",
      "trend": "stable"
    }}
  ],
  "detailedReport": {{
    "title": "ì „ì²´ì ìœ¼ë¡œ ê±´ê°•í•œ ë³´í–‰ì´ì§€ë§Œ, ì¢Œìš° ê· í˜•ì„ ì¡°ê¸ˆ ë” ì‹ ê²½ ì“°ì‹œë©´ ì¢‹ê² ì–´ìš”",
    "content": "ì•ˆë…•í•˜ì„¸ìš”! {patient_info['user_id']}ë‹˜ì˜ ë³´í–‰ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.\\n\\nã€ì–´ë¥´ì‹ ì˜ ê±¸ìŒê±¸ì´ ìƒíƒœã€‘\\nğŸ˜Š ì¢‹ì€ ì ë“¤\\nâ€¢ ë³´í–‰ ì†ë„ê°€ ë˜ë˜ ë¶„ë“¤ê³¼ ë¹„ìŠ·í•´ì„œ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤\\nâ€¢ ë°œì„ ë‚´ë”›ëŠ” ì‹œê°„ì´ ì¼ì •í•´ì„œ ì•ˆì •ì ì´ì—ìš”\\nâ€¢ ì „ì²´ì ì¸ ê±¸ìŒ ë¦¬ë“¬ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤\\n\\nâš ï¸ ì¡°ê¸ˆ ì‹ ê²½ ì“¸ ì \\nâ€¢ Stage 1ì—ì„œ warning/danger ìƒíƒœì¸ ì§€í‘œë“¤ ì„¤ëª…\\nâ€¢ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„ë“¤ì„ ë¶€ë“œëŸ½ê²Œ ì œì‹œ\\n\\nã€ê±´ê°• ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?ã€‘\\nê±±ì •í•˜ì‹¤ í•„ìš” ì—†ì–´ìš”! íŒŒí‚¨ìŠ¨ë³‘ì´ë‚˜ ë‡Œì¡¸ì¤‘ ê°™ì€ ì§ˆë³‘ ì§•í›„ëŠ” ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\\në‹¤ë§Œ ë‚˜ì´ê°€ ë“¤ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ìƒê¸°ëŠ” ë³€í™”ë“¤ì´ë‹ˆê¹Œ ë¯¸ë¦¬ë¯¸ë¦¬ ê´€ë¦¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\\n\\nã€ì§‘ì—ì„œ ì‰½ê²Œ í•  ìˆ˜ ìˆëŠ” ìš´ë™ã€‘\\nğŸš¶â€â™€ï¸ ë§¤ì¼ í•˜ë©´ ì¢‹ì€ ê²ƒë“¤\\nâ€¢ ë™ë„¤ í•œ ë°”í€´ ì²œì²œíˆ ê±·ê¸° (30ë¶„ ì •ë„)\\nâ€¢ ì˜ì ì¡ê³  í•œë°œë¡œ ì„œê¸° (30ì´ˆì”© 3ë²ˆ)\\nâ€¢ TV ë³´ë©´ì„œ ì œìë¦¬ ê±¸ìŒ\\n\\nğŸŠâ€â™€ï¸ ì¼ì£¼ì¼ì— 2-3ë²ˆ\\nâ€¢ ë™ë„¤ ìˆ˜ì˜ì¥ì—ì„œ ë¬¼ì† ê±·ê¸°\\nâ€¢ ê³µì›ì—ì„œ ì¹œêµ¬ë“¤ê³¼ ê°€ë²¼ìš´ ì²´ì¡°\\nâ€¢ ì§‘ì—ì„œ ìŠ¤íŠ¸ë ˆì¹­ (ìœ íŠœë¸Œ ë³´ë©´ì„œ)\\n\\nê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”! ê±´ê°•í•œ ë³´í–‰ ìœ ì§€í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤ ğŸ˜Š"
  }}
}}

**ì¤‘ìš” ì§€ì¹¨**:
1. scoreëŠ” 0-100 ì‚¬ì´ì˜ ì •ìˆ˜ (Stage 1 ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ê³„ì‚°)
2. diseasesì˜ probabilityëŠ” 0.0-1.0 ì‚¬ì´ì˜ ì†Œìˆ˜ (ë°±ë¶„ìœ¨ì´ ì•„ë‹˜!)
3. diseasesì˜ statusëŠ” "ì •ìƒ ë²”ìœ„", "ì£¼ì˜ í•„ìš”", "ìœ„í—˜ ë²”ìœ„" ì¤‘ í•˜ë‚˜
4. detailedReportì˜ contentëŠ” ì‹¤ì œ Stage 1 ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ì‘ì„±
5. ì¹œê·¼í•˜ê³  ì•ˆì‹¬ì‹œí‚¤ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±
6. ì˜ë£Œë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ í‰ê°€ ìˆ˜í–‰
7. ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

            # Get Stage 2 results
            stage2_response = self.invoke_llm(stage2_llm_prompt)
            self.logger.info(f"âœ… Stage 2 ì™„ë£Œ: {len(stage2_response)} characters")
            
            # Parse Stage 2 JSON response with robust error handling
            try:
                # Remove any markdown formatting and extract JSON
                stage2_response_clean = stage2_response.strip()
                if stage2_response_clean.startswith('```json'):
                    stage2_response_clean = stage2_response_clean[7:]
                if stage2_response_clean.endswith('```'):
                    stage2_response_clean = stage2_response_clean[:-3]
                stage2_response_clean = stage2_response_clean.strip()
                
                stage2_result = json.loads(stage2_response_clean)
                
                # Validate required fields
                required_fields = ['score', 'status', 'riskLevel', 'diseases', 'detailedReport']
                for field in required_fields:
                    if field not in stage2_result:
                        raise ValueError(f"Missing required field: {field}")
                
                # Validate diseases structure
                diseases = stage2_result.get('diseases', [])
                if not isinstance(diseases, list):
                    raise ValueError("diseases must be a list")
                
                for disease in diseases:
                    if not all(key in disease for key in ['id', 'name', 'probability', 'status', 'trend']):
                        raise ValueError(f"Missing required fields in disease: {disease}")
                    # Ensure probability is between 0.0 and 1.0
                    if not (0.0 <= disease['probability'] <= 1.0):
                        disease['probability'] = min(max(disease['probability'], 0.0), 1.0)
                
                # Validate detailedReport structure
                detailed_report = stage2_result.get('detailedReport', {})
                if not isinstance(detailed_report, dict) or 'title' not in detailed_report or 'content' not in detailed_report:
                    raise ValueError("detailedReport must have title and content fields")
                
                # Ensure score is integer between 0-100
                score = stage2_result.get('score', 85)
                if not isinstance(score, int) or not (0 <= score <= 100):
                    stage2_result['score'] = 85
                
                print(f"âœ… Stage 2 ì„±ê³µ: ì¢…í•© ì§„ë‹¨ ì™„ë£Œ (ì ìˆ˜: {stage2_result['score']})")
                
            except (json.JSONDecodeError, ValueError, KeyError) as e:
                print(f"âŒ Stage 2 JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                # Fallback Stage 2 result matching GaitAnalysisPage.jsx structure
                stage2_result = {
                    "score": 85,
                    "status": "ë³´í–‰ì´ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤",
                    "riskLevel": "ì •ìƒ ë‹¨ê³„",
                    "diseases": [
                        {
                            "id": "parkinson",
                            "name": "íŒŒí‚¨ìŠ¨ë³‘",
                            "probability": 0.15,
                            "status": "ì •ìƒ ë²”ìœ„",
                            "trend": "stable"
                        },
                        {
                            "id": "stroke",
                            "name": "ë‡Œì¡¸ì¤‘",
                            "probability": 0.10,
                            "status": "ì •ìƒ ë²”ìœ„",
                            "trend": "stable"
                        },
                        {
                            "id": "fall_risk",
                            "name": "ë‚™ìƒ ìœ„í—˜",
                            "probability": 0.20,
                            "status": "ì •ìƒ ë²”ìœ„",
                            "trend": "stable"
                        }
                    ],
                    "detailedReport": {
                        "title": "ì „ì²´ì ìœ¼ë¡œ ê±´ê°•í•œ ë³´í–‰ ìƒíƒœì…ë‹ˆë‹¤",
                        "content": f"ì•ˆë…•í•˜ì„¸ìš”! {patient_info['user_id']}ë‹˜ì˜ ë³´í–‰ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.\\n\\nğŸ˜Š ì „ë°˜ì ìœ¼ë¡œ ê±´ê°•í•œ ë³´í–‰ íŒ¨í„´ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nã€ì§‘ì—ì„œ ì‰½ê²Œ í•  ìˆ˜ ìˆëŠ” ìš´ë™ã€‘\\nğŸš¶â€â™€ï¸ ë§¤ì¼ í•˜ë©´ ì¢‹ì€ ê²ƒë“¤\\nâ€¢ ë™ë„¤ í•œ ë°”í€´ ì²œì²œíˆ ê±·ê¸° (30ë¶„ ì •ë„)\\nâ€¢ ì˜ì ì¡ê³  í•œë°œë¡œ ì„œê¸° (30ì´ˆì”© 3ë²ˆ)\\n\\nê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”! ê±´ê°•í•œ ë³´í–‰ ìœ ì§€í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤ ğŸ˜Š"
                    }
                }
            
            # Combine Stage 1 and Stage 2 results for final output
            final_result = {
                "indicators": indicators,
                "score": stage2_result.get("score", 85),
                "status": stage2_result.get("status", "ë³´í–‰ì´ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤"),
                "riskLevel": stage2_result.get("riskLevel", "ì •ìƒ ë‹¨ê³„"),
                "diseases": stage2_result.get("diseases", []),
                "detailedReport": stage2_result.get("detailedReport", {
                    "title": "ë³´í–‰ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "content": "ì „ë°˜ì ìœ¼ë¡œ ê±´ê°•í•œ ë³´í–‰ íŒ¨í„´ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
                })
            }
            
            print(f"ğŸ¯ ìµœì¢… ê²°ê³¼ êµ¬ì¡°:")
            print(f"   - indicators: {len(final_result['indicators'])}ê°œ")
            print(f"   - score: {final_result['score']}")
            print(f"   - diseases: {len(final_result['diseases'])}ê°œ")
            print(f"   - detailedReport: {len(final_result['detailedReport']['content'])}ì")
            
            # Store results in state for next node
            state["diagnosis_result"] = final_result
            state["rag_diagnosis_completed"] = True
            
            return state
            
        except Exception as e:
            error_msg = f"2-stage RAG diagnosis failed: {str(e)}"
            self.logger.error(error_msg)
            return StateManager.set_error(state, error_msg, "rag_diagnosis_error")

    def _check_existing_embeddings(self) -> int:
        """
        ChromaDBì— ê¸°ì¡´ ì„ë² ë”© ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        
        Returns:
            int: ê¸°ì¡´ ë¬¸ì„œì˜ ê°œìˆ˜ (0ì´ë©´ ìƒˆë¡œ ì„ë² ë”© í•„ìš”)
        """
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ ê°•ì œ ë¦¬ë¡œë”© ì˜µì…˜ ì œê³µ
            force_reload = os.getenv('RAG_FORCE_RELOAD', 'false').lower() == 'true'
            if force_reload:
                self.logger.info("ğŸ”„ RAG_FORCE_RELOAD=true - ê°•ì œë¡œ PDF ì¬ë¡œë”© ìˆ˜í–‰")
                return 0
            
            # ChromaDBì—ì„œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ë¥¼ í™•ì¸
            collection = self.vector_store._collection
            
            # ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
            document_count = collection.count()
            
            if document_count > 0:
                self.logger.info(f"ğŸ’¾ ê¸°ì¡´ ChromaDB ë°ì´í„°: {document_count}ê°œ ë¬¸ì„œ ë°œê²¬")
                return document_count
            else:
                self.logger.info("ğŸ“­ ChromaDBê°€ ë¹„ì–´ìˆìŒ - ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± í•„ìš”")
                return 0
                
        except Exception as e:
            self.logger.warning(f"ê¸°ì¡´ ì„ë² ë”© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìƒˆë¡œ ì„ë² ë”©í•˜ë„ë¡ 0 ë°˜í™˜
            return 0
    
    def _format_retrieved_knowledge(self, docs: list, stage_name: str) -> str:
        """Format retrieved documents for LLM prompt"""
        
        knowledge = ""
        for i, doc in enumerate(docs, 1):
            source_file = doc.metadata.get('source_file', 'unknown_source')
            doc_type = doc.metadata.get('document_type', 'unknown_type')
            page_num = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
            
            content_snippet = doc.page_content.strip()
            if len(content_snippet) > 500:
                content_snippet = content_snippet[:500] + "..."
            
            knowledge += f"""
=== ì°¸ì¡°ë¬¸í—Œ {i} ({stage_name}): {source_file} ===
ë¬¸ì„œìœ í˜•: {doc_type}
í˜ì´ì§€: {page_num}
ê´€ë ¨ë‚´ìš©:
{content_snippet}

"""
        return knowledge
    
    def _extract_source_info(self, docs: list) -> list:
        """Extract source information from documents"""
        
        source_info = []
        for i, doc in enumerate(docs, 1):
            source_info.append({
                "ë²ˆí˜¸": i,
                "íŒŒì¼ëª…": doc.metadata.get('source_file', 'unknown_source'),
                "ë¬¸ì„œìœ í˜•": doc.metadata.get('document_type', 'unknown_type'),
                "í˜ì´ì§€": doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                "ë‚´ìš©ê¸¸ì´": len(doc.page_content)
            })
        return source_info

class StoreDiagnosisNode(BaseNode):
    """
    Node 11: Store medical diagnosis to Supabase
    Saves RAG-generated diagnosis results in simplified structure
    """
    
    def __init__(self):
        super().__init__(PipelineStages.STORE_DIAGNOSIS)
    
    def execute(self, state: GraphState) -> GraphState:
        """Store medical diagnosis to Supabase database"""
        
        # Check for new simplified structure
        if "diagnosis_result" not in state:
            return StateManager.set_error(state, "Missing diagnosis_result from RAG analysis", "validation_error")
        
        diagnosis_result = state["diagnosis_result"]
        user_id = state.get("user_id", "unknown")
        session_id = state.get("session_id", "unknown")

        try:
            from supabase import create_client
            import json
            
            # Use Service Role key to bypass RLS policies
            supabase_url = os.getenv('SUPABASE_URL')
            supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
            
            if not supabase_url or not supabase_key:
                raise ValueError("Supabase credentials not found in environment variables")
                
            supabase = create_client(supabase_url, supabase_key)
            
            # Prepare data for storage with new simplified structure
            storage_data = {
                'session_id': session_id,
                'user_id': user_id,
                'diagnosis_json': diagnosis_result,  # Store simplified structure directly
                'retrieved_papers': 0,  # Not tracking paper count anymore
                'ai_model_used': os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
                'confidence_score': self._calculate_confidence_score(diagnosis_result),
                'knowledge_base_version': 'medical_pdfs_v1',
                'processing_time_seconds': None
            }

            # Store to Supabase
            result = supabase.table('gait_diagnosis').insert(storage_data).execute()
            
            if result.data:
                stored_record = result.data[0]
                record_id = stored_record.get('id')
                state["diagnosis_record_id"] = record_id
                state["diagnosis_stored"] = True
                
                print(f"âœ… ì§„ë‹¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: Record ID {record_id}")
                return state
            else:
                error_info = getattr(result, 'error', 'Unknown error')
                return StateManager.set_error(state, f"Failed to store diagnosis: {error_info}", "storage_error")
            
        except Exception as e:
            error_msg = f"Diagnosis storage failed: {str(e)}"
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {error_msg}")
            return StateManager.set_error(state, error_msg, "storage_execution_error")
    
    def _calculate_confidence_score(self, diagnosis_result: dict) -> float:
        """Calculate confidence score based on diagnosis results"""
        try:
            # Base confidence from overall score
            score = diagnosis_result.get("score", 85)
            base_confidence = score / 100.0
            
            # Adjust based on risk level
            risk_level = diagnosis_result.get("riskLevel", "ì •ìƒ ë‹¨ê³„")
            if risk_level == "ì •ìƒ ë‹¨ê³„":
                confidence = min(1.0, base_confidence + 0.05)
            elif risk_level == "ìœ„í—˜ ë‹¨ê³„":
                confidence = max(0.3, base_confidence - 0.15)
            else:
                confidence = base_confidence
            
            # Check indicator consistency
            indicators = diagnosis_result.get("indicators", [])
            if indicators:
                normal_count = sum(1 for ind in indicators if ind.get("status") == "normal")
                consistency_boost = (normal_count / len(indicators)) * 0.1
                confidence = min(1.0, confidence + consistency_boost)
            
            return round(confidence, 3)
            
        except Exception as e:
            print(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.75  # Default confidence
