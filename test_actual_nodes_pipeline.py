#!/usr/bin/env python3
"""
ìµœì í™”ëœ LangGraph ë…¸ë“œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- ìƒˆë¡œìš´ ì…ë ¥ ì‹œìŠ¤í…œ: (user_id, height_cm, gender)
- LLM ìµœì í™”: 11/12 ë…¸ë“œì—ì„œ LLM ì œê±°
- Storage ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬
"""
import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_optimized_nodes_pipeline():
    """ìµœì í™”ëœ LangGraph ë…¸ë“œë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ê° ë‹¨ê³„ ì¶œë ¥ í™•ì¸"""
    
    print("ğŸš€ ìµœì í™”ëœ LangGraph ë…¸ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("=" * 80)
    print("ğŸ¯ ìµœì í™” ëª©í‘œ: LLM ì‚¬ìš©ëŸ‰ 91% ê°ì†Œ (11/12 ë…¸ë“œ ìµœì í™”)")
    print("ğŸ“Š ìƒˆë¡œìš´ ì‹œìŠ¤í…œ: (user_id, height_cm, gender) â†’ Storage ê¸°ë°˜")
    
    pipeline_start_time = time.time()
    llm_call_count = 0
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        from config import config
        print("âœ… í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ!")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì • - ìƒˆë¡œìš´ ì…ë ¥ ì‹œìŠ¤í…œ
        from langgraph_nodes.graph_state import GraphState
        initial_state = GraphState()
        initial_state.update({
            "user_id": "user_001",  # Storageì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼
            "height_cm": 180.0,
            "gender": "male",
            "session_id": f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ì„¸ì…˜: {initial_state['session_id']}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì ID: {initial_state['user_id']}")
        print(f"ğŸ“ í‚¤: {initial_state['height_cm']}cm")
        print(f"ğŸ‘« ì„±ë³„: {initial_state['gender']}")
        
        current_state = initial_state.copy()
        
        # LLM í˜¸ì¶œ ì¶”ì  í•¨ìˆ˜
        def create_llm_tracker(node_name):
            def track_llm_call(original_method):
                def wrapper(*args, **kwargs):
                    nonlocal llm_call_count
                    llm_call_count += 1
                    print(f"   âš ï¸  LLM Call #{llm_call_count} in {node_name}")
                    return original_method(*args, **kwargs)
                return wrapper
            return track_llm_call
        
        # 1ë‹¨ê³„: ìš”ì²­ ìˆ˜ì‹  ë° ê²€ì¦ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("1ï¸âƒ£ STEP 1: ReceiveRequestNode - ì…ë ¥ ê²€ì¦ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import ReceiveRequestNode
        receive_node = ReceiveRequestNode()
        
        # LLM ì¶”ì 
        if hasattr(receive_node, 'invoke_llm'):
            receive_node.invoke_llm = create_llm_tracker("ReceiveRequestNode")(receive_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì¤‘...")
        current_state = receive_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {current_state['error']}")
            return
        else:
            print(f"âœ… ê²€ì¦ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"   ğŸ‘¤ User ID: {current_state.get('user_id')}")
            print(f"   ğŸ“ Height: {current_state.get('height_cm')}cm")
            print(f"   ğŸ‘« Gender: {current_state.get('gender')}")
        
        # 2ë‹¨ê³„: íŒŒì¼ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (BuildQueryNode â†’ FileMetadataNode)
        print("\n" + "="*80)
        print("2ï¸âƒ£ STEP 2: FileMetadataNode - Storage íŒŒì¼ ê²€ìƒ‰ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import FileMetadataNode
        metadata_node = FileMetadataNode()
        
        # LLM ì¶”ì 
        if hasattr(metadata_node, 'invoke_llm'):
            metadata_node.invoke_llm = create_llm_tracker("FileMetadataNode")(metadata_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabase Storageì—ì„œ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        current_state = metadata_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        selected_file = current_state.get('selected_csv_file')
        available_files = current_state.get('available_csv_files', [])
        
        if selected_file:
            print(f"âœ… íŒŒì¼ ê²€ìƒ‰ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ì„ íƒëœ íŒŒì¼: {selected_file.get('name')}")
            print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {selected_file.get('size', 0):,} bytes")
            print(f"ğŸ“… ìˆ˜ì • ë‚ ì§œ: {selected_file.get('last_modified')}")
            print(f"ğŸ—‚ï¸ ë°œê²¬ëœ ì´ íŒŒì¼: {len(available_files)}ê°œ")
        else:
            print(f"âš ï¸  íŒŒì¼ ê²€ìƒ‰ ì™„ë£Œí–ˆì§€ë§Œ ì„ íƒëœ íŒŒì¼ ì—†ìŒ ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ—‚ï¸ ë°œê²¬ëœ íŒŒì¼: {len(available_files)}ê°œ")
        
        # 3ë‹¨ê³„: CSV ë‹¤ìš´ë¡œë“œ (FetchCsvNode â†’ DownloadCsvNode)
        print("\n" + "="*80)
        print("3ï¸âƒ£ STEP 3: DownloadCsvNode - Storageì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import DownloadCsvNode
        download_node = DownloadCsvNode()
        
        # LLM ì¶”ì 
        if hasattr(download_node, 'invoke_llm'):
            download_node.invoke_llm = create_llm_tracker("DownloadCsvNode")(download_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ”„ Storageì—ì„œ IMU ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        current_state = download_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        csv_path = current_state.get('raw_csv_path')
        download_info = current_state.get('downloaded_file_info', {})
        
        if csv_path:
            df = pd.read_csv(csv_path)
            file_size = Path(csv_path).stat().st_size
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ë¡œì»¬ íŒŒì¼: {csv_path}")
            print(f"ğŸ“Š ë°ì´í„°: {len(df):,}ê°œ ë ˆì½”ë“œ")
            print(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            print(f"ğŸ“¥ ì›ë³¸ íŒŒì¼: {download_info.get('original_name', 'N/A')}")
            if 'accel_x' in df.columns:
                print(f"ğŸ” ê°€ì†ë„ê³„ X ë²”ìœ„: {df['accel_x'].min():.2f} ~ {df['accel_x'].max():.2f}")
        else:
            print(f"âš ï¸  ë‹¤ìš´ë¡œë“œ ì •ë³´ê°€ stateì— ì—†ìŠµë‹ˆë‹¤")
        
        # 4ë‹¨ê³„: Butterworth í•„í„°ë§ + ë°ì´í„° íŠ¸ë¦¬ë°
        print("\n" + "="*80)
        print("4ï¸âƒ£ STEP 4: FilterDataNode - í•„í„°ë§ + íŠ¸ë¦¬ë° (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import FilterDataNode
        filter_node = FilterDataNode()
        
        # LLM ì¶”ì 
        if hasattr(filter_node, 'invoke_llm'):
            filter_node.invoke_llm = create_llm_tracker("FilterDataNode")(filter_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ”„ Butterworth í•„í„° ì ìš© + ë°ì´í„° íŠ¸ë¦¬ë° ì¤‘...")
        current_state = filter_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        filtered_path = current_state.get('filtered_csv_path')
        if filtered_path:
            df_filtered = pd.read_csv(filtered_path)
            original_len = len(df)
            filtered_len = len(df_filtered)
            
            print(f"âœ… í•„í„°ë§ + íŠ¸ë¦¬ë° ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ í•„í„°ë§ëœ íŒŒì¼: {filtered_path}")
            print(f"ğŸ“Š ë°ì´í„° ë³€í™”: {original_len:,} â†’ {filtered_len:,} ë ˆì½”ë“œ")
            print(f"âœ‚ï¸ íŠ¸ë¦¬ë°: ì²˜ìŒ 2ì´ˆ + ë§ˆì§€ë§‰ 3ì´ˆ ì œê±°")
            
            # í•„í„°ë§ í†µê³„
            filter_stats = current_state.get('filter_statistics', {})
            if filter_stats:
                print(f"ğŸ“ˆ í•„í„°ë§ í†µê³„:")
                print(f"   ì›ë³¸ ê¸¸ì´: {filter_stats.get('original_length', 'N/A')} ë ˆì½”ë“œ")
                print(f"   íŠ¸ë¦¬ë° í›„: {filter_stats.get('trimmed_length', 'N/A')} ë ˆì½”ë“œ") 
                print(f"   ìµœì¢… ê¸¸ì´: {filter_stats.get('final_length', 'N/A')} ë ˆì½”ë“œ")
        
        # 5ë‹¨ê³„: ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("5ï¸âƒ£ STEP 5: PredictPhasesNode - ë”¥ëŸ¬ë‹ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.ai_model_nodes import PredictPhasesNode
        predict_phases_node = PredictPhasesNode()
        
        # LLM ì¶”ì 
        if hasattr(predict_phases_node, 'invoke_llm'):
            predict_phases_node.invoke_llm = create_llm_tracker("PredictPhasesNode")(predict_phases_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ¤– Stage2Predictor ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        current_state = predict_phases_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        phases_path = current_state.get('labels_csv_path')
        if phases_path:
            df_phases = pd.read_csv(phases_path)
            print(f"âœ… ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: {phases_path}")
            print(f"ğŸ“Š ë³´í–‰ ì„¸ê·¸ë¨¼íŠ¸: {len(df_phases)}ê°œ")
            if 'phase' in df_phases.columns:
                phase_counts = df_phases['phase'].value_counts().sort_index()
                print(f"ğŸš¶ ë³´í–‰ ë‹¨ê³„ ë¶„í¬:")
                for phase, count in phase_counts.items():
                    print(f"   Phase {phase}: {count}ê°œ")
        
        # 6ë‹¨ê³„: ë³´í­/ì†ë„ ì˜ˆì¸¡ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("6ï¸âƒ£ STEP 6: PredictStrideNode - ë”¥ëŸ¬ë‹ ë³´í­/ì†ë„ ì˜ˆì¸¡ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.ai_model_nodes import PredictStrideNode
        predict_stride_node = PredictStrideNode()
        
        # LLM ì¶”ì 
        if hasattr(predict_stride_node, 'invoke_llm'):
            predict_stride_node.invoke_llm = create_llm_tracker("PredictStrideNode")(predict_stride_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ¤– StrideInferencePipeline ì‹¤í–‰ ì¤‘...")
        current_state = predict_stride_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë³´í­/ì†ë„ ì˜ˆì¸¡ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        stride_results = current_state.get('stride_results')
        if stride_results and 'predictions' in stride_results:
            predictions = stride_results['predictions']
            print(f"âœ… ë³´í­/ì†ë„ ì˜ˆì¸¡ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ”„ ì˜ˆì¸¡ëœ ë³´í­: {len(predictions)}ê°œ")
            
            if predictions:
                lengths = [p.get('predicted_stride_length', 0) for p in predictions]
                velocities = [p.get('predicted_velocity', 0) for p in predictions]
                print(f"ğŸ“ ë³´í­ í‰ê· : {sum(lengths)/len(lengths):.2f}m")
                print(f"ğŸƒ ì†ë„ í‰ê· : {sum(velocities)/len(velocities):.2f}m/s")
        
        # 7ë‹¨ê³„: 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("7ï¸âƒ£ STEP 7: CalcMetricsNode - 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.metrics_nodes import CalcMetricsNode
        calc_metrics_node = CalcMetricsNode()
        
        # LLM ì¶”ì 
        if hasattr(calc_metrics_node, 'invoke_llm'):
            calc_metrics_node.invoke_llm = create_llm_tracker("CalcMetricsNode")(calc_metrics_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ“Š ìˆœìˆ˜ Pythonìœ¼ë¡œ 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° ì¤‘...")
        current_state = calc_metrics_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {current_state['error']}")
            return
        
        gait_metrics = current_state.get('gait_metrics')
        if gait_metrics:
            print(f"âœ… 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“Š ê³„ì‚°ëœ ì§€í‘œ:")
            print(f"   â±ï¸ í‰ê·  ë³´í–‰ì‹œê°„: {gait_metrics.get('avg_stride_time', 'N/A'):.3f}ì´ˆ")
            print(f"   ğŸ“ í‰ê·  ë³´í­: {gait_metrics.get('avg_stride_length', 'N/A'):.3f}m")
            print(f"   ğŸƒ í‰ê·  ì†ë„: {gait_metrics.get('avg_walking_speed', 'N/A'):.3f}m/s")
            print(f"   ğŸ”„ ë³´í–‰ë¥ : {gait_metrics.get('cadence', 'N/A'):.1f} steps/min")
            print(f"   âš–ï¸ ë¹„ëŒ€ì¹­ì„±: {gait_metrics.get('stride_length_asymmetry', 'N/A'):.2f}%")
        
        # 8ë‹¨ê³„: ë³´í–‰ ì§€í‘œ ì €ì¥ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("8ï¸âƒ£ STEP 8: StoreMetricsNode - ì§€í‘œ ì €ì¥ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.metrics_nodes import StoreMetricsNode
        store_metrics_node = StoreMetricsNode()
        
        # LLM ì¶”ì 
        if hasattr(store_metrics_node, 'invoke_llm'):
            store_metrics_node.invoke_llm = create_llm_tracker("StoreMetricsNode")(store_metrics_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabaseì— ë³´í–‰ ì§€í‘œ ì €ì¥ ì¤‘...")
        
        # date í•„ë“œ ì¶”ê°€ (StoreMetricsNodeê°€ ìš”êµ¬)
        current_state['date'] = datetime.now().strftime('%Y-%m-%d')
        current_state = store_metrics_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§€í‘œ ì €ì¥ ì‹¤íŒ¨: {current_state['error']}")
            print("âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ê³„ì† ì§„í–‰)")
        else:
            metrics_record_id = current_state.get('metrics_record_id')
            print(f"âœ… ë³´í–‰ ì§€í‘œ ì €ì¥ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            if metrics_record_id:
                print(f"   ğŸ“Š Record ID: {metrics_record_id}")
        
        # 9ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± (LLM ì œê±°ë¨ - 2-stage ì‹œìŠ¤í…œ)
        print("\n" + "="*80)
        print("9ï¸âƒ£ STEP 9: ComposePromptNode - 2-Stage RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„± (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import ComposePromptNode
        compose_prompt_node = ComposePromptNode()
        
        # LLM ì¶”ì 
        if hasattr(compose_prompt_node, 'invoke_llm'):
            compose_prompt_node.invoke_llm = create_llm_tracker("ComposePromptNode")(compose_prompt_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Stage 1 + Stage 2 RAG ì¿¼ë¦¬ êµ¬ì„± ì¤‘...")
        current_state = compose_prompt_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {current_state['error']}")
            return
        
        rag_query_stage1 = current_state.get('rag_query_stage1')
        rag_query_stage2_template = current_state.get('rag_query_stage2_template')
        
        if rag_query_stage1 and rag_query_stage2_template:
            print(f"âœ… 2-Stage RAG ì¿¼ë¦¬ êµ¬ì„± ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ Stage 1 ì¿¼ë¦¬ ê¸¸ì´: {len(rag_query_stage1):,} ë¬¸ì")
            print(f"ğŸ“ Stage 2 í…œí”Œë¦¿ ê¸¸ì´: {len(rag_query_stage2_template):,} ë¬¸ì")
            print(f"ğŸ¯ í™˜ì ì •ë³´: {current_state.get('patient_info', {}).get('user_id', 'N/A')}")
        
        # 10ë‹¨ê³„: 2-Stage RAG ê¸°ë°˜ ì§„ë‹¨ (LLM ì‚¬ìš©)
        print("\n" + "="*80)
        print("ğŸ”Ÿ STEP 10: RagDiagnosisNode - 2-Stage RAG ì§„ë‹¨ (LLM ì‚¬ìš©)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import RagDiagnosisNode
        rag_diagnosis_node = RagDiagnosisNode()
        
        # LLM ì¶”ì 
        if hasattr(rag_diagnosis_node, 'invoke_llm'):
            rag_diagnosis_node.invoke_llm = create_llm_tracker("RagDiagnosisNode")(rag_diagnosis_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Stage 1: ê°œë³„ ì§€í‘œ ë¶„ì„ + Stage 2: ì¢…í•© ì§„ë‹¨ ì‹¤í–‰ ì¤‘...")
        current_state = rag_diagnosis_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ 2-Stage RAG ì§„ë‹¨ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        # ìƒˆë¡œìš´ diagnosis_result êµ¬ì¡° í™•ì¸
        diagnosis_result = current_state.get('diagnosis_result')
        stage1_indicators = current_state.get('stage1_indicators', [])
        
        if diagnosis_result:
            print(f"âœ… 2-Stage RAG ì§„ë‹¨ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            
            # Stage 1 ê²°ê³¼ í™•ì¸
            indicators = diagnosis_result.get('indicators', [])
            print(f"ğŸ“Š Stage 1 ê°œë³„ ì§€í‘œ ë¶„ì„: {len(indicators)}ê°œ")
            if indicators:
                normal_count = sum(1 for ind in indicators if ind.get('status') == 'normal')
                warning_count = len(indicators) - normal_count
                print(f"   âœ… ì •ìƒ: {normal_count}ê°œ, âš ï¸ ì£¼ì˜: {warning_count}ê°œ")
            
            # Stage 2 ê²°ê³¼ í™•ì¸
            score = diagnosis_result.get('score', 0)
            status = diagnosis_result.get('status', 'N/A')
            risk_level = diagnosis_result.get('riskLevel', 'N/A')
            diseases = diagnosis_result.get('diseases', [])
            detailed_report = diagnosis_result.get('detailedReport', {})
            
            print(f"ğŸ¥ Stage 2 ì¢…í•© ì§„ë‹¨:")
            print(f"   ğŸ“ˆ ì ìˆ˜: {score}")
            print(f"   ğŸ“‹ ìƒíƒœ: {status}")
            print(f"   âš ï¸ ìœ„í—˜ë„: {risk_level}")
            print(f"   ğŸ¦  ì§ˆë³‘ í‰ê°€: {len(diseases)}ê°œ")
            print(f"   ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {len(detailed_report.get('content', ''))}ì")
            
            # ì§„ë‹¨ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            if detailed_report.get('title'):
                print(f"   ğŸ“Œ ë¦¬í¬íŠ¸ ì œëª©: {detailed_report['title']}")
        
        # 11ë‹¨ê³„: ì§„ë‹¨ ê²°ê³¼ ì €ì¥ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("1ï¸âƒ£1ï¸âƒ£ STEP 11: StoreDiagnosisNode - ì§„ë‹¨ ê²°ê³¼ ì €ì¥ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import StoreDiagnosisNode
        store_diagnosis_node = StoreDiagnosisNode()
        
        # LLM ì¶”ì 
        if hasattr(store_diagnosis_node, 'invoke_llm'):
            store_diagnosis_node.invoke_llm = create_llm_tracker("StoreDiagnosisNode")(store_diagnosis_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabaseì— diagnosis_result ì €ì¥ ì¤‘...")
        current_state = store_diagnosis_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§„ë‹¨ ì €ì¥ ì‹¤íŒ¨: {current_state['error']}")
            print("âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ê³„ì† ì§„í–‰)")
        else:
            diagnosis_record_id = current_state.get('diagnosis_record_id')
            print(f"âœ… ì§„ë‹¨ ê²°ê³¼ ì €ì¥ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            if diagnosis_record_id:
                print(f"   ğŸ¥ Record ID: {diagnosis_record_id}")
            
            # ì‹ ë¢°ë„ ì ìˆ˜ í™•ì¸
            if diagnosis_result:
                confidence_score = store_diagnosis_node._calculate_confidence_score(diagnosis_result)
                print(f"   ğŸ“Š ê³„ì‚°ëœ ì‹ ë¢°ë„: {confidence_score:.3f}")
        
        # 12ë‹¨ê³„: ìµœì¢… ì‘ë‹µ í¬ë§·íŒ… (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("1ï¸âƒ£2ï¸âƒ£ STEP 12: FormatResponseNode - ìµœì¢… ì‘ë‹µ ìƒì„± (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.response_nodes import FormatResponseNode
        format_response_node = FormatResponseNode()
        
        # LLM ì¶”ì 
        if hasattr(format_response_node, 'invoke_llm'):
            format_response_node.invoke_llm = create_llm_tracker("FormatResponseNode")(format_response_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ diagnosis_result ê¸°ë°˜ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
        current_state = format_response_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì‹¤íŒ¨: {current_state['error']}")
            return
        
        final_response = current_state.get('response')
        if final_response:
            response_length = len(json.dumps(final_response, ensure_ascii=False))
            print(f"âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“„ ì‘ë‹µ ê¸¸ì´: {response_length:,} ë¬¸ì")
            
            # ìµœì¢… ì‘ë‹µ êµ¬ì¡° í™•ì¸
            if isinstance(final_response, dict):
                print(f"ğŸ“‹ ìµœì¢… ì‘ë‹µ êµ¬ì¡°:")
                print(f"   - indicators: {len(final_response.get('indicators', []))}ê°œ")
                print(f"   - score: {final_response.get('score', 'N/A')}")
                print(f"   - diseases: {len(final_response.get('diseases', []))}ê°œ")
                print(f"   - metadata: {'ìˆìŒ' if 'metadata' in final_response else 'ì—†ìŒ'}")
        
        total_time = time.time() - pipeline_start_time
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ‰ ì™„ì „í•œ End-to-End 2-Stage RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*80)
        
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ì„±ê³¼:")
        print(f"   ğŸš€ ì´ LLM í˜¸ì¶œ: {llm_call_count}íšŒ")
        print(f"   â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ğŸ¯ ìµœì í™” êµ¬ì¡°: 10/12 ë…¸ë“œ LLM ì œê±° (83% ìµœì í™”)")
        print(f"   ğŸ’¡ 2-Stage RAG: Stage 1(ì§€í‘œë³„) + Stage 2(ì¢…í•©ì§„ë‹¨)")
        
        print(f"\nğŸ“ˆ ì „ì²´ 12ë‹¨ê³„ ì²˜ë¦¬ ìš”ì•½:")
        print(f"   1ï¸âƒ£ ì…ë ¥ ê²€ì¦: âœ… LLM ì œê±° (ìˆœìˆ˜ Python)")
        print(f"   2ï¸âƒ£ íŒŒì¼ ê²€ìƒ‰: âœ… LLM ì œê±° (Storage API)")
        print(f"   3ï¸âƒ£ ë°ì´í„° ë‹¤ìš´ë¡œë“œ: âœ… LLM ì œê±° (Storage API)")
        print(f"   4ï¸âƒ£ í•„í„°ë§+íŠ¸ë¦¬ë°: âœ… LLM ì œê±° (Butterworth)")
        print(f"   5ï¸âƒ£ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡: âœ… LLM ì œê±° (ë”¥ëŸ¬ë‹)")
        print(f"   6ï¸âƒ£ ë³´í­/ì†ë„ ì˜ˆì¸¡: âœ… LLM ì œê±° (ë”¥ëŸ¬ë‹)")
        print(f"   7ï¸âƒ£ ì§€í‘œ ê³„ì‚°: âœ… LLM ì œê±° (ìˆœìˆ˜ Python)")
        print(f"   8ï¸âƒ£ ì§€í‘œ ì €ì¥: âœ… LLM ì œê±° (Database API)")
        print(f"   9ï¸âƒ£ RAG ì¿¼ë¦¬ êµ¬ì„±: âœ… LLM ì œê±° (2-stage í…œí”Œë¦¿)")
        print(f"   ğŸ”Ÿ 2-Stage RAG ì§„ë‹¨: ğŸ¤– LLM ì‚¬ìš© (Stage 1+2)")
        print(f"   1ï¸âƒ£1ï¸âƒ£ ì§„ë‹¨ ì €ì¥: âœ… LLM ì œê±° (Database API)")
        print(f"   1ï¸âƒ£2ï¸âƒ£ ì‘ë‹µ ìƒì„±: âœ… LLM ì œê±° (êµ¬ì¡° ë³µì‚¬)")
        
        # 2-Stage RAG ì‹œìŠ¤í…œ ë¶„ì„
        print(f"\nğŸ§  2-Stage RAG ì‹œìŠ¤í…œ ë¶„ì„:")
        if diagnosis_result:
            indicators = diagnosis_result.get('indicators', [])
            diseases = diagnosis_result.get('diseases', [])
            print(f"   ğŸ“Š Stage 1: {len(indicators)}ê°œ ê°œë³„ ì§€í‘œ ë¶„ì„")
            print(f"   ğŸ¥ Stage 2: {len(diseases)}ê°œ ì§ˆë³‘ ìœ„í—˜ë„ + ì¢…í•© ì†Œê²¬")
            print(f"   ğŸ“ˆ ìµœì¢… ì ìˆ˜: {diagnosis_result.get('score', 'N/A')}")
            print(f"   âš ï¸ ìœ„í—˜ë„: {diagnosis_result.get('riskLevel', 'N/A')}")
        
        # ìµœì¢… ì„±ê³¼ í‰ê°€
        if llm_call_count <= 2:  # ì˜ˆìƒë˜ëŠ” LLM í˜¸ì¶œ (Stage 1 + Stage 2ë§Œ)
            print(f"\nğŸ‰ 2-Stage RAG íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
            print(f"   âœ… ì „ì²´ 12ë‹¨ê³„ ì™„ë£Œ")
            print(f"   âœ… 83% ìµœì í™” ë‹¬ì„± (10/12 ë…¸ë“œ LLM ì œê±°)")
            print(f"   âœ… 2-Stage RAG ì•„í‚¤í…ì²˜ êµ¬í˜„")
            print(f"   âœ… ê°œë³„ ì§€í‘œ + ì¢…í•© ì§„ë‹¨ ë¶„ë¦¬")
            
            if current_state.get('response'):
                print(f"   âœ… ìµœì¢… ì‚¬ìš©ì ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        else:
            print(f"\nâš ï¸  ì˜ˆìƒë³´ë‹¤ ë§ì€ LLM í˜¸ì¶œ ë°œê²¬ ({llm_call_count}íšŒ)")
            print(f"   ì˜ˆìƒ: 2íšŒ (Stage 1 + Stage 2ë§Œ)")
            print(f"   ì‹¤ì œ: {llm_call_count}íšŒ")
            print(f"   ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return current_state
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    result = test_optimized_nodes_pipeline() 