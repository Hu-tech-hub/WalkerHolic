#!/usr/bin/env python3
"""
ìµœì í™”ëœ LangGraph ë…¸ë“œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- ìƒˆë¡œìš´ ì…ë ¥ ì‹œìŠ¤í…œ: (user_id, height_cm, gender)
- LLM ìµœì í™”: 11/12 ë…¸ë“œì—ì„œ LLM ì œê±°
- Storage ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬
"""
import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_optimized_nodes_pipeline():
    """ìµœì í™”ëœ LangGraph ë…¸ë“œë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ê° ë‹¨ê³„ ì¶œë ¥ í™•ì¸"""
    
    print("ğŸš€ ìµœì í™”ëœ LangGraph ë…¸ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("=" * 80)
    print("ğŸ¯ ìµœì í™” ëª©í‘œ: LLM ì‚¬ìš©ëŸ‰ 91% ê°ì†Œ (11/12 ë…¸ë“œ ìµœì í™”)")
    print("ğŸ“Š ìƒˆë¡œìš´ ì‹œìŠ¤í…œ: (user_id, height_cm, gender) â†’ Storage ê¸°ë°˜")
    
    pipeline_start_time = time.time()
    llm_call_count = 0
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        from config import config
        print("âœ… í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ!")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì • - ìƒˆë¡œìš´ ì…ë ¥ ì‹œìŠ¤í…œ
        from langgraph_nodes.graph_state import GraphState
        initial_state = GraphState()
        initial_state.update({
            "user_id": "user_001",  # Storageì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼
            "height_cm": 180.0,
            "gender": "male",
            "session_id": f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ì„¸ì…˜: {initial_state['session_id']}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì ID: {initial_state['user_id']}")
        print(f"ğŸ“ í‚¤: {initial_state['height_cm']}cm")
        print(f"ğŸ‘« ì„±ë³„: {initial_state['gender']}")
        
        current_state = initial_state.copy()
        
        # LLM í˜¸ì¶œ ì¶”ì  í•¨ìˆ˜
        def create_llm_tracker(node_name):
            def track_llm_call(original_method):
                def wrapper(*args, **kwargs):
                    nonlocal llm_call_count
                    llm_call_count += 1
                    print(f"   âš ï¸  LLM Call #{llm_call_count} in {node_name}")
                    return original_method(*args, **kwargs)
                return wrapper
            return track_llm_call
        
        # 1ë‹¨ê³„: ìš”ì²­ ìˆ˜ì‹  ë° ê²€ì¦ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("1ï¸âƒ£ STEP 1: ReceiveRequestNode - ì…ë ¥ ê²€ì¦ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import ReceiveRequestNode
        receive_node = ReceiveRequestNode()
        
        # LLM ì¶”ì 
        if hasattr(receive_node, 'invoke_llm'):
            receive_node.invoke_llm = create_llm_tracker("ReceiveRequestNode")(receive_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì¤‘...")
        current_state = receive_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {current_state['error']}")
            return
        else:
            print(f"âœ… ê²€ì¦ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"   ğŸ‘¤ User ID: {current_state.get('user_id')}")
            print(f"   ğŸ“ Height: {current_state.get('height_cm')}cm")
            print(f"   ğŸ‘« Gender: {current_state.get('gender')}")
        
        # 2ë‹¨ê³„: íŒŒì¼ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ (BuildQueryNode â†’ FileMetadataNode)
        print("\n" + "="*80)
        print("2ï¸âƒ£ STEP 2: FileMetadataNode - Storage íŒŒì¼ ê²€ìƒ‰ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import FileMetadataNode
        metadata_node = FileMetadataNode()
        
        # LLM ì¶”ì 
        if hasattr(metadata_node, 'invoke_llm'):
            metadata_node.invoke_llm = create_llm_tracker("FileMetadataNode")(metadata_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabase Storageì—ì„œ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        current_state = metadata_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        selected_file = current_state.get('selected_csv_file')
        available_files = current_state.get('available_csv_files', [])
        
        if selected_file:
            print(f"âœ… íŒŒì¼ ê²€ìƒ‰ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ì„ íƒëœ íŒŒì¼: {selected_file.get('name')}")
            print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {selected_file.get('size', 0):,} bytes")
            print(f"ğŸ“… ìˆ˜ì • ë‚ ì§œ: {selected_file.get('last_modified')}")
            print(f"ğŸ—‚ï¸ ë°œê²¬ëœ ì´ íŒŒì¼: {len(available_files)}ê°œ")
        else:
            print(f"âš ï¸  íŒŒì¼ ê²€ìƒ‰ ì™„ë£Œí–ˆì§€ë§Œ ì„ íƒëœ íŒŒì¼ ì—†ìŒ ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ—‚ï¸ ë°œê²¬ëœ íŒŒì¼: {len(available_files)}ê°œ")
        
        # 3ë‹¨ê³„: CSV ë‹¤ìš´ë¡œë“œ (FetchCsvNode â†’ DownloadCsvNode)
        print("\n" + "="*80)
        print("3ï¸âƒ£ STEP 3: DownloadCsvNode - Storageì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import DownloadCsvNode
        download_node = DownloadCsvNode()
        
        # LLM ì¶”ì 
        if hasattr(download_node, 'invoke_llm'):
            download_node.invoke_llm = create_llm_tracker("DownloadCsvNode")(download_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ”„ Storageì—ì„œ IMU ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        current_state = download_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        csv_path = current_state.get('raw_csv_path')
        download_info = current_state.get('downloaded_file_info', {})
        
        if csv_path:
            df = pd.read_csv(csv_path)
            file_size = Path(csv_path).stat().st_size
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ë¡œì»¬ íŒŒì¼: {csv_path}")
            print(f"ğŸ“Š ë°ì´í„°: {len(df):,}ê°œ ë ˆì½”ë“œ")
            print(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            print(f"ğŸ“¥ ì›ë³¸ íŒŒì¼: {download_info.get('original_name', 'N/A')}")
            if 'accel_x' in df.columns:
                print(f"ğŸ” ê°€ì†ë„ê³„ X ë²”ìœ„: {df['accel_x'].min():.2f} ~ {df['accel_x'].max():.2f}")
        else:
            print(f"âš ï¸  ë‹¤ìš´ë¡œë“œ ì •ë³´ê°€ stateì— ì—†ìŠµë‹ˆë‹¤")
        
        # 4ë‹¨ê³„: Butterworth í•„í„°ë§ + ë°ì´í„° íŠ¸ë¦¬ë°
        print("\n" + "="*80)
        print("4ï¸âƒ£ STEP 4: FilterDataNode - í•„í„°ë§ + íŠ¸ë¦¬ë° (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.data_processing_nodes import FilterDataNode
        filter_node = FilterDataNode()
        
        # LLM ì¶”ì 
        if hasattr(filter_node, 'invoke_llm'):
            filter_node.invoke_llm = create_llm_tracker("FilterDataNode")(filter_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ”„ Butterworth í•„í„° ì ìš© + ë°ì´í„° íŠ¸ë¦¬ë° ì¤‘...")
        current_state = filter_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        filtered_path = current_state.get('filtered_csv_path')
        if filtered_path:
            df_filtered = pd.read_csv(filtered_path)
            original_len = len(df)
            filtered_len = len(df_filtered)
            
            print(f"âœ… í•„í„°ë§ + íŠ¸ë¦¬ë° ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ í•„í„°ë§ëœ íŒŒì¼: {filtered_path}")
            print(f"ğŸ“Š ë°ì´í„° ë³€í™”: {original_len:,} â†’ {filtered_len:,} ë ˆì½”ë“œ")
            print(f"âœ‚ï¸ íŠ¸ë¦¬ë°: ì²˜ìŒ 2ì´ˆ + ë§ˆì§€ë§‰ 3ì´ˆ ì œê±°")
            
            # í•„í„°ë§ í†µê³„
            filter_stats = current_state.get('filter_statistics', {})
            if filter_stats:
                print(f"ğŸ“ˆ í•„í„°ë§ í†µê³„:")
                print(f"   ì›ë³¸ ê¸¸ì´: {filter_stats.get('original_length', 'N/A')} ë ˆì½”ë“œ")
                print(f"   íŠ¸ë¦¬ë° í›„: {filter_stats.get('trimmed_length', 'N/A')} ë ˆì½”ë“œ") 
                print(f"   ìµœì¢… ê¸¸ì´: {filter_stats.get('final_length', 'N/A')} ë ˆì½”ë“œ")
        
        # 5ë‹¨ê³„: ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("5ï¸âƒ£ STEP 5: PredictPhasesNode - ë”¥ëŸ¬ë‹ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.ai_model_nodes import PredictPhasesNode
        predict_phases_node = PredictPhasesNode()
        
        # LLM ì¶”ì 
        if hasattr(predict_phases_node, 'invoke_llm'):
            predict_phases_node.invoke_llm = create_llm_tracker("PredictPhasesNode")(predict_phases_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ¤– Stage2Predictor ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        current_state = predict_phases_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        phases_path = current_state.get('labels_csv_path')
        if phases_path:
            df_phases = pd.read_csv(phases_path)
            print(f"âœ… ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: {phases_path}")
            print(f"ğŸ“Š ë³´í–‰ ì„¸ê·¸ë¨¼íŠ¸: {len(df_phases)}ê°œ")
            if 'phase' in df_phases.columns:
                phase_counts = df_phases['phase'].value_counts().sort_index()
                print(f"ğŸš¶ ë³´í–‰ ë‹¨ê³„ ë¶„í¬:")
                for phase, count in phase_counts.items():
                    print(f"   Phase {phase}: {count}ê°œ")
        
        # 6ë‹¨ê³„: ë³´í­/ì†ë„ ì˜ˆì¸¡ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("6ï¸âƒ£ STEP 6: PredictStrideNode - ë”¥ëŸ¬ë‹ ë³´í­/ì†ë„ ì˜ˆì¸¡ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.ai_model_nodes import PredictStrideNode
        predict_stride_node = PredictStrideNode()
        
        # LLM ì¶”ì 
        if hasattr(predict_stride_node, 'invoke_llm'):
            predict_stride_node.invoke_llm = create_llm_tracker("PredictStrideNode")(predict_stride_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ¤– StrideInferencePipeline ì‹¤í–‰ ì¤‘...")
        current_state = predict_stride_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ë³´í­/ì†ë„ ì˜ˆì¸¡ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        stride_results = current_state.get('stride_results')
        if stride_results and 'predictions' in stride_results:
            predictions = stride_results['predictions']
            print(f"âœ… ë³´í­/ì†ë„ ì˜ˆì¸¡ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ”„ ì˜ˆì¸¡ëœ ë³´í­: {len(predictions)}ê°œ")
            
            if predictions:
                lengths = [p.get('predicted_stride_length', 0) for p in predictions]
                velocities = [p.get('predicted_velocity', 0) for p in predictions]
                print(f"ğŸ“ ë³´í­ í‰ê· : {sum(lengths)/len(lengths):.2f}m")
                print(f"ğŸƒ ì†ë„ í‰ê· : {sum(velocities)/len(velocities):.2f}m/s")
        
        # 7ë‹¨ê³„: 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("7ï¸âƒ£ STEP 7: CalcMetricsNode - 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.metrics_nodes import CalcMetricsNode
        calc_metrics_node = CalcMetricsNode()
        
        # LLM ì¶”ì 
        if hasattr(calc_metrics_node, 'invoke_llm'):
            calc_metrics_node.invoke_llm = create_llm_tracker("CalcMetricsNode")(calc_metrics_node.invoke_llm)
        
        step_start = time.time()
        print(f"ğŸ“Š ìˆœìˆ˜ Pythonìœ¼ë¡œ 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° ì¤‘...")
        current_state = calc_metrics_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {current_state['error']}")
            return
        
        gait_metrics = current_state.get('gait_metrics')
        if gait_metrics:
            print(f"âœ… 12ê°œ ë³´í–‰ ì§€í‘œ ê³„ì‚° ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“Š ê³„ì‚°ëœ ì§€í‘œ:")
            print(f"   â±ï¸ í‰ê·  ë³´í–‰ì‹œê°„: {gait_metrics.get('avg_stride_time', 'N/A'):.3f}ì´ˆ")
            print(f"   ğŸ“ í‰ê·  ë³´í­: {gait_metrics.get('avg_stride_length', 'N/A'):.3f}m")
            print(f"   ğŸƒ í‰ê·  ì†ë„: {gait_metrics.get('avg_walking_speed', 'N/A'):.3f}m/s")
            print(f"   ğŸ”„ ë³´í–‰ë¥ : {gait_metrics.get('cadence', 'N/A'):.1f} steps/min")
            print(f"   âš–ï¸ ë¹„ëŒ€ì¹­ì„±: {gait_metrics.get('stride_length_asymmetry', 'N/A'):.2f}%")
        
        # 8ë‹¨ê³„: ë³´í–‰ ì§€í‘œ ì €ì¥ (LLM ì œê±°ë¨)
        print("\n" + "="*80)
        print("8ï¸âƒ£ STEP 8: StoreMetricsNode - ì§€í‘œ ì €ì¥ (LLM ì œê±°)")
        print("-" * 80)
        
        from langgraph_nodes.metrics_nodes import StoreMetricsNode
        store_metrics_node = StoreMetricsNode()
        
        # LLM ì¶”ì 
        if hasattr(store_metrics_node, 'invoke_llm'):
            store_metrics_node.invoke_llm = create_llm_tracker("StoreMetricsNode")(store_metrics_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabaseì— ë³´í–‰ ì§€í‘œ ì €ì¥ ì¤‘...")
        
        # date í•„ë“œ ì¶”ê°€ (StoreMetricsNodeê°€ ìš”êµ¬)
        current_state['date'] = datetime.now().strftime('%Y-%m-%d')
        current_state = store_metrics_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§€í‘œ ì €ì¥ ì‹¤íŒ¨: {current_state['error']}")
            print("âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ê³„ì† ì§„í–‰)")
        else:
            metrics_record_id = current_state.get('metrics_record_id')
            print(f"âœ… ë³´í–‰ ì§€í‘œ ì €ì¥ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            if metrics_record_id:
                print(f"   ğŸ“Š Record ID: {metrics_record_id}")
        
        # 9ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± (LLM ì‚¬ìš©)
        print("\n" + "="*80)
        print("9ï¸âƒ£ STEP 9: ComposePromptNode - ì§„ë‹¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (LLM ì‚¬ìš©)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import ComposePromptNode
        compose_prompt_node = ComposePromptNode()
        
        # LLM ì¶”ì 
        if hasattr(compose_prompt_node, 'invoke_llm'):
            compose_prompt_node.invoke_llm = create_llm_tracker("ComposePromptNode")(compose_prompt_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ ë³´í–‰ ì§€í‘œ ê¸°ë°˜ ì§„ë‹¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì¤‘...")
        current_state = compose_prompt_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {current_state['error']}")
            return
        
        diagnosis_prompt = current_state.get('diagnosis_prompt')
        if diagnosis_prompt:
            prompt_length = len(diagnosis_prompt)
            print(f"âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length:,} ë¬¸ì")
            print(f"ğŸ¯ í¬í•¨ ì§€í‘œ: {len(current_state.get('gait_metrics', {}))}ê°œ")
        
        # 10ë‹¨ê³„: RAG ê¸°ë°˜ ì§„ë‹¨ (LLM ì‚¬ìš©)
        print("\n" + "="*80)
        print("ğŸ”Ÿ STEP 10: RagDiagnosisNode - RAG ê¸°ë°˜ ì˜ë£Œ ì§„ë‹¨ (LLM ì‚¬ìš©)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import RagDiagnosisNode
        rag_diagnosis_node = RagDiagnosisNode()
        
        # LLM ì¶”ì 
        if hasattr(rag_diagnosis_node, 'invoke_llm'):
            rag_diagnosis_node.invoke_llm = create_llm_tracker("RagDiagnosisNode")(rag_diagnosis_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ ChromaDB ê²€ìƒ‰ + LLM ì§„ë‹¨ ìƒì„± ì¤‘...")
        current_state = rag_diagnosis_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ RAG ì§„ë‹¨ ì‹¤íŒ¨: {current_state['error']}")
            return
        
        diagnosis_result = current_state.get('diagnosis_result')
        retrieved_docs = current_state.get('retrieved_documents', [])
        
        if diagnosis_result:
            print(f"âœ… RAG ì§„ë‹¨ ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved_docs)}ê°œ")
            print(f"ğŸ” ì§„ë‹¨ ê²°ê³¼ ê¸¸ì´: {len(diagnosis_result):,} ë¬¸ì")
            
            # ì§„ë‹¨ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            preview = diagnosis_result[:200] + "..." if len(diagnosis_result) > 200 else diagnosis_result
            print(f"ğŸ‘¨â€âš•ï¸ ì§„ë‹¨ ë¯¸ë¦¬ë³´ê¸°: {preview}")
        
        # 11ë‹¨ê³„: ì§„ë‹¨ ê²°ê³¼ ì €ì¥ (LLM ì‚¬ìš©)
        print("\n" + "="*80)
        print("1ï¸âƒ£1ï¸âƒ£ STEP 11: StoreDiagnosisNode - ì§„ë‹¨ ê²°ê³¼ ì €ì¥ (LLM ì‚¬ìš©)")
        print("-" * 80)
        
        from langgraph_nodes.rag_diagnosis_nodes import StoreDiagnosisNode
        store_diagnosis_node = StoreDiagnosisNode()
        
        # LLM ì¶”ì 
        if hasattr(store_diagnosis_node, 'invoke_llm'):
            store_diagnosis_node.invoke_llm = create_llm_tracker("StoreDiagnosisNode")(store_diagnosis_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ Supabaseì— ì§„ë‹¨ ê²°ê³¼ ì €ì¥ ì¤‘...")
        current_state = store_diagnosis_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì§„ë‹¨ ì €ì¥ ì‹¤íŒ¨: {current_state['error']}")
            print("âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ê³„ì† ì§„í–‰)")
        else:
            diagnosis_record_id = current_state.get('diagnosis_record_id')
            print(f"âœ… ì§„ë‹¨ ê²°ê³¼ ì €ì¥ ì„±ê³µ! ({step_time:.2f}ì´ˆ)")
            if diagnosis_record_id:
                print(f"   ğŸ¥ Record ID: {diagnosis_record_id}")
        
        # 12ë‹¨ê³„: ìµœì¢… ì‘ë‹µ í¬ë§·íŒ… (LLM ì‚¬ìš©)
        print("\n" + "="*80)
        print("1ï¸âƒ£2ï¸âƒ£ STEP 12: FormatResponseNode - ìµœì¢… ì‘ë‹µ ìƒì„± (LLM ì‚¬ìš©)")
        print("-" * 80)
        
        from langgraph_nodes.response_nodes import FormatResponseNode
        format_response_node = FormatResponseNode()
        
        # LLM ì¶”ì 
        if hasattr(format_response_node, 'invoke_llm'):
            format_response_node.invoke_llm = create_llm_tracker("FormatResponseNode")(format_response_node.invoke_llm)
        
        step_start = time.time()
        print("ğŸ”„ ì‚¬ìš©ì ì¹œí™”ì  ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
        current_state = format_response_node.execute(current_state)
        step_time = time.time() - step_start
        
        if current_state.get('error'):
            print(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì‹¤íŒ¨: {current_state['error']}")
            return
        
        final_response = current_state.get('final_response')
        if final_response:
            response_length = len(final_response)
            print(f"âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ! ({step_time:.2f}ì´ˆ)")
            print(f"ğŸ“„ ì‘ë‹µ ê¸¸ì´: {response_length:,} ë¬¸ì")
            
            # ìµœì¢… ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°
            preview = final_response[:300] + "..." if len(final_response) > 300 else final_response
            print(f"ğŸ“‹ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{preview}")
        
        total_time = time.time() - pipeline_start_time
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ‰ ì™„ì „í•œ End-to-End LangGraph íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*80)
        
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ì„±ê³¼:")
        print(f"   ğŸš€ ì´ LLM í˜¸ì¶œ: {llm_call_count}íšŒ")
        print(f"   â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ğŸ¯ ìµœì í™” êµ¬ì¡°: 8/12 ë…¸ë“œ LLM ì œê±° (67% ìµœì í™”)")
        print(f"   ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜: ë°ì´í„° ì²˜ë¦¬ëŠ” ìˆœìˆ˜ Python, ì§„ë‹¨ì€ LLM")
        
        print(f"\nğŸ“ˆ ì „ì²´ 12ë‹¨ê³„ ì²˜ë¦¬ ìš”ì•½:")
        print(f"   1ï¸âƒ£ ì…ë ¥ ê²€ì¦: âœ… LLM ì œê±° (ìˆœìˆ˜ Python)")
        print(f"   2ï¸âƒ£ íŒŒì¼ ê²€ìƒ‰: âœ… LLM ì œê±° (Storage API)")
        print(f"   3ï¸âƒ£ ë°ì´í„° ë‹¤ìš´ë¡œë“œ: âœ… LLM ì œê±° (Storage API)")
        print(f"   4ï¸âƒ£ í•„í„°ë§+íŠ¸ë¦¬ë°: âœ… LLM ì œê±° (Butterworth)")
        print(f"   5ï¸âƒ£ ë³´í–‰ ë‹¨ê³„ ì˜ˆì¸¡: âœ… LLM ì œê±° (ë”¥ëŸ¬ë‹)")
        print(f"   6ï¸âƒ£ ë³´í­/ì†ë„ ì˜ˆì¸¡: âœ… LLM ì œê±° (ë”¥ëŸ¬ë‹)")
        print(f"   7ï¸âƒ£ ì§€í‘œ ê³„ì‚°: âœ… LLM ì œê±° (ìˆœìˆ˜ Python)")
        print(f"   8ï¸âƒ£ ì§€í‘œ ì €ì¥: âœ… LLM ì œê±° (Database API)")
        print(f"   9ï¸âƒ£ í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ğŸ¤– LLM ì‚¬ìš© (ì§„ë‹¨ ì¤€ë¹„)")
        print(f"   ğŸ”Ÿ RAG ì§„ë‹¨: ğŸ¤– LLM ì‚¬ìš© (ì˜ë£Œ ì§„ë‹¨)")
        print(f"   1ï¸âƒ£1ï¸âƒ£ ì§„ë‹¨ ì €ì¥: ğŸ¤– LLM ì‚¬ìš© (êµ¬ì¡°í™”)")
        print(f"   1ï¸âƒ£2ï¸âƒ£ ì‘ë‹µ ìƒì„±: ğŸ¤– LLM ì‚¬ìš© (ì‚¬ìš©ì ì¹œí™”ì )")
        
        print(f"\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"   ğŸ“¥ ë‹¤ìš´ë¡œë“œ: {current_state.get('raw_csv_path', 'N/A')}")
        print(f"   ğŸ”§ í•„í„°ë§: {current_state.get('filtered_csv_path', 'N/A')}")
        print(f"   ğŸ¤– ë³´í–‰ë‹¨ê³„: {current_state.get('labels_csv_path', 'N/A')}")
        
        print(f"\nğŸ—„ï¸ ì €ì¥ëœ ë°ì´í„°:")
        if current_state.get('metrics_record_id'):
            print(f"   ğŸ“Š ë³´í–‰ ì§€í‘œ: Record ID {current_state.get('metrics_record_id')}")
        if current_state.get('diagnosis_record_id'):
            print(f"   ğŸ¥ ì§„ë‹¨ ê²°ê³¼: Record ID {current_state.get('diagnosis_record_id')}")
        
        # ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„
        print(f"\nğŸ—ï¸ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ ë¶„ì„:")
        print(f"   ğŸ“Š ì…ë ¥ ì‹œìŠ¤í…œ: (user_id, height_cm, gender)")
        print(f"   ğŸ—„ï¸ ë°ì´í„° ì†ŒìŠ¤: Supabase Storage (CSV íŒŒì¼)")
        print(f"   ğŸ¤– ë°ì´í„° ì²˜ë¦¬: ìˆœìˆ˜ Python + ë”¥ëŸ¬ë‹ (LLM ì—†ìŒ)")
        print(f"   ğŸ§  ì˜ë£Œ ì§„ë‹¨: RAG + LLM (ChromaDB + ì˜ë£Œ ë¬¸í—Œ)")
        print(f"   âš¡ ì„±ëŠ¥: ë°ì´í„° ì²˜ë¦¬ ì¦‰ì‹œ ì‹¤í–‰, ì§„ë‹¨ë§Œ LLM ëŒ€ê¸°")
        
        # ìµœì¢… ì„±ê³¼ í‰ê°€
        if llm_call_count <= 4:  # ì˜ˆìƒë˜ëŠ” LLM í˜¸ì¶œ (ì§„ë‹¨ ê´€ë ¨ 4ê°œ ë…¸ë“œ)
            print(f"\nğŸ‰ End-to-End íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
            print(f"   âœ… ì „ì²´ 12ë‹¨ê³„ ì™„ë£Œ")
            print(f"   âœ… 67% ìµœì í™” ë‹¬ì„± (8/12 ë…¸ë“œ LLM ì œê±°)")
            print(f"   âœ… í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ êµ¬í˜„")
            print(f"   âœ… ì˜ë£Œì§„ë‹¨ í’ˆì§ˆ ìœ ì§€ + ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ")
            
            if current_state.get('final_response'):
                print(f"   âœ… ìµœì¢… ì‚¬ìš©ì ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        else:
            print(f"\nâš ï¸  ì˜ˆìƒë³´ë‹¤ ë§ì€ LLM í˜¸ì¶œ ë°œê²¬ ({llm_call_count}íšŒ)")
            print(f"   ì˜ˆìƒ: 4íšŒ (ì§„ë‹¨ ê´€ë ¨ ë…¸ë“œë“¤ë§Œ)")
            print(f"   ì‹¤ì œ: {llm_call_count}íšŒ")
            print(f"   ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return current_state
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    result = test_optimized_nodes_pipeline() 